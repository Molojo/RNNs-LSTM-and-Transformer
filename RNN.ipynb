{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Using device: mps\n",
      "Loaded 39557 text entries from train.jsonl. Total length: 14673121 characters\n",
      "Loaded 9890 text entries from test.jsonl. Total length: 3684340 characters\n",
      "Number of train tokens: 3402656\n",
      "Number of val tokens: 855555\n",
      "Number of training sequences: 3402536\n",
      "Number of validation sequences: 855435\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 709\u001b[0m\n\u001b[1;32m    706\u001b[0m    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal elapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 709\u001b[0m    main()\n",
      "Cell \u001b[0;32mIn[1], line 593\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of validation sequences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_seqs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# ------------------ Create Dataset Objects and DataLoaders ------------------\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m LanguageModelDataset(train_seqs, max_seq_length)\n\u001b[1;32m    594\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m LanguageModelDataset(val_seqs, max_seq_length)\n\u001b[1;32m    595\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    596\u001b[0m                           num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 184\u001b[0m, in \u001b[0;36mLanguageModelDataset.__init__\u001b[0;34m(self, sequences, seq_length)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sequences, seq_length):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Filter sequences to ensure they have the exact required length.\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m [(seq[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], seq[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;241m==\u001b[39m seq_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " #!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Foundational AI Project 2 – Language Modeling with RNNs, LSTMs, and Transformer (Graduate Version)\n",
    "\n",
    "This script trains three language models (RNN, LSTM, Transformer) for text generation.\n",
    "It uses a SentencePiece BPE tokenizer (vocab size=10000) to tokenize text from JSONL files,\n",
    "builds fixed-length sequences via a sliding window approach, and trains the models\n",
    "using early stopping with a cosine annealing learning rate scheduler.\n",
    "Evaluation metrics include perplexity (exp(cross-entropy loss)), token accuracy,\n",
    "and BLEU score (computed with nltk). Sample outputs and loss curves with detailed plots\n",
    "are generated, and model performance is compared.\n",
    "Graduate-level requirements such as temperature-based decoding are supported in the prompt methods.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR  # Cosine learning rate scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sentencepiece as spm  # for subword tokenization\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Select computation device: GPU if available, otherwise CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"[✓] Using device: {device}\")\n",
    "\n",
    "###############################################################################\n",
    "# Positional Encoding Module\n",
    "###############################################################################\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes sinusoidal positional encodings and adds them to token embeddings.\n",
    "    This implementation follows 'Attention is All You Need' (Vaswani et al., 2017).\n",
    "    \n",
    "    Args:\n",
    "        embed_dim (int): Embedding dimension (d_model).\n",
    "        max_len (int): Maximum sequence length (number of positions).\n",
    "        dropout (float): Dropout probability applied after adding positional encodings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Initialize an empty tensor for positional encoding: shape [max_len, embed_dim]\n",
    "        pos_enc = torch.zeros(max_len, embed_dim)\n",
    "        # Create a column vector of positions [0, 1, ..., max_len-1]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Compute the divisor term for the sinusoidal functions\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float) * -(math.log(10000.0) / embed_dim))\n",
    "        # Apply sine to even indices and cosine to odd indices of the embedding dimension\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)  # even indices: sin(position/scale)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)  # odd indices: cos(position/scale)\n",
    "        # Add an extra batch dimension so that pos_enc becomes [1, max_len, embed_dim]\n",
    "        pos_enc = pos_enc.unsqueeze(0)\n",
    "        # Register pos_enc as a buffer so it is saved with the model and moved across devices\n",
    "        self.register_buffer(\"pos_enc\", pos_enc)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds positional encoding to input tensor x.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input embeddings of shape [batch_size, seq_length, embed_dim].\n",
    "        Returns:\n",
    "            Tensor: Output embeddings with added positional encoding.\n",
    "        \"\"\"\n",
    "        # Slice the positional encoding to the input sequence length and add to x.\n",
    "        x = x + self.pos_enc[:, :x.size(1)]\n",
    "        # Apply dropout and return\n",
    "        return self.dropout(x)\n",
    "\n",
    "###############################################################################\n",
    "# Data Preparation Functions\n",
    "###############################################################################\n",
    "def train_tokenizer_if_needed(tokenizer_model_prefix=\"tokenizer\", vocab_size=10000, training_text_file=\"train.txt\"):\n",
    "    \"\"\"\n",
    "    Trains a SentencePiece tokenizer if the model file is not found.\n",
    "    The tokenizer is used to encode text into subword tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer_model_prefix (str): Prefix for the tokenizer model filename.\n",
    "        vocab_size (int): Subword vocabulary size.\n",
    "        training_text_file (str): Plain text file used to train the tokenizer.\n",
    "    \n",
    "    Returns:\n",
    "        SentencePieceProcessor: The trained tokenizer.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"{tokenizer_model_prefix}.model\"):\n",
    "        print(\"Training tokenizer...\")\n",
    "        # Read training text from file\n",
    "        with open(training_text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            training_text = f.read()\n",
    "        # Save the training text to a temporary file needed by SentencePieceTrainer.\n",
    "        with open(\"temp_training.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(training_text)\n",
    "        # Train the SentencePiece model using BPE\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=\"temp_training.txt\",\n",
    "            model_prefix=tokenizer_model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "            model_type=\"bpe\",\n",
    "            character_coverage=1.0\n",
    "        )\n",
    "        # Remove the temporary file\n",
    "        os.remove(\"temp_training.txt\")\n",
    "    # Load the tokenizer model\n",
    "    sp = spm.SentencePieceProcessor(model_file=f\"{tokenizer_model_prefix}.model\")\n",
    "    return sp\n",
    "\n",
    "def load_and_tokenize(file_path, sp):\n",
    "    \"\"\"\n",
    "    Loads text data from a JSONL file and tokenizes it.\n",
    "    Each JSON object should have \"prompt\" and \"completion\" fields.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file.\n",
    "        sp (SentencePieceProcessor): Trained SentencePiece tokenizer.\n",
    "    \n",
    "    Returns:\n",
    "        list[int]: List of token IDs from the combined text.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                # Parse JSON line\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # Skip lines that are not valid JSON\n",
    "            prompt = obj.get(\"prompt\", \"\")\n",
    "            completion = obj.get(\"completion\", \"\")\n",
    "            # Combine prompt and completion\n",
    "            text = (prompt + \" \" + completion).strip()\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "    # Combine all texts into one large text separated by newlines\n",
    "    combined = \"\\n\".join(texts)\n",
    "    print(f\"Loaded {len(texts)} text entries from {file_path}. Total length: {len(combined)} characters\")\n",
    "    # Tokenize the combined text using SentencePiece\n",
    "    return sp.encode(combined, out_type=int)\n",
    "\n",
    "def build_sequences(token_ids, seq_length):\n",
    "    \"\"\"\n",
    "    Creates overlapping sequences from token IDs.\n",
    "    Each sequence will have (seq_length + 1) tokens to allow input/target pairing.\n",
    "    \n",
    "    Args:\n",
    "        token_ids (list[int]): List of token IDs.\n",
    "        seq_length (int): Desired input sequence length.\n",
    "    \n",
    "    Returns:\n",
    "        list[list[int]]: List where each sublist is a token sequence of length seq_length+1.\n",
    "    \"\"\"\n",
    "    return [token_ids[i:i+seq_length+1] for i in range(len(token_ids) - seq_length)]\n",
    "\n",
    "###############################################################################\n",
    "# Custom Dataset Class for Language Modeling\n",
    "###############################################################################\n",
    "class LanguageModelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for language modeling.\n",
    "    Each sample is a tuple: (input_tokens, target_tokens) where target_tokens is the input shifted by one.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, seq_length):\n",
    "        # Filter sequences to ensure they have the exact required length.\n",
    "        self.samples = [(seq[:-1], seq[1:]) for seq in sequences if len(seq) == seq_length + 1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inp, target = self.samples[idx]\n",
    "        return torch.tensor(inp, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "###############################################################################\n",
    "# Text Generation and Model Definitions\n",
    "###############################################################################\n",
    "def generate_text(model, tokenizer, prompt_text, max_length=128, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates text by autoregressively sampling from the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained language model.\n",
    "        tokenizer (SentencePieceProcessor): Tokenizer to encode/decode text.\n",
    "        prompt_text (str): The initial text prompt.\n",
    "        max_length (int): Maximum number of tokens to generate (excluding the prompt tokens).\n",
    "        temperature (float): Sampling temperature, where values near 0 imply greedy decoding.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated text (decoded).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Encode prompt text to token IDs.\n",
    "    token_ids = tokenizer.encode(prompt_text, out_type=int)\n",
    "    generated = token_ids.copy()  # Copy prompt tokens to start generation\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Prepare input tensor of shape [1, current_length]\n",
    "            input_ids = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "            logits = model(input_ids)  # Forward pass through the model; shape: [1, seq_len, vocab_size]\n",
    "            next_logits = logits[0, -1, :]  # Take the logits for the last time step\n",
    "            # Determine next token\n",
    "            if temperature < 1e-5:\n",
    "                # Greedy: take the token with maximum probability\n",
    "                next_token = torch.argmax(next_logits).item()\n",
    "            else:\n",
    "                # Scale logits by temperature\n",
    "                scaled_logits = next_logits / temperature\n",
    "                # Compute probabilities\n",
    "                probs = torch.softmax(scaled_logits, dim=0)\n",
    "                # Sample the next token\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated.append(next_token)\n",
    "            # Stop if end-of-sequence token is produced.\n",
    "            if next_token == tokenizer.eos_id():\n",
    "                break\n",
    "    # Decode the generated token IDs back into text.\n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla RNN-based language model.\n",
    "    \n",
    "    Architecture:\n",
    "      - Embedding layer: maps token IDs to a continuous representation.\n",
    "      - RNN layers: processes the sequence.\n",
    "      - Fully connected layer: projects RNN outputs to vocabulary logits.\n",
    "    \n",
    "    Includes a prompt method that supports temperature-based generation\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Map token indices to embeddings\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)  # RNN layer(s)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)  # Output projection layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x has shape [batch_size, seq_length]\n",
    "        embeds = self.embedding(x)  # Convert tokens to embeddings: shape [batch, seq_length, embed_dim]\n",
    "        output, _ = self.rnn(embeds)  # Process embeddings with RNN: output shape [batch, seq_length, hidden_dim]\n",
    "        logits = self.fc(output)  # Project to vocabulary logits: [batch, seq_length, vocab_size]\n",
    "        return logits\n",
    "    \n",
    "    def prompt(self, tokenizer, prompt_text, max_length=128, temperature=1.0):\n",
    "        # Generate text using the generate_text function\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based language model.\n",
    "    \n",
    "    Architecture similar to RNNLanguageModel but uses nn.LSTM layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        output, _ = self.lstm(embeds)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "    \n",
    "    def prompt(self, tokenizer, prompt_text, max_length=128, temperature=1.0):\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based language model.\n",
    "    \n",
    "    Architecture:\n",
    "      - Embedding layer followed by positional encoding.\n",
    "      - Transformer encoder: captures long-range dependencies.\n",
    "      - Fully-connected layer to output vocabulary logits.\n",
    "      \n",
    "    Positional encoding ensures that token order is captured.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_seq_length, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Incorporate positional encoding to add sequence order information.\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=max_seq_length, dropout=dropout)\n",
    "        # Define a single Transformer encoder layer and stack multiple layers.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
    "                                                   dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convert token indices to embeddings.\n",
    "        embeds = self.embedding(x)  # [batch, seq_length, embed_dim]\n",
    "        # Add positional encodings.\n",
    "        encoded = self.pos_encoder(embeds)\n",
    "        # Transformer expects input shape [seq_length, batch, embed_dim]; so we transpose.\n",
    "        encoded = encoded.transpose(0, 1)\n",
    "        # Pass through Transformer encoder.\n",
    "        transformer_out = self.transformer_encoder(encoded)\n",
    "        # Transpose back to [batch, seq_length, embed_dim].\n",
    "        transformer_out = transformer_out.transpose(0, 1)\n",
    "        # Project to vocabulary logits.\n",
    "        logits = self.fc(transformer_out)\n",
    "        return logits\n",
    "    \n",
    "    def prompt(self, tokenizer, prompt_text, max_length=128, temperature=1.0):\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "###############################################################################\n",
    "# Training, Evaluation, and Plotting Functions\n",
    "###############################################################################\n",
    "def train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, scheduler, device, patience=5):\n",
    "    \"\"\"\n",
    "    Trains the model for a maximum number of epochs with early stopping.\n",
    "    \n",
    "    Uses mini-batch gradient descent with gradient clipping and a cosine annealing scheduler.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The language model.\n",
    "        train_loader (DataLoader): Training data loader.\n",
    "        val_loader (DataLoader): Validation data loader.\n",
    "        num_epochs (int): Maximum epochs.\n",
    "        criterion: Loss function (CrossEntropyLoss).\n",
    "        optimizer: Optimizer (e.g., AdamW).\n",
    "        scheduler: CosineAnnealingLR scheduler.\n",
    "        device: Computation device.\n",
    "        patience (int): Number of epochs to wait without improvement for early stopping.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Lists of training and validation losses per epoch.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_train_loss = 0\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # Training loop over mini-batches\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients for this batch\n",
    "            outputs = model(inputs)  # Forward pass: shape [batch, seq_length, vocab_size]\n",
    "            # Compute loss by flattening outputs and targets\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            loss.backward()  # Backpropagate loss\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()  # Update model parameters\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation evaluation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Step the LR scheduler after each epoch\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_duration = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch} | LR: {current_lr:.6f} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Duration: {epoch_duration:.2f}s\")\n",
    "\n",
    "        # Early stopping mechanism: if validation loss doesn't improve, increment counter\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered. Restoring best model state.\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the provided dataset.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained language model.\n",
    "        data_loader (DataLoader): DataLoader for evaluation.\n",
    "        criterion: Loss function.\n",
    "        device: Computation device.\n",
    "        \n",
    "    Returns:\n",
    "        float: Average loss computed over all batches.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    \"\"\"\n",
    "    Computes perplexity from cross-entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        loss (float): Average cross-entropy loss.\n",
    "    \n",
    "    Returns:\n",
    "        float: Perplexity as exp(loss).\n",
    "    \"\"\"\n",
    "    return np.exp(loss)\n",
    "\n",
    "def compute_token_accuracy(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Computes token-level accuracy over the entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained language model.\n",
    "        data_loader (DataLoader): DataLoader for the dataset.\n",
    "        device: Computation device.\n",
    "    \n",
    "    Returns:\n",
    "        float: Fraction of tokens correctly predicted.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += targets.numel()\n",
    "    return correct / total\n",
    "\n",
    "def compute_bleu(reference, candidate):\n",
    "    \"\"\"\n",
    "    Computes BLEU score comparing a reference sentence to a generated candidate sentence.\n",
    "    \n",
    "    Args:\n",
    "        reference (str): Ground truth sentence.\n",
    "        candidate (str): Generated sentence.\n",
    "    \n",
    "    Returns:\n",
    "        float: BLEU score (0 to 1).\n",
    "    \"\"\"\n",
    "    return sentence_bleu([nltk.word_tokenize(reference)], nltk.word_tokenize(candidate))\n",
    "\n",
    "def plot_loss_curve(train_losses, val_losses, model_name):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss curves with annotations.\n",
    "    \n",
    "    Args:\n",
    "        train_losses (list[float]): Training losses per epoch.\n",
    "        val_losses (list[float]): Validation losses per epoch.\n",
    "        model_name (str): Name of the model (used for title and saving the plot).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    # Plot training loss with marker circle\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o', linestyle='-', linewidth=2)\n",
    "    # Plot validation loss with square marker\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\", marker='s', linestyle='-', linewidth=2)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Loss\", fontsize=12)\n",
    "    plt.title(f\"{model_name} Loss Curve\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    # Annotate final loss values with arrows\n",
    "    plt.annotate(f\"Final Train: {train_losses[-1]:.4f}\", xy=(epochs[-1], train_losses[-1]),\n",
    "                 xytext=(epochs[-1]-5, train_losses[-1]+0.05),\n",
    "                 arrowprops=dict(facecolor='blue', shrink=0.05),\n",
    "                 fontsize=10, color='blue')\n",
    "    plt.annotate(f\"Final Val: {val_losses[-1]:.4f}\", xy=(epochs[-1], val_losses[-1]),\n",
    "                 xytext=(epochs[-1]-5, val_losses[-1]+0.05),\n",
    "                 arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "                 fontsize=10, color='red')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name}_loss.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# Main Training and Evaluation Pipeline\n",
    "###############################################################################\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to train, evaluate, and compare the language models.\n",
    "    \n",
    "    This function performs the following steps:\n",
    "      1. Sets hyperparameters and file paths.\n",
    "      2. Prepares the tokenizer (training it if needed).\n",
    "      3. Loads and tokenizes training and validation datasets.\n",
    "      4. Builds fixed-length sequences and creates DataLoaders.\n",
    "      5. Initializes the RNN, LSTM, and Transformer models.\n",
    "      6. Trains each model with early stopping, gradient clipping, and a cosine scheduler.\n",
    "      7. Evaluates each model using perplexity, token accuracy, and BLEU score.\n",
    "      8. Generates sample text for a fixed prompt.\n",
    "      9. Saves model parameters and displays performance comparisons through plots.\n",
    "    \"\"\"\n",
    "    global_start = time.time()\n",
    "\n",
    "    # ------------------ Hyperparameters ------------------\n",
    "    vocab_size = 10000          # Vocabulary size for tokenizer\n",
    "    embed_dim = 256             # Embedding dimension for tokens\n",
    "    hidden_dim = 256            # Hidden dimension for RNN/LSTM and transformer feedforward\n",
    "    num_layers = 2              # Number of layers in RNN/LSTM/Transformer encoder\n",
    "    num_heads = 8               # Number of attention heads in Transformer\n",
    "    max_seq_length = 128         # Input sequence length (without EOS token)\n",
    "    batch_size = 256            # Training batch size\n",
    "    num_epochs = 30             # Maximum number of epochs for training\n",
    "    learning_rate = 1e-4        # Initial learning rate\n",
    "    dropout_rate = 0.2         # Dropout rate for Transformer positional encoding and encoder layers\n",
    "    weight_decay = 1e-4         # Weight decay for optimizer regularization\n",
    "    pad_token_id = 3            # Token ID for padding (should match tokenizer's setting)\n",
    "\n",
    "    # ------------------ File Paths ------------------\n",
    "    # Update these paths as needed.\n",
    "    train_file = \"train.jsonl\"\n",
    "    test_file = \"test.jsonl\"\n",
    "    \n",
    "    if not os.path.exists(train_file) or not os.path.exists(test_file):\n",
    "        raise FileNotFoundError(\"train.jsonl and/or test.jsonl not found.\")\n",
    "    \n",
    "    # ------------------ Prepare Training Text for Tokenizer ------------------\n",
    "    tokenizer_training_file = \"train.txt\"\n",
    "    if not os.path.exists(tokenizer_training_file):\n",
    "        texts = []\n",
    "        with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                prompt = obj.get(\"prompt\", \"\")\n",
    "                completion = obj.get(\"completion\", \"\")\n",
    "                text = (prompt + \" \" + completion).strip()\n",
    "                if text:\n",
    "                    texts.append(text)\n",
    "        # Combine all text entries into one large file.\n",
    "        combined_text = \"\\n\".join(texts)\n",
    "        with open(tokenizer_training_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(combined_text)\n",
    "\n",
    "    # ------------------ Tokenizer Preparation ------------------\n",
    "    sp = train_tokenizer_if_needed(tokenizer_model_prefix=\"tokenizer\", vocab_size=vocab_size, training_text_file=tokenizer_training_file)\n",
    "\n",
    "    # ------------------ Load and Tokenize Datasets ------------------\n",
    "    train_tokens = load_and_tokenize(train_file, sp)\n",
    "    val_tokens = load_and_tokenize(test_file, sp)\n",
    "\n",
    "    # ------------------ Build Fixed-Length Token Sequences ------------------\n",
    "    train_seqs = build_sequences(train_tokens, max_seq_length)\n",
    "    val_seqs = build_sequences(val_tokens, max_seq_length)\n",
    "    print(f\"Number of train tokens: {len(train_tokens)}\")\n",
    "    print(f\"Number of val tokens: {len(val_tokens)}\")\n",
    "    print(f\"Number of training sequences: {len(train_seqs)}\")\n",
    "    print(f\"Number of validation sequences: {len(val_seqs)}\")\n",
    "\n",
    "    # ------------------ Create Dataset Objects and DataLoaders ------------------\n",
    "    train_dataset = LanguageModelDataset(train_seqs, max_seq_length)\n",
    "    val_dataset = LanguageModelDataset(val_seqs, max_seq_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=0, pin_memory=True if device.type == \"cuda\" else False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                            num_workers=0, pin_memory=True if device.type == \"cuda\" else False)\n",
    "\n",
    "    # ------------------ Initialize Models ------------------\n",
    "    models = {\n",
    "        \"RNN\": RNNLanguageModel(vocab_size, embed_dim, hidden_dim, num_layers),\n",
    "        #\"LSTM\": LSTMLanguageModel(vocab_size, embed_dim, hidden_dim, num_layers),\n",
    "        #\"Transformer\": TransformerLanguageModel(vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_seq_length, dropout=dropout_rate)\n",
    "    }\n",
    "\n",
    "    model_results = {}  # Dictionary to hold evaluation metrics for each model\n",
    "\n",
    "    # ------------------ Train, Evaluate and Compare Models ------------------\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} Model ---\")\n",
    "        model.to(device)\n",
    "        # Use CrossEntropyLoss ignoring the padding token.\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        # Using a cosine annealing scheduler that adjusts the learning rate over epochs.\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-5)\n",
    "        \n",
    "        # Train model and record time.\n",
    "        model_start_time = time.time()\n",
    "        train_losses, val_losses = train_model(model, train_loader, val_loader, num_epochs,\n",
    "                                                criterion, optimizer, scheduler, device, patience=5)\n",
    "        training_time = time.time() - model_start_time\n",
    "        print(f\"Total training time for {name}: {training_time:.2f} seconds\")\n",
    "        \n",
    "        # Plot loss curves with detailed annotations.\n",
    "        plot_loss_curve(train_losses, val_losses, name)\n",
    "        \n",
    "        # Evaluate model on validation set.\n",
    "        val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "        perplexity = compute_perplexity(val_loss)\n",
    "        token_accuracy = compute_token_accuracy(model, val_loader, device) * 100\n",
    "        \n",
    "        # Compute BLEU score on a random sample from validation dataset.\n",
    "        sample_idx = random.randint(0, len(val_dataset) - 1)\n",
    "        sample_input, sample_target = val_dataset[sample_idx]\n",
    "        prompt_text = sp.decode(sample_input.tolist())\n",
    "        reference_text = sp.decode(sample_target.tolist())\n",
    "        generated_text = model.prompt(sp, prompt_text, max_length=128, temperature=1.0)\n",
    "        bleu = compute_bleu(reference_text, generated_text)\n",
    "        \n",
    "        print(f\"{name} | Perplexity: {perplexity:.2f} | Token Accuracy: {token_accuracy:.2f}% | BLEU: {bleu:.4f}\")\n",
    "        \n",
    "        # Generate sample output for a fixed prompt.\n",
    "        fixed_prompt = \"Which do you prefer? Dogs or cats?\"\n",
    "        sample_output = model.prompt(sp, fixed_prompt, max_length=128, temperature=1.0)\n",
    "        print(\"Sample generated output:\", sample_output)\n",
    "        \n",
    "        # Save the trained model state.\n",
    "        torch.save(model.state_dict(), f\"{name}_model.pt\")\n",
    "        print(f\"{name} model saved as {name}_model.pt\")\n",
    "        \n",
    "        # Store evaluation metrics for later comparison.\n",
    "        model_results[name] = {\n",
    "            \"Perplexity\": perplexity,\n",
    "            \"Token Accuracy (%)\": token_accuracy,\n",
    "            \"BLEU Score\": bleu,\n",
    "            \"Training Time (s)\": training_time\n",
    "        }\n",
    "    \n",
    "    # ------------------ Compare Model Performance ------------------\n",
    "    print(\"\\n--- Model Performance Summary ---\")\n",
    "    header = f\"{'Model':<15} {'Perplexity':<12} {'Token Accuracy (%)':<20} {'BLEU Score':<12} {'Train Time (s)':<15}\"\n",
    "    print(header)\n",
    "    for model_name, metrics in model_results.items():\n",
    "        print(f\"{model_name:<15} {metrics['Perplexity']:<12.2f} {metrics['Token Accuracy (%)']:<20.2f} \"\n",
    "              f\"{metrics['BLEU Score']:<12.4f} {metrics['Training Time (s)']:<15.2f}\")\n",
    "    \n",
    "    # Create comparative bar plots.\n",
    "    model_names = list(model_results.keys())\n",
    "    perplexities = [model_results[m][\"Perplexity\"] for m in model_names]\n",
    "    accuracies = [model_results[m][\"Token Accuracy (%)\"] for m in model_names]\n",
    "    bleu_scores = [model_results[m][\"BLEU Score\"] for m in model_names]\n",
    "    train_times = [model_results[m][\"Training Time (s)\"] for m in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.2\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    axs[0, 0].bar(x, perplexities, width, color=\"skyblue\")\n",
    "    axs[0, 0].set_title(\"Perplexity\")\n",
    "    axs[0, 0].set_xticks(x)\n",
    "    axs[0, 0].set_xticklabels(model_names)\n",
    "    \n",
    "    axs[0, 1].bar(x, accuracies, width, color=\"lightgreen\")\n",
    "    axs[0, 1].set_title(\"Token Accuracy (%)\")\n",
    "    axs[0, 1].set_xticks(x)\n",
    "    axs[0, 1].set_xticklabels(model_names)\n",
    "    \n",
    "    axs[1, 0].bar(x, bleu_scores, width, color=\"salmon\")\n",
    "    axs[1, 0].set_title(\"BLEU Score\")\n",
    "    axs[1, 0].set_xticks(x)\n",
    "    axs[1, 0].set_xticklabels(model_names)\n",
    "    \n",
    "    axs[1, 1].bar(x, train_times, width, color=\"plum\")\n",
    "    axs[1, 1].set_title(\"Training Time (s)\")\n",
    "    axs[1, 1].set_xticks(x)\n",
    "    axs[1, 1].set_xticklabels(model_names)\n",
    "    \n",
    "    plt.suptitle(\"Model Performance and Computational Requirements Comparison\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(\"model_comparison.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    total_time = time.time() - global_start\n",
    "    print(f\"\\nTotal elapsed time: {total_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
