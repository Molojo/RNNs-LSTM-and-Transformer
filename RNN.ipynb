{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Using device: cuda\n",
      "Found existing tokenizer model: bpe_tokenizer_v1.model\n",
      "Tokenizer loaded. Vocab size: 10000. Special IDs: UNK=0, BOS=1, EOS=2, PAD=3\n",
      "Using Padding Token ID: 3\n",
      "Effective vocabulary size: 10000\n",
      "\n",
      "Loading and tokenizing datasets...\n",
      "Loaded 39557 text entries from train.jsonl (out of 39557 lines). Total combined length: 14870901 characters\n",
      "Tokenized into 3757733 tokens.\n",
      "Loaded 9890 text entries from test.jsonl (out of 9890 lines). Total combined length: 3733785 characters\n",
      "Tokenized into 943202 tokens.\n",
      "\n",
      "Building sequences...\n",
      "Number of training sequences created: 3757605\n",
      "Number of validation sequences created: 943074\n",
      "\n",
      "Creating Dataset objects and DataLoaders...\n",
      "Train dataset size: 3757605 samples\n",
      "Validation dataset size: 943074 samples\n",
      "\n",
      "Initializing models...\n",
      "\n",
      "===== Processing Model: RNN =====\n",
      "Model: RNN, Trainable Parameters: 3,932,432\n",
      "--- Starting Training for RNN ---\n",
      "Epoch 01/30 | LR: 0.000300 | Train Loss: 4.8696 | Val Loss: 4.5345 | Duration: 1785.97s\n",
      "  New best validation loss: 4.5345. Saving model state.\n",
      "Epoch 02/30 | LR: 0.000300 | Train Loss: 4.5810 | Val Loss: 4.4425 | Duration: 1801.90s\n",
      "  New best validation loss: 4.4425. Saving model state.\n",
      "Epoch 03/30 | LR: 0.000300 | Train Loss: 4.5035 | Val Loss: 4.3980 | Duration: 1798.34s\n",
      "  New best validation loss: 4.3980. Saving model state.\n",
      "Epoch 04/30 | LR: 0.000300 | Train Loss: 4.4569 | Val Loss: 4.3684 | Duration: 1787.83s\n",
      "  New best validation loss: 4.3684. Saving model state.\n",
      "Epoch 05/30 | LR: 0.000300 | Train Loss: 4.4247 | Val Loss: 4.3529 | Duration: 1798.05s\n",
      "  New best validation loss: 4.3529. Saving model state.\n",
      "Epoch 06/30 | LR: 0.000300 | Train Loss: 4.4014 | Val Loss: 4.3380 | Duration: 1788.24s\n",
      "  New best validation loss: 4.3380. Saving model state.\n",
      "Epoch 07/30 | LR: 0.000300 | Train Loss: 4.3840 | Val Loss: 4.3300 | Duration: 1797.36s\n",
      "  New best validation loss: 4.3300. Saving model state.\n",
      "Epoch 08/30 | LR: 0.000300 | Train Loss: 4.3704 | Val Loss: 4.3228 | Duration: 1793.47s\n",
      "  New best validation loss: 4.3228. Saving model state.\n",
      "Epoch 09/30 | LR: 0.000300 | Train Loss: 4.3594 | Val Loss: 4.3173 | Duration: 1785.61s\n",
      "  New best validation loss: 4.3173. Saving model state.\n",
      "Epoch 10/30 | LR: 0.000300 | Train Loss: 4.3503 | Val Loss: 4.3130 | Duration: 1779.59s\n",
      "  New best validation loss: 4.3130. Saving model state.\n",
      "Epoch 11/30 | LR: 0.000300 | Train Loss: 4.3425 | Val Loss: 4.3090 | Duration: 1777.52s\n",
      "  New best validation loss: 4.3090. Saving model state.\n",
      "Epoch 12/30 | LR: 0.000300 | Train Loss: 4.3359 | Val Loss: 4.3067 | Duration: 1772.49s\n",
      "  New best validation loss: 4.3067. Saving model state.\n",
      "Epoch 13/30 | LR: 0.000300 | Train Loss: 4.3299 | Val Loss: 4.3013 | Duration: 1799.96s\n",
      "  New best validation loss: 4.3013. Saving model state.\n",
      "Epoch 14/30 | LR: 0.000300 | Train Loss: 4.3246 | Val Loss: 4.3001 | Duration: 1803.64s\n",
      "  New best validation loss: 4.3001. Saving model state.\n",
      "Epoch 15/30 | LR: 0.000300 | Train Loss: 4.3200 | Val Loss: 4.2992 | Duration: 1812.63s\n",
      "  New best validation loss: 4.2992. Saving model state.\n",
      "Epoch 16/30 | LR: 0.000300 | Train Loss: 4.3158 | Val Loss: 4.2970 | Duration: 1790.89s\n",
      "  New best validation loss: 4.2970. Saving model state.\n",
      "Epoch 17/30 | LR: 0.000300 | Train Loss: 4.3120 | Val Loss: 4.2948 | Duration: 1759.28s\n",
      "  New best validation loss: 4.2948. Saving model state.\n",
      "Epoch 18/30 | LR: 0.000300 | Train Loss: 4.3086 | Val Loss: 4.2950 | Duration: 1759.11s\n",
      "  Validation loss did not improve for 1 epoch(s).\n",
      "Epoch 19/30 | LR: 0.000300 | Train Loss: 4.3055 | Val Loss: 4.2919 | Duration: 1762.74s\n",
      "  New best validation loss: 4.2919. Saving model state.\n",
      "Epoch 20/30 | LR: 0.000300 | Train Loss: 4.3025 | Val Loss: 4.2911 | Duration: 1735.04s\n",
      "  New best validation loss: 4.2911. Saving model state.\n",
      "Epoch 21/30 | LR: 0.000300 | Train Loss: 4.2996 | Val Loss: 4.2887 | Duration: 1733.16s\n",
      "  New best validation loss: 4.2887. Saving model state.\n",
      "Epoch 22/30 | LR: 0.000300 | Train Loss: 4.2969 | Val Loss: 4.2881 | Duration: 1726.43s\n",
      "  New best validation loss: 4.2881. Saving model state.\n",
      "Epoch 23/30 | LR: 0.000300 | Train Loss: 4.2942 | Val Loss: 4.2874 | Duration: 1732.64s\n",
      "  New best validation loss: 4.2874. Saving model state.\n",
      "Epoch 24/30 | LR: 0.000300 | Train Loss: 4.2918 | Val Loss: 4.2868 | Duration: 1726.88s\n",
      "  New best validation loss: 4.2868. Saving model state.\n",
      "Epoch 25/30 | LR: 0.000300 | Train Loss: 4.2894 | Val Loss: 4.2850 | Duration: 1732.02s\n",
      "  New best validation loss: 4.2850. Saving model state.\n",
      "Epoch 26/30 | LR: 0.000300 | Train Loss: 4.2872 | Val Loss: 4.2864 | Duration: 1726.69s\n",
      "  Validation loss did not improve for 1 epoch(s).\n",
      "Epoch 27/30 | LR: 0.000300 | Train Loss: 4.2850 | Val Loss: 4.2859 | Duration: 1747.34s\n",
      "  Validation loss did not improve for 2 epoch(s).\n",
      "Epoch 28/30 | LR: 0.000300 | Train Loss: 4.2829 | Val Loss: 4.2844 | Duration: 1757.23s\n",
      "  New best validation loss: 4.2844. Saving model state.\n",
      "Epoch 29/30 | LR: 0.000300 | Train Loss: 4.2809 | Val Loss: 4.2840 | Duration: 1754.67s\n",
      "  New best validation loss: 4.2840. Saving model state.\n",
      "Epoch 30/30 | LR: 0.000300 | Train Loss: 4.2791 | Val Loss: 4.2817 | Duration: 1757.64s\n",
      "  New best validation loss: 4.2817. Saving model state.\n",
      "--- Finished Training for RNN ---\n",
      "Total Training Time: 53084.49 seconds\n",
      "Loss curve saved as RNN_loss_curve.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Final RNN Model on Validation Set ---\n",
      "Calculating BLEU score on 50 validation samples...\n",
      "Average BLEU-50 score: 0.4464\n",
      "\n",
      "--- RNN Final Evaluation Summary ---\n",
      "  Validation Loss: 4.2817\n",
      "  Perplexity:        72.37\n",
      "  Token Accuracy:    25.73%\n",
      "  Avg BLEU Score:    0.4464 (Sampled, lower temp generation)\n",
      "  (Note: Perplexity/Accuracy reflect next-token prediction; BLEU reflects n-gram overlap in generated samples)\n",
      "\n",
      "--- Sample Generations ---\n",
      "Prompt: 'The best way to learn programming is'\n",
      "  Temp=0.5: The best way to learn programming is at the end of the same time, and so to be found in the most simple character of the most interesting. the commentators of the translation and the \"good, ah, in the text of sun tz. 50030m the commentators have been omitted. the\n",
      "  Temp=1.0: The best way to learn programming is inevitable. chapter xviii waria at last he closed her book on the ground, himself. the silence now was je vousas, peels on their boats. the fithe hour before their obeying the desert, while lios ix the line might alone cease to fill\n",
      "  Temp=1.5: The best way to learn programming is occupation. you shall treat so dispositions when very tempt to have had grown satisfactionthen greatly really irogany character x have'cked stone circle for you and absurd and careful of thoughtsmin toes long visit to imagine letting herself write her condition liked. countwhen he. ten minutes afterwards continued on x\n",
      "\n",
      "RNN model state dictionary saved to RNN_model_final.pt\n",
      "\n",
      "===== Processing Model: LSTM =====\n",
      "Model: LSTM, Trainable Parameters: 4,179,728\n",
      "--- Starting Training for LSTM ---\n",
      "Epoch 01/30 | LR: 0.000300 | Train Loss: 4.9585 | Val Loss: 4.4936 | Duration: 1983.57s\n",
      "  New best validation loss: 4.4936. Saving model state.\n",
      "Epoch 02/30 | LR: 0.000300 | Train Loss: 4.4982 | Val Loss: 4.3313 | Duration: 1998.27s\n",
      "  New best validation loss: 4.3313. Saving model state.\n",
      "Epoch 03/30 | LR: 0.000300 | Train Loss: 4.3685 | Val Loss: 4.2601 | Duration: 1974.95s\n",
      "  New best validation loss: 4.2601. Saving model state.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1440\u001b[39m\n\u001b[32m   1437\u001b[39m \u001b[38;5;66;03m# --- Script Entry Point ---\u001b[39;00m\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# Execute the main function when the script is run directly.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1440\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1272\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1268\u001b[39m scheduler = ReduceLROnPlateau(optimizer, mode=\u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, factor=lr_factor, patience=lr_patience)\n\u001b[32m   1270\u001b[39m \u001b[38;5;66;03m# --- Train the Model ---\u001b[39;00m\n\u001b[32m   1271\u001b[39m \u001b[38;5;66;03m# Calls the training function, which handles the epoch loop, early stopping, etc.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1272\u001b[39m train_losses, val_losses, training_time = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[38;5;66;03m# --- Plot Loss Curves ---\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[38;5;66;03m# Visualize the training and validation loss progression.\u001b[39;00m\n\u001b[32m   1280\u001b[39m plot_loss_curve(train_losses, val_losses, name)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 759\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, criterion, optimizer, scheduler, device, patience, model_name)\u001b[39m\n\u001b[32m    756\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Skip backward pass and optimizer step for this batch.\u001b[39;00m\n\u001b[32m    758\u001b[39m \u001b[38;5;66;03m# Backward pass: Compute gradients of the loss w.r.t. model parameters.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[38;5;66;03m# Gradient Clipping: Prevent exploding gradients by clipping the norm of gradients.\u001b[39;00m\n\u001b[32m    762\u001b[39m \u001b[38;5;66;03m# Helps stabilize training, especially with RNNs/LSTMs.\u001b[39;00m\n\u001b[32m    763\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/olojo/lib/python3.12/site-packages/torch/_tensor.py:570\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    567\u001b[39m     \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._tensor_str._str(\u001b[38;5;28mself\u001b[39m, tensor_contents=tensor_contents)\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(\n\u001b[32m    571\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient=\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph=\u001b[38;5;28;01mNone\u001b[39;00m, create_graph=\u001b[38;5;28;01mFalse\u001b[39;00m, inputs=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    572\u001b[39m ):\n\u001b[32m    573\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[32m    574\u001b[39m \n\u001b[32m    575\u001b[39m \u001b[33;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    614\u001b[39m \u001b[33;03m            used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Foundational AI Project 2 â€“ Language Modeling with RNNs, LSTMs, and Transformer (Graduate Version)\n",
    "\n",
    "This script trains three language models (RNN, LSTM, Transformer) for text generation.\n",
    "It uses a SentencePiece BPE tokenizer (vocab size=10000) to tokenize text from JSONL files,\n",
    "builds fixed-length sequences via a sliding window approach, and trains the models\n",
    "using early stopping with a ReduceLROnPlateau learning rate scheduler.\n",
    "Evaluation metrics include perplexity (exp(cross-entropy loss)), token accuracy,\n",
    "and BLEU score (computed with nltk on a sample). Sample outputs and loss curves with detailed plots\n",
    "are generated, and model performance is compared.\n",
    "Graduate-level requirements such as temperature-based decoding are supported in the prompt methods.\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # Learning rate scheduler that reduces LR when a metric plateaus\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sentencepiece as spm  # For BPE tokenization\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction # BLEU score computation with smoothing\n",
    "\n",
    "# --- NLTK Data Check ---\n",
    "# Ensure the 'punkt' tokenizer data (used by nltk.word_tokenize for BLEU) is available.\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading NLTK 'punkt' tokenizer data...\")\n",
    "    nltk.download('punkt', quiet=True) # Download quietly\n",
    "\n",
    "# --- Reproducibility ---\n",
    "# Set random seeds for Python, NumPy, and PyTorch to ensure reproducible results.\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# Ensure reproducibility on CUDA if available (can slightly slow down computation)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Device Selection ---\n",
    "# Select the appropriate computation device (GPU > MPS > CPU).\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(seed) # Set seed for all GPUs\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    # Check if MPS (Apple Silicon GPU) is available and functional.\n",
    "    try:\n",
    "        # Perform a simple tensor operation on MPS to verify usability.\n",
    "        torch.ones(1, device=\"mps\")\n",
    "        device = torch.device(\"mps\")\n",
    "    except Exception:\n",
    "        # Fallback to CPU if MPS check fails.\n",
    "        print(\"MPS device found but may not be usable. Falling back to CPU.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"[âœ“] Using device: {device}\")\n",
    "\n",
    "###############################################################################\n",
    "# Positional Encoding Module (for Transformer)\n",
    "###############################################################################\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects positional information into token embeddings using sinusoidal functions.\n",
    "    This allows the Transformer model, which lacks inherent sequence order awareness,\n",
    "    to utilize token position information. Based on 'Attention is All You Need'.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): The embedding dimension (d_model).\n",
    "        max_len (int): The maximum sequence length for which to precompute encodings.\n",
    "                       Should be at least as large as the longest sequence length.\n",
    "        dropout (float): Dropout rate applied after adding positional encodings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, max_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a matrix for positional encodings: [max_len, embed_dim]\n",
    "        pos_enc = torch.zeros(max_len, embed_dim)\n",
    "\n",
    "        # Create position indices: [max_len, 1] (tensor([[0.], [1.], ..., [max_len-1]]))\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Calculate the division term for the frequencies.\n",
    "        # Formula: 1 / (10000^(2i / embed_dim))\n",
    "        # Use log space for numerical stability: exp(-log(10000) * (2i / embed_dim))\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float) * -(math.log(10000.0) / embed_dim))\n",
    "\n",
    "        # Calculate sinusoidal encodings:\n",
    "        # Even indices (0, 2, 4, ...): PE(pos, 2i) = sin(pos / (10000^(2i / embed_dim)))\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Odd indices (1, 3, 5, ...): PE(pos, 2i+1) = cos(pos / (10000^(2i / embed_dim)))\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension: [1, max_len, embed_dim]\n",
    "        # This allows easy broadcasting when adding to batch embeddings.\n",
    "        pos_enc = pos_enc.unsqueeze(0)\n",
    "\n",
    "        # Register 'pos_enc' as a buffer. Buffers are part of the model's state_dict\n",
    "        # but are not considered model parameters (not updated by optimizer).\n",
    "        self.register_buffer(\"pos_enc\", pos_enc)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Adds positional encoding to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of token embeddings.\n",
    "                        Shape: [batch_size, seq_length, embed_dim].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor with positional information added.\n",
    "                    Shape: [batch_size, seq_length, embed_dim].\n",
    "        \"\"\"\n",
    "        # Add positional encoding to the input embeddings.\n",
    "        # Slice precomputed encodings 'pos_enc' to match the input sequence length (x.size(1)).\n",
    "        # Shape: [batch_size, seq_length, embed_dim] + [1, seq_length, embed_dim] -> [batch_size, seq_length, embed_dim]\n",
    "        x = x + self.pos_enc[:, :x.size(1)]\n",
    "\n",
    "        # Apply dropout for regularization.\n",
    "        return self.dropout(x)\n",
    "\n",
    "###############################################################################\n",
    "# Data Preparation Functions\n",
    "###############################################################################\n",
    "def train_tokenizer_if_needed(tokenizer_model_prefix: str = \"tokenizer\", vocab_size: int = 10000, training_text_file: str = \"merged_corpus.txt\") -> spm.SentencePieceProcessor:\n",
    "    \"\"\"\n",
    "    Trains a SentencePiece BPE (Byte-Pair Encoding) tokenizer on the provided text file\n",
    "    if the tokenizer model files (.model, .vocab) do not already exist.\n",
    "\n",
    "    Args:\n",
    "        tokenizer_model_prefix (str): The prefix for the output model and vocabulary files.\n",
    "        vocab_size (int): The target vocabulary size for the tokenizer.\n",
    "        training_text_file (str): The path to the plain text file used for training the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        spm.SentencePieceProcessor: An instance of the loaded SentencePiece processor.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the training text file does not exist and model files are missing.\n",
    "        Exception: If tokenizer training fails.\n",
    "    \"\"\"\n",
    "    model_path = f\"{tokenizer_model_prefix}.model\"\n",
    "    vocab_path = f\"{tokenizer_model_prefix}.vocab\"\n",
    "\n",
    "    # Check if both model and vocab files exist.\n",
    "    if not os.path.exists(model_path) or not os.path.exists(vocab_path):\n",
    "        print(f\"Tokenizer model ('{model_path}' or '{vocab_path}') not found. Training...\")\n",
    "        # Check if the training data file exists.\n",
    "        if not os.path.exists(training_text_file):\n",
    "             raise FileNotFoundError(f\"Tokenizer training file '{training_text_file}' not found. Cannot train tokenizer.\")\n",
    "\n",
    "        try:\n",
    "            # Train the SentencePiece model using the specified parameters.\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                input=training_text_file,          # Path to the training text file.\n",
    "                model_prefix=tokenizer_model_prefix, # Prefix for output files (.model, .vocab).\n",
    "                vocab_size=vocab_size,             # Target size of the vocabulary.\n",
    "                model_type=\"bpe\",                  # Use Byte-Pair Encoding algorithm.\n",
    "                character_coverage=1.0,            # Try to cover all characters in the training data.\n",
    "                # Define IDs for standard special tokens. SentencePiece uses these defaults if not specified.\n",
    "                # UNK (Unknown): Represents out-of-vocabulary words.\n",
    "                # BOS (Beginning-of-Sequence): Optional start token.\n",
    "                # EOS (End-of-Sequence): Marks the end of a sentence or sequence.\n",
    "                # PAD (Padding): Used to make sequences in a batch the same length.\n",
    "                unk_id=0,       # Typically ID 0 for <unk>\n",
    "                bos_id=1,       # Typically ID 1 for <s>\n",
    "                eos_id=2,       # Typically ID 2 for </s>\n",
    "                pad_id=3        # Explicitly set PAD ID to 3. If set to -1, PAD is disabled.\n",
    "            )\n",
    "            print(\"Tokenizer training complete.\")\n",
    "        except Exception as e:\n",
    "            # Handle potential errors during training.\n",
    "            print(f\"Error training tokenizer: {e}\")\n",
    "            # Clean up potentially incomplete model/vocab files if training failed.\n",
    "            if os.path.exists(model_path): os.remove(model_path)\n",
    "            if os.path.exists(vocab_path): os.remove(vocab_path)\n",
    "            raise # Re-raise the exception after cleanup.\n",
    "    else:\n",
    "        # If model files exist, skip training.\n",
    "        print(f\"Found existing tokenizer model: {model_path}\")\n",
    "\n",
    "    # Load the trained tokenizer model from the file.\n",
    "    sp = spm.SentencePieceProcessor(model_file=model_path)\n",
    "    # Print tokenizer details for verification.\n",
    "    print(f\"Tokenizer loaded. Vocab size: {sp.vocab_size()}. Special IDs: \"\n",
    "          f\"UNK={sp.unk_id()}, BOS={sp.bos_id()}, EOS={sp.eos_id()}, PAD={sp.pad_id()}\")\n",
    "    return sp\n",
    "\n",
    "def load_and_tokenize(file_path: str, sp: spm.SentencePieceProcessor) -> list[int]:\n",
    "    \"\"\"\n",
    "    Loads text data from a JSONL file (each line is a JSON object), extracts \"prompt\"\n",
    "    and \"completion\" fields, combines them, concatenates all entries using the\n",
    "    tokenizer's EOS token as a separator, and tokenizes the entire text into a single\n",
    "    list of token IDs.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file.\n",
    "        sp (SentencePieceProcessor): The initialized SentencePiece tokenizer instance.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A flat list of token IDs representing the tokenized content of the file.\n",
    "    \"\"\"\n",
    "    texts = [] # List to hold individual text entries (prompt + completion)\n",
    "    count = 0  # Counter for total lines processed\n",
    "\n",
    "    # Read the JSONL file line by line.\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            try:\n",
    "                # Attempt to parse the line as a JSON object.\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip lines that are not valid JSON.\n",
    "                # print(f\"Warning: Skipping invalid JSON line in {file_path} (line {count})\")\n",
    "                continue\n",
    "            # Extract \"prompt\" and \"completion\" fields, defaulting to empty strings if missing.\n",
    "            prompt = obj.get(\"prompt\", \"\")\n",
    "            completion = obj.get(\"completion\", \"\")\n",
    "            # Combine prompt and completion, stripping leading/trailing whitespace.\n",
    "            text = (prompt + \" \" + completion).strip()\n",
    "            # Add the combined text to the list if it's not empty.\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "\n",
    "    # --- Determine the EOS token string for separation ---\n",
    "    eos_id = sp.eos_id() # Get the integer ID for the EOS token.\n",
    "    eos_string = \"\"      # Initialize EOS string representation.\n",
    "\n",
    "    if eos_id is not None and eos_id >= 0: # Check if a valid EOS ID exists.\n",
    "        try:\n",
    "            # Decode the EOS ID back to its string representation (e.g., \"</s>\").\n",
    "            # Need to pass it as a list to decode.\n",
    "            eos_string = sp.decode([eos_id])\n",
    "            # Handle cases where the decoded special token might be empty or unexpected.\n",
    "            # Use common default '</s>' if ID is 2 and decode is empty.\n",
    "            if not eos_string and eos_id == 2:\n",
    "                eos_string = \"</s>\"\n",
    "            # If still empty after checks, issue a warning and fallback to newline.\n",
    "            if not eos_string:\n",
    "                 print(f\"Warning: Decoded EOS token ID {eos_id} resulted in an empty string. Using newline fallback.\")\n",
    "                 eos_string = \"\\n\"\n",
    "        except Exception as e:\n",
    "             # Handle errors during decoding and fallback to newline.\n",
    "             print(f\"Warning: Could not decode EOS token ID {eos_id}. Error: {e}. Using newline fallback.\")\n",
    "             eos_string = \"\\n\"\n",
    "    else:\n",
    "        # If no EOS token ID is defined in the tokenizer, use newline as separator.\n",
    "        print(\"Warning: EOS token ID not found in tokenizer. Using newline as separator.\")\n",
    "        eos_string = \"\\n\"\n",
    "\n",
    "    # Define the separator string, adding newlines around the EOS token for clarity.\n",
    "    # Strip potential whitespace from the decoded token itself.\n",
    "    separator = f\"\\n{eos_string.strip()}\\n\"\n",
    "\n",
    "    # Join all extracted text entries into a single large string using the separator.\n",
    "    # This creates one continuous corpus for sequence building.\n",
    "    combined = separator.join(texts)\n",
    "    # --- End of EOS handling and text combination ---\n",
    "\n",
    "    print(f\"Loaded {len(texts)} text entries from {file_path} (out of {count} lines). \"\n",
    "          f\"Total combined length: {len(combined)} characters\")\n",
    "\n",
    "    # Tokenize the entire combined text into a list of integer IDs.\n",
    "    token_ids = sp.encode(combined, out_type=int)\n",
    "    print(f\"Tokenized into {len(token_ids)} tokens.\")\n",
    "    return token_ids\n",
    "\n",
    "def build_sequences(token_ids: list[int], seq_length: int) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Creates overlapping sequences of a fixed length from a flat list of token IDs\n",
    "    using a sliding window approach. Each sequence has length `seq_length + 1`\n",
    "    to facilitate creating input/target pairs (input = seq[:-1], target = seq[1:]).\n",
    "\n",
    "    Args:\n",
    "        token_ids (list[int]): The flat list of token IDs from the tokenized corpus.\n",
    "        seq_length (int): The desired length of the *input* sequences. The generated\n",
    "                          sequences will have length `seq_length + 1`.\n",
    "\n",
    "    Returns:\n",
    "        list[list[int]]: A list of sequences, where each sequence is a list of token IDs\n",
    "                         of length `seq_length + 1`. Returns an empty list if no\n",
    "                         valid sequences can be created.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `seq_length` is not positive.\n",
    "    \"\"\"\n",
    "    # Validate seq_length.\n",
    "    if seq_length <= 0:\n",
    "        raise ValueError(\"seq_length must be positive.\")\n",
    "    # Check if there are enough tokens to form even one sequence.\n",
    "    if len(token_ids) <= seq_length:\n",
    "         print(f\"Warning: Token list length ({len(token_ids)}) is not greater than seq_length ({seq_length}). \"\n",
    "               \"Cannot generate sequences.\")\n",
    "         return []\n",
    "\n",
    "    # Use a list comprehension for efficient sliding window creation.\n",
    "    # For each starting index `i`, take a slice of length `seq_length + 1`.\n",
    "    # Stop when the slice would go beyond the end of the `token_ids` list.\n",
    "    sequences = [token_ids[i : i + seq_length + 1] for i in range(len(token_ids) - seq_length)]\n",
    "    return sequences\n",
    "\n",
    "###############################################################################\n",
    "# Custom Dataset Class for Language Modeling\n",
    "###############################################################################\n",
    "class LanguageModelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for language modeling. It takes a list of token sequences\n",
    "    (each of length `seq_length + 1`) and provides input/target pairs for training.\n",
    "    Each sample retrieved by `__getitem__` is a tuple (input_tokens, target_tokens),\n",
    "    where `target_tokens` are the `input_tokens` shifted one position to the right.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences: list[list[int]], seq_length: int):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            sequences (list[list[int]]): A list of token sequences, where each sequence\n",
    "                                         is expected to have length `seq_length + 1`.\n",
    "            seq_length (int): The length of the input sequences (targets will also have this length).\n",
    "        \"\"\"\n",
    "        # Create input/target pairs: input = sequence[:-1], target = sequence[1:]\n",
    "        # Include a check to filter out any sequences that might not have the exact required length.\n",
    "        # This should ideally not happen if `build_sequences` is used correctly, but acts as a safeguard.\n",
    "        self.samples = [(seq[:-1], seq[1:]) for seq in sequences if len(seq) == seq_length + 1]\n",
    "\n",
    "        # Report if any sequences were filtered out due to incorrect length.\n",
    "        if len(self.samples) < len(sequences):\n",
    "             print(f\"Warning: Filtered out {len(sequences) - len(self.samples)} sequences due to \"\n",
    "                   f\"incorrect length (expected {seq_length + 1}).\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the total number of samples (input/target pairs) in the dataset.\"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieves the sample (input/target pair) at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "                - input_tokens (torch.Tensor): Tensor of input token IDs (dtype=torch.long).\n",
    "                - target_tokens (torch.Tensor): Tensor of target token IDs (dtype=torch.long).\n",
    "        \"\"\"\n",
    "        inp_ids, target_ids = self.samples[idx]\n",
    "        # Convert the lists of token IDs to PyTorch tensors of type long.\n",
    "        return torch.tensor(inp_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "###############################################################################\n",
    "# Text Generation Function and Model Definitions\n",
    "###############################################################################\n",
    "def generate_text(model: nn.Module, tokenizer: spm.SentencePieceProcessor, prompt_text: str, max_length: int = 128, temperature: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Generates text autoregressively starting from a given prompt using the provided model.\n",
    "\n",
    "    Supports temperature-based sampling for controlling randomness.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained language model (RNN, LSTM, or Transformer).\n",
    "        tokenizer (SentencePieceProcessor): The tokenizer used for encoding/decoding.\n",
    "        prompt_text (str): The initial text to seed the generation.\n",
    "        max_length (int): The maximum number of *new* tokens to generate after the prompt.\n",
    "        temperature (float): Controls the randomness of sampling.\n",
    "                             - temperature=0 (or close to 0): Greedy decoding (always pick the most likely token).\n",
    "                             - temperature=1.0: Standard sampling from the model's predicted probabilities.\n",
    "                             - temperature > 1.0: Increases randomness, makes less likely tokens more probable.\n",
    "                             - 0 < temperature < 1.0: Decreases randomness, favors more likely tokens.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text, including the original prompt.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode (disables dropout, etc.).\n",
    "    model.eval()\n",
    "\n",
    "    # Get the model's device (CPU or GPU).\n",
    "    model_device = next(model.parameters()).device\n",
    "\n",
    "    # Encode the prompt text into token IDs.\n",
    "    # `add_bos=False`, `add_eos=False`: Assume BOS/EOS are handled during sequence creation/training, not needed here.\n",
    "    token_ids = tokenizer.encode(prompt_text, out_type=int, add_bos=False, add_eos=False)\n",
    "    generated = token_ids.copy() # Start the generated sequence with the prompt tokens.\n",
    "\n",
    "    # Get the EOS token ID from the tokenizer.\n",
    "    eos_token_id = tokenizer.eos_id()\n",
    "    # Check if EOS token is defined; use -1 if not (won't match any valid token).\n",
    "    if eos_token_id is None or eos_token_id < 0:\n",
    "        print(\"Warning: EOS token ID not found in tokenizer. Generation might not stop via EOS.\")\n",
    "        eos_token_id = -1 # Use a value that won't be generated.\n",
    "\n",
    "    # Autoregressive generation loop: generate one token at a time.\n",
    "    with torch.no_grad(): # Disable gradient calculations during inference.\n",
    "        # Note: RNN/LSTM state handling could be added here if the model's forward pass requires/returns state.\n",
    "        # This implementation assumes the model handles state internally or is stateless (like the Transformer here).\n",
    "        for _ in range(max_length):\n",
    "            # Prepare the current sequence as input to the model.\n",
    "            # Shape: [1, current_sequence_length] (add batch dimension).\n",
    "            input_ids = torch.tensor([generated], dtype=torch.long, device=model_device)\n",
    "\n",
    "            # --- Model Forward Pass ---\n",
    "            # Get logits from the model.\n",
    "            # Shape: [batch_size=1, sequence_length, vocab_size]\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            # --- Get Logits for Next Token ---\n",
    "            # We only need the logits for the very last token position to predict the next token.\n",
    "            # Shape: [vocab_size]\n",
    "            next_logits = logits[0, -1, :]\n",
    "\n",
    "            # --- Token Sampling ---\n",
    "            # Apply temperature scaling and sample the next token.\n",
    "            if temperature < 1e-5:\n",
    "                # Greedy decoding: Select the token with the highest logit.\n",
    "                next_token = torch.argmax(next_logits).item()\n",
    "            else:\n",
    "                # Temperature sampling:\n",
    "                # Scale logits by temperature. Lower temp -> sharper distribution, higher temp -> flatter distribution.\n",
    "                scaled_logits = next_logits / temperature\n",
    "                # Apply softmax to convert scaled logits to probabilities.\n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "                # Sample one token according to the calculated probabilities.\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            # --- Stopping Condition ---\n",
    "            # Stop generation if the EOS token is sampled.\n",
    "            if next_token == eos_token_id:\n",
    "                break\n",
    "\n",
    "            # Append the sampled token ID to the generated sequence.\n",
    "            generated.append(next_token)\n",
    "\n",
    "            # Optional: Implement sequence length limits based on model's max length\n",
    "            # if hasattr(model, 'max_seq_length') and len(generated) > model.max_seq_length:\n",
    "            #     generated = generated[-model.max_seq_length:] # Truncate from the beginning\n",
    "\n",
    "    # Decode the final list of token IDs (including prompt) back into a string.\n",
    "    # `skip_special_tokens=False` (default) will include decoded special tokens like </s> if present.\n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple language model based on vanilla Recurrent Neural Networks (RNNs).\n",
    "\n",
    "    Architecture:\n",
    "        1. Embedding Layer: Maps input token IDs to dense vectors.\n",
    "        2. Dropout Layer: Applied after embedding.\n",
    "        3. RNN Layer(s): Processes the sequence of embeddings.\n",
    "        4. Dropout Layer: Applied after RNN output.\n",
    "        5. Linear Layer (FC): Maps RNN hidden states to vocabulary logits.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embed_dim (int): Dimension of token embeddings.\n",
    "        hidden_dim (int): Dimension of RNN hidden states.\n",
    "        num_layers (int): Number of stacked RNN layers.\n",
    "        dropout (float): Dropout probability for dropout layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_layers: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # `batch_first=True`: Input/output tensors have shape [batch, seq, feature].\n",
    "        # Dropout is applied between RNN layers if num_layers > 1.\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers,\n",
    "                          batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        # Fully connected layer to project RNN output to vocabulary size.\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        # Separate dropout layer applied after embedding and RNN output.\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the RNN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of token IDs. Shape: [batch_size, seq_length].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of logits. Shape: [batch_size, seq_length, vocab_size].\n",
    "        \"\"\"\n",
    "        # 1. Embeddings\n",
    "        # Shape: [batch_size, seq_length] -> [batch_size, seq_length, embed_dim]\n",
    "        embeds = self.dropout_layer(self.embedding(x))\n",
    "\n",
    "        # 2. RNN\n",
    "        # `output` contains hidden states for all time steps. Shape: [batch, seq_length, hidden_dim]\n",
    "        # `_` holds the final hidden state (h_n). We don't need it directly for logits.\n",
    "        output, _ = self.rnn(embeds)\n",
    "        # Apply dropout to the RNN output sequence.\n",
    "        output = self.dropout_layer(output)\n",
    "\n",
    "        # 3. Fully Connected Layer\n",
    "        # Project hidden states to vocabulary logits.\n",
    "        # Shape: [batch, seq_length, hidden_dim] -> [batch, seq_length, vocab_size]\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "    def prompt(self, tokenizer: spm.SentencePieceProcessor, prompt_text: str, max_length: int = 128, temperature: float = 1.0) -> str:\n",
    "        \"\"\" Convenience method to generate text using this model. \"\"\"\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A language model based on Long Short-Term Memory (LSTM) networks.\n",
    "\n",
    "    Architecture is similar to RNNLanguageModel, but uses LSTM layers which are\n",
    "    generally better at capturing long-range dependencies.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embed_dim (int): Dimension of token embeddings.\n",
    "        hidden_dim (int): Dimension of LSTM hidden and cell states.\n",
    "        num_layers (int): Number of stacked LSTM layers.\n",
    "        dropout (float): Dropout probability for dropout layers and between LSTM layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_layers: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # `batch_first=True`: Input/output tensors have shape [batch, seq, feature].\n",
    "        # Dropout is applied between LSTM layers if num_layers > 1.\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        # Fully connected layer to project LSTM output to vocabulary size.\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        # Separate dropout layer applied after embedding and LSTM output.\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the LSTM model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of token IDs. Shape: [batch_size, seq_length].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of logits. Shape: [batch_size, seq_length, vocab_size].\n",
    "        \"\"\"\n",
    "        # 1. Embeddings\n",
    "        # Shape: [batch_size, seq_length] -> [batch_size, seq_length, embed_dim]\n",
    "        embeds = self.dropout_layer(self.embedding(x))\n",
    "\n",
    "        # 2. LSTM\n",
    "        # `output` contains hidden states for all time steps. Shape: [batch, seq_length, hidden_dim]\n",
    "        # `_` holds the final hidden state (h_n) and cell state (c_n). We don't need them directly for logits.\n",
    "        output, _ = self.lstm(embeds)\n",
    "        # Apply dropout to the LSTM output sequence.\n",
    "        output = self.dropout_layer(output)\n",
    "\n",
    "        # 3. Fully Connected Layer\n",
    "        # Project hidden states to vocabulary logits.\n",
    "        # Shape: [batch, seq_length, hidden_dim] -> [batch, seq_length, vocab_size]\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "    def prompt(self, tokenizer: spm.SentencePieceProcessor, prompt_text: str, max_length: int = 128, temperature: float = 1.0) -> str:\n",
    "        \"\"\" Convenience method to generate text using this model. \"\"\"\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A language model based on the Transformer architecture (specifically, the Encoder part).\n",
    "    Uses self-attention mechanism to capture dependencies between tokens, potentially\n",
    "    handling long-range dependencies better than RNNs/LSTMs. Includes positional encoding\n",
    "    and a causal mask for autoregressive generation.\n",
    "\n",
    "    Architecture:\n",
    "        1. Embedding Layer: Maps token IDs to vectors. Embeddings are scaled.\n",
    "        2. Positional Encoding: Adds positional information to embeddings.\n",
    "        3. Transformer Encoder: Consists of multiple Transformer Encoder Layers.\n",
    "           - Each layer has Multi-Head Self-Attention and a Feed-Forward Network.\n",
    "           - A causal mask is applied to ensure autoregressive property (attend only to past tokens).\n",
    "        4. Linear Layer (FC): Maps Transformer output to vocabulary logits.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embed_dim (int): Dimension of token embeddings (d_model).\n",
    "        num_heads (int): Number of attention heads in Multi-Head Self-Attention. Must divide embed_dim.\n",
    "        hidden_dim (int): Dimension of the feed-forward network inside Transformer layers.\n",
    "        num_layers (int): Number of stacked Transformer Encoder layers.\n",
    "        max_seq_length (int): Maximum sequence length the model can handle (used for causal mask).\n",
    "        dropout (float): Dropout probability used in positional encoding and Transformer layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, num_heads: int, hidden_dim: int, num_layers: int, max_seq_length: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        # Ensure embedding dimension is divisible by the number of heads.\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=max_seq_length, dropout=dropout)\n",
    "\n",
    "        # Define a single Transformer Encoder layer.\n",
    "        # `batch_first=True` ensures input/output shapes are [batch, seq, feature].\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
    "                                                   dim_feedforward=hidden_dim, dropout=dropout,\n",
    "                                                   batch_first=True) # Crucial for shape consistency\n",
    "        # Stack multiple encoder layers.\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Fully connected layer to project Transformer output to vocabulary size.\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        # Store embedding dimension and max sequence length for later use.\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # --- Causal Mask ---\n",
    "        # Generate and register a causal (subsequent) mask. This prevents attention\n",
    "        # to future positions, which is essential for autoregressive language modeling.\n",
    "        # The mask shape will be [max_seq_length, max_seq_length].\n",
    "        self.register_buffer('causal_mask', self._generate_square_subsequent_mask(max_seq_length))\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates a square causal mask of size [sz, sz].\n",
    "        Masked positions (future tokens) are filled with float('-inf'),\n",
    "        unmasked positions (current and past tokens) are filled with float(0.0).\n",
    "        The Transformer layer adds this mask to the attention scores before softmax.\n",
    "        \"\"\"\n",
    "        # Create an upper triangle matrix of 1s.\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        # Fill with -inf where mask is 0 (future positions), and 0.0 where mask is 1 (current/past).\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of token IDs. Shape: [batch_size, seq_length].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of logits. Shape: [batch_size, seq_length, vocab_size].\n",
    "        \"\"\"\n",
    "        # Get the actual sequence length from the input batch.\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # --- Prepare Mask ---\n",
    "        # Select the appropriate part of the precomputed causal mask for the current sequence length.\n",
    "        # The mask needs to be on the same device as the input tensor `x`.\n",
    "        # Shape required by TransformerEncoder: [seq_len, seq_len]\n",
    "        current_mask = self.causal_mask[:seq_len, :seq_len].to(x.device)\n",
    "\n",
    "        # --- Embeddings and Positional Encoding ---\n",
    "        # Convert token IDs to embeddings. Shape: [batch, seq_len] -> [batch, seq_len, embed_dim]\n",
    "        # Scale embeddings by sqrt(embed_dim) as suggested in the 'Attention is All You Need' paper.\n",
    "        embeds = self.embedding(x) * math.sqrt(self.embed_dim)\n",
    "        # Add positional encodings. Shape remains [batch, seq_len, embed_dim].\n",
    "        encoded = self.pos_encoder(embeds)\n",
    "\n",
    "        # --- Transformer Encoder ---\n",
    "        # Pass the encoded sequence through the Transformer encoder layers.\n",
    "        # The `mask` argument ensures causal attention.\n",
    "        # Input shape (due to batch_first=True): [batch, seq_len, embed_dim]\n",
    "        # Output shape: [batch, seq_len, embed_dim]\n",
    "        transformer_out = self.transformer_encoder(encoded, mask=current_mask)\n",
    "\n",
    "        # --- Fully Connected Layer ---\n",
    "        # Project Transformer output to vocabulary logits.\n",
    "        # Shape: [batch, seq_len, embed_dim] -> [batch, seq_len, vocab_size]\n",
    "        logits = self.fc(transformer_out)\n",
    "        return logits\n",
    "\n",
    "    def prompt(self, tokenizer: spm.SentencePieceProcessor, prompt_text: str, max_length: int = 128, temperature: float = 1.0) -> str:\n",
    "        \"\"\" Convenience method to generate text using this model. \"\"\"\n",
    "        # The forward pass already incorporates the causal mask, so generate_text works correctly.\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "###############################################################################\n",
    "# Training, Evaluation, and Plotting Functions\n",
    "###############################################################################\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, num_epochs: int,\n",
    "                criterion: nn.Module, optimizer: optim.Optimizer, scheduler: ReduceLROnPlateau,\n",
    "                device: torch.device, patience: int = 5, model_name: str = \"Model\") -> tuple[list, list, float]:\n",
    "    \"\"\"\n",
    "    Trains the language model using the provided data loaders and hyperparameters.\n",
    "    Implements mini-batch gradient descent, gradient clipping, ReduceLROnPlateau\n",
    "    learning rate scheduling, and early stopping based on validation loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The language model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        num_epochs (int): The maximum number of epochs to train for.\n",
    "        criterion (nn.Module): The loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer (optim.Optimizer): The optimizer (e.g., AdamW).\n",
    "        scheduler (ReduceLROnPlateau): The learning rate scheduler.\n",
    "        device (torch.device): The device (CPU/GPU) to train on.\n",
    "        patience (int): Number of epochs to wait for validation loss improvement before stopping early.\n",
    "        model_name (str): Name of the model (used for logging).\n",
    "\n",
    "    Returns:\n",
    "        tuple[list, list, float]: A tuple containing:\n",
    "            - train_losses (list): List of average training losses per epoch.\n",
    "            - val_losses (list): List of average validation losses per epoch.\n",
    "            - total_training_time (float): Total time taken for training in seconds.\n",
    "    \"\"\"\n",
    "    # Move the model to the specified device.\n",
    "    model.to(device)\n",
    "\n",
    "    # Lists to store loss values for plotting.\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    # Variables for early stopping.\n",
    "    best_val_loss = float('inf') # Initialize with infinity; lower is better.\n",
    "    best_model_state = None      # To store the state_dict of the best model.\n",
    "    epochs_no_improve = 0        # Counter for epochs without validation loss improvement.\n",
    "\n",
    "    print(f\"--- Starting Training for {model_name} ---\")\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # --- Training Phase ---\n",
    "        model.train() # Set the model to training mode (enables dropout, etc.).\n",
    "        total_train_loss = 0.0\n",
    "        num_batches = 0\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            # Move batch data to the target device.\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Zero the gradients accumulated from the previous batch.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: Get model predictions (logits).\n",
    "            # Shape: [batch, seq_length, vocab_size]\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            # Reshape outputs and targets for CrossEntropyLoss:\n",
    "            # Outputs: [batch * seq_length, vocab_size]\n",
    "            # Targets: [batch * seq_length]\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "            # Check for NaN loss, which can indicate instability.\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"Warning: NaN loss detected at epoch {epoch}, batch {batch_idx}. Skipping batch update.\")\n",
    "                # Consider logging more details or stopping if NaNs persist.\n",
    "                continue # Skip backward pass and optimizer step for this batch.\n",
    "\n",
    "            # Backward pass: Compute gradients of the loss w.r.t. model parameters.\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Clipping: Prevent exploding gradients by clipping the norm of gradients.\n",
    "            # Helps stabilize training, especially with RNNs/LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update model parameters using the computed gradients.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss for averaging later.\n",
    "            total_train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Optional: Log progress within an epoch.\n",
    "            # if (batch_idx + 1) % 100 == 0:\n",
    "            #     print(f\"  Epoch {epoch}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Calculate average training loss for the epoch.\n",
    "        if num_batches == 0:\n",
    "             print(\"Warning: No batches processed in training epoch {epoch}. Check data loader.\")\n",
    "             avg_train_loss = 0.0\n",
    "        else:\n",
    "            avg_train_loss = total_train_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval() # Set the model to evaluation mode (disables dropout, etc.).\n",
    "        total_val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "        with torch.no_grad(): # Disable gradient calculations during validation.\n",
    "            for inputs, targets in val_loader:\n",
    "                # Move batch data to the target device.\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                # Forward pass.\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate loss.\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "                # Check for NaN loss during validation.\n",
    "                if torch.isnan(loss):\n",
    "                     print(f\"Warning: NaN loss detected during validation epoch {epoch}. Skipping batch contribution.\")\n",
    "                     continue\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                num_val_batches += 1\n",
    "\n",
    "        # Calculate average validation loss for the epoch.\n",
    "        if num_val_batches == 0:\n",
    "            print(f\"Warning: No batches processed in validation epoch {epoch}. Check validation loader.\")\n",
    "            avg_val_loss = float('inf') # Assign infinity if no validation batches.\n",
    "        else:\n",
    "            avg_val_loss = total_val_loss / num_val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        epoch_duration = time.time() - epoch_start\n",
    "        # Get the current learning rate from the optimizer.\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Print epoch summary.\n",
    "        print(f\"Epoch {epoch:02d}/{num_epochs} | LR: {current_lr:.6f} | Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | Duration: {epoch_duration:.2f}s\")\n",
    "\n",
    "        # --- Learning Rate Scheduling ---\n",
    "        # Step the scheduler based on the average validation loss.\n",
    "        # ReduceLROnPlateau will decrease LR if `avg_val_loss` hasn't improved for `scheduler.patience` epochs.\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # --- Early Stopping Logic ---\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            # Validation loss improved.\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0 # Reset the counter.\n",
    "            # Save the model state dictionary of the best performing model so far.\n",
    "            # Clone tensors to CPU to avoid GPU memory issues when loading later.\n",
    "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            print(f\"  New best validation loss: {best_val_loss:.4f}. Saving model state.\")\n",
    "            # Optional: Save the best model checkpoint to disk immediately.\n",
    "            # torch.save(best_model_state, f\"{model_name}_best_checkpoint.pt\")\n",
    "        else:\n",
    "            # Validation loss did not improve.\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n",
    "            # Check if patience limit is reached.\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch} epochs due to no improvement for {patience} epochs.\")\n",
    "                # Load the best model state if it was saved.\n",
    "                if best_model_state:\n",
    "                    print(\"Restoring best model state...\")\n",
    "                    # Ensure the model is on the correct device before loading state dict keys might need mapping if device changed\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                else:\n",
    "                    # This case should ideally not happen if training ran for at least one epoch where loss improved.\n",
    "                    print(\"Warning: Early stopping triggered, but no best model state was saved (potential issue).\")\n",
    "                break # Exit the training loop.\n",
    "\n",
    "    # --- End of Training Loop ---\n",
    "    total_training_time = time.time() - total_start_time\n",
    "    print(f\"--- Finished Training for {model_name} ---\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
    "\n",
    "    # If early stopping was triggered, ensure the best model state is loaded before returning.\n",
    "    # This check handles the case where training finishes exactly on the patience limit.\n",
    "    if epochs_no_improve >= patience and best_model_state:\n",
    "        # Check if the current model state is already the best one (it should be if restored in the loop)\n",
    "        # This load is potentially redundant but safe.\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Ensured best model state is loaded after early stopping.\")\n",
    "\n",
    "    return train_losses, val_losses, total_training_time\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, data_loader: DataLoader, criterion: nn.Module, device: torch.device) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on a given dataset (e.g., validation or test set).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained language model.\n",
    "        data_loader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        criterion (nn.Module): The loss function used during training.\n",
    "        device (torch.device): The device (CPU/GPU) to run evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss over the entire dataset. Returns float('inf') if evaluation fails.\n",
    "    \"\"\"\n",
    "    model.eval() # Set the model to evaluation mode.\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad(): # Disable gradient calculations.\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Reshape for loss calculation.\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            # Accumulate loss if it's valid.\n",
    "            if not torch.isnan(loss):\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            else:\n",
    "                 print(\"Warning: NaN loss encountered during final evaluation.\")\n",
    "\n",
    "    # Calculate average loss.\n",
    "    if num_batches == 0:\n",
    "        print(\"Error: No batches processed during evaluation. Check data loader.\")\n",
    "        return float('inf') # Indicate error or empty dataset.\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def compute_perplexity(loss: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculates perplexity from the average cross-entropy loss.\n",
    "    Perplexity = exp(average_loss). Lower perplexity indicates a better model fit.\n",
    "\n",
    "    Args:\n",
    "        loss (float): The average cross-entropy loss.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated perplexity. Returns float('inf') if loss is invalid or too large.\n",
    "    \"\"\"\n",
    "    # Handle invalid loss values.\n",
    "    if loss is None or loss == float('inf') or loss < 0:\n",
    "        return float('inf')\n",
    "    try:\n",
    "        # Calculate perplexity using the exponential function.\n",
    "        perplexity = math.exp(loss)\n",
    "        return perplexity\n",
    "    except OverflowError:\n",
    "        # Handle cases where the loss is extremely large, causing exp() to overflow.\n",
    "        print(f\"Warning: Loss value {loss} too large, resulting in perplexity overflow.\")\n",
    "        return float('inf')\n",
    "\n",
    "def compute_token_accuracy(model: nn.Module, data_loader: DataLoader, device: torch.device) -> float:\n",
    "    \"\"\"\n",
    "    Computes the token-level accuracy of the model on a given dataset.\n",
    "    Accuracy = (Number of correctly predicted tokens) / (Total number of tokens).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained language model.\n",
    "        data_loader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): The device (CPU/GPU) to run evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        float: The token accuracy (between 0.0 and 1.0).\n",
    "    \"\"\"\n",
    "    model.eval() # Set the model to evaluation mode.\n",
    "    correct = 0 # Counter for correctly predicted tokens.\n",
    "    total = 0   # Counter for total tokens evaluated.\n",
    "    with torch.no_grad(): # Disable gradient calculations.\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Get model logits. Shape: [batch, seq_len, vocab_size]\n",
    "            outputs = model(inputs)\n",
    "            # Get the index of the highest logit for each position (predicted token ID).\n",
    "            # Shape: [batch, seq_len]\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            # Compare predictions with the actual target tokens element-wise.\n",
    "            # Sum up the number of correct predictions in the batch.\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            # Add the total number of tokens in the batch targets to the total count.\n",
    "            total += targets.numel()\n",
    "\n",
    "    # Calculate accuracy, handling the case of zero total tokens.\n",
    "    if total == 0:\n",
    "        print(\"Warning: Zero tokens found in accuracy calculation. Check data loader.\")\n",
    "        return 0.0\n",
    "    return correct / total\n",
    "\n",
    "def compute_bleu(reference: str, candidate: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes the BLEU (Bilingual Evaluation Understudy) score between a candidate\n",
    "    (generated) sentence and a reference sentence using NLTK.\n",
    "\n",
    "    Note: BLEU is primarily designed for machine translation and measures n-gram precision.\n",
    "    Its applicability to open-ended text generation can be limited, but it provides\n",
    "    a rough measure of lexical overlap.\n",
    "\n",
    "    Args:\n",
    "        reference (str): The ground truth sentence (target).\n",
    "        candidate (str): The generated sentence (prediction).\n",
    "\n",
    "    Returns:\n",
    "        float: The BLEU score (typically between 0 and 1). Returns 0.0 if candidate\n",
    "               or reference is empty or if an error occurs.\n",
    "    \"\"\"\n",
    "    # Return 0 if either string is empty, as BLEU requires content.\n",
    "    if not candidate or not reference:\n",
    "        return 0.0\n",
    "\n",
    "    # Use a smoothing function (Chen & Cherry method 1) to handle cases where\n",
    "    # higher-order n-grams (e.g., 4-grams) might not appear in the candidate,\n",
    "    # preventing a score of 0 just because of short sentences or lack of overlap.\n",
    "    chencherry = SmoothingFunction()\n",
    "\n",
    "    try:\n",
    "        # Tokenize the reference and candidate sentences using NLTK's word tokenizer.\n",
    "        # `sentence_bleu` expects references as a list of lists of tokens.\n",
    "        ref_tokens = [nltk.word_tokenize(reference.lower())] # Use lowercase for case-insensitive comparison\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "\n",
    "        # Calculate BLEU score (defaults to BLEU-4, considering 1 to 4-grams).\n",
    "        bleu = sentence_bleu(ref_tokens, cand_tokens, smoothing_function=chencherry.method1)\n",
    "    except Exception as e:\n",
    "        # Catch potential errors during tokenization or BLEU calculation.\n",
    "        print(f\"Warning: Could not compute BLEU score. Error: {e}\")\n",
    "        # Print truncated versions for debugging.\n",
    "        print(f\"  Reference (truncated): '{reference[:100]}...'\")\n",
    "        print(f\"  Candidate (truncated): '{candidate[:100]}...'\")\n",
    "        bleu = 0.0 # Return 0.0 on error.\n",
    "    return bleu\n",
    "\n",
    "def plot_loss_curve(train_losses: list[float], val_losses: list[float], model_name: str):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss curves over epochs and saves the plot to a file.\n",
    "    Includes annotations for minimum validation loss and final loss values.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list[float]): List of average training losses per epoch.\n",
    "        val_losses (list[float]): List of average validation losses per epoch.\n",
    "        model_name (str): Name of the model (used for title and filename).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6)) # Set figure size.\n",
    "    epochs = range(1, len(train_losses) + 1) # X-axis values (epochs).\n",
    "\n",
    "    # Plot training and validation losses.\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o', linestyle='-', linewidth=2)\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\", marker='s', linestyle='-', linewidth=2)\n",
    "\n",
    "    # Add labels and title.\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Loss\", fontsize=12)\n",
    "    plt.title(f\"{model_name} Training & Validation Loss Curve\", fontsize=14)\n",
    "\n",
    "    # Add legend.\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    # Add grid for better readability.\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # --- Annotations ---\n",
    "    # Find minimum validation loss and the epoch it occurred at.\n",
    "    if val_losses: # Ensure val_losses is not empty.\n",
    "        min_val_loss = min(val_losses)\n",
    "        min_val_epoch = val_losses.index(min_val_loss) + 1 # Add 1 for 1-based epoch indexing.\n",
    "        # Add an annotation arrow pointing to the minimum validation loss point.\n",
    "        plt.annotate(f\"Min Val Loss: {min_val_loss:.4f} at Epoch {min_val_epoch}\",\n",
    "                     xy=(min_val_epoch, min_val_loss), # Point coordinates\n",
    "                     xytext=(min_val_epoch + 0.5, min_val_loss + 0.1 * min_val_loss), # Text position (offset)\n",
    "                     arrowprops=dict(facecolor='red', shrink=0.05, width=1, headwidth=8), # Arrow style\n",
    "                     fontsize=10, color='red')\n",
    "\n",
    "    # Annotate final loss values at the last epoch.\n",
    "    if len(epochs) > 0:\n",
    "        final_epoch = epochs[-1]\n",
    "        final_train_loss = train_losses[-1]\n",
    "        final_val_loss = val_losses[-1]\n",
    "        # Use scatter points to highlight the final values.\n",
    "        # `zorder=5` ensures points are plotted on top of lines.\n",
    "        plt.scatter(final_epoch, final_train_loss, color='blue', s=50, zorder=5,\n",
    "                    label=f\"Final Train: {final_train_loss:.4f}\")\n",
    "        plt.scatter(final_epoch, final_val_loss, color='orange', s=50, zorder=5,\n",
    "                    label=f\"Final Val: {final_val_loss:.4f}\")\n",
    "        plt.legend() # Update legend to include scatter point labels.\n",
    "\n",
    "    # Adjust layout to prevent labels from overlapping.\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to a file.\n",
    "    filename = f\"{model_name}_loss_curve.png\"\n",
    "    plt.savefig(filename, dpi=300) # Save with high resolution.\n",
    "    print(f\"Loss curve saved as {filename}\")\n",
    "\n",
    "    # Display the plot.\n",
    "    plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# Main Training and Evaluation Pipeline\n",
    "###############################################################################\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the entire process:\n",
    "    1. Sets up hyperparameters and file paths.\n",
    "    2. Prepares the tokenizer (trains if necessary).\n",
    "    3. Loads and preprocesses data (tokenization, sequence building).\n",
    "    4. Creates DataLoaders.\n",
    "    5. Initializes RNN, LSTM, and Transformer models.\n",
    "    6. Trains each model with evaluation, LR scheduling, and early stopping.\n",
    "    7. Calculates final metrics (perplexity, accuracy, BLEU).\n",
    "    8. Generates sample text from each model.\n",
    "    9. Saves trained models.\n",
    "    10. Compares model performance using tables and plots.\n",
    "    \"\"\"\n",
    "    global_start = time.time() # Record start time for total duration.\n",
    "\n",
    "    # ------------------ Hyperparameters ------------------\n",
    "    # These values can be tuned based on the specific dataset, task, and available hardware.\n",
    "    vocab_size = 10000         # Target vocabulary size for the BPE tokenizer.\n",
    "    embed_dim = 256            # Dimensionality of token embeddings.\n",
    "    hidden_dim = 128           # Hidden dimension for RNN/LSTM and feed-forward layer in Transformer.\n",
    "                               # Increased from original script for potentially more capacity.\n",
    "    num_layers = 2             # Number of layers in RNN/LSTM/Transformer encoder stacks.\n",
    "    num_heads = 8              # Number of attention heads in Transformer (must divide embed_dim).\n",
    "    max_seq_length = 128       # Maximum length of input sequences fed to the models.\n",
    "    batch_size = 128            # Number of sequences per training batch. Smaller batches can sometimes help generalization\n",
    "                               # and reduce memory usage, but may slow down training.\n",
    "    num_epochs = 30            # Maximum number of training epochs. Early stopping might finish sooner.\n",
    "    learning_rate = 3e-4       # Initial learning rate for the AdamW optimizer.\n",
    "    dropout_rate = 0.2         # Dropout rate for regularization in models.\n",
    "    weight_decay = 0.01        # Weight decay (L2 regularization) for the AdamW optimizer. Helps prevent overfitting.\n",
    "    # Note: pad_token_id is determined dynamically from the tokenizer later.\n",
    "\n",
    "    # ReduceLROnPlateau Scheduler Parameters\n",
    "    lr_patience = 2            # Number of epochs with no improvement in validation loss before reducing LR.\n",
    "    lr_factor = 0.5            # Factor by which the learning rate will be reduced (new_lr = lr * factor).\n",
    "\n",
    "    # Early Stopping Parameters\n",
    "    early_stopping_patience = 5 # Number of epochs with no improvement in validation loss before stopping training.\n",
    "\n",
    "    # ------------------ File Paths ------------------\n",
    "    train_file = \"train.jsonl\"                  # Path to the training data JSONL file.\n",
    "    test_file = \"test.jsonl\"                    # Path to the validation/test data JSONL file.\n",
    "    tokenizer_model_prefix = \"bpe_tokenizer_v1\" # Prefix for saving/loading the tokenizer model.\n",
    "    tokenizer_training_file = \"merged_corpus.txt\" # Path to the text file used FOR training the tokenizer.\n",
    "                                                  # This file will be created if it doesn't exist.\n",
    "\n",
    "    # --- File Existence Checks ---\n",
    "    if not os.path.exists(train_file):\n",
    "        raise FileNotFoundError(f\"Training data file not found: {train_file}\")\n",
    "    if not os.path.exists(test_file):\n",
    "        raise FileNotFoundError(f\"Validation/Test data file not found: {test_file}\")\n",
    "\n",
    "    # ------------------ Prepare Training Text for Tokenizer (if needed) ------------------\n",
    "    # Check if the tokenizer model already exists. If not, prepare the training text file.\n",
    "    if not os.path.exists(f\"{tokenizer_model_prefix}.model\"):\n",
    "        # If the consolidated training text file doesn't exist, create it from the train_file.\n",
    "        if not os.path.exists(tokenizer_training_file):\n",
    "            print(f\"Creating combined text file for tokenizer training: {tokenizer_training_file}\")\n",
    "            texts = []\n",
    "            # Read the training JSONL file.\n",
    "            with open(train_file, \"r\", encoding=\"utf-8\") as f_in:\n",
    "                for line in f_in:\n",
    "                    try:\n",
    "                        # Parse each line as JSON.\n",
    "                        obj = json.loads(line)\n",
    "                        # Extract prompt and completion.\n",
    "                        prompt = obj.get(\"prompt\", \"\")\n",
    "                        completion = obj.get(\"completion\", \"\")\n",
    "                        # Combine and strip whitespace.\n",
    "                        text = (prompt + \" \" + completion).strip()\n",
    "                        # Add non-empty text to the list.\n",
    "                        if text:\n",
    "                            texts.append(text)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Skip invalid JSON lines.\n",
    "                        continue\n",
    "            # Join all texts with newlines and write to the tokenizer training file.\n",
    "            combined_text = \"\\n\".join(texts)\n",
    "            with open(tokenizer_training_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "                f_out.write(combined_text)\n",
    "            print(f\"Saved combined text ({len(texts)} entries) to {tokenizer_training_file}\")\n",
    "        else:\n",
    "            # Use the existing file if found.\n",
    "            print(f\"Using existing tokenizer training file: {tokenizer_training_file}\")\n",
    "\n",
    "    # ------------------ Tokenizer Preparation ------------------\n",
    "    # Train the tokenizer if needed, or load the existing one.\n",
    "    sp = train_tokenizer_if_needed(tokenizer_model_prefix=tokenizer_model_prefix,\n",
    "                                    vocab_size=vocab_size,\n",
    "                                    training_text_file=tokenizer_training_file)\n",
    "\n",
    "    # Determine the padding token ID from the loaded tokenizer.\n",
    "    pad_token_id = sp.pad_id()\n",
    "    print(f\"Using Padding Token ID: {pad_token_id}\")\n",
    "\n",
    "    # Set the `ignore_index` for the CrossEntropyLoss function.\n",
    "    # If the tokenizer defined a PAD token (pad_id >= 0), ignore it during loss calculation.\n",
    "    # Otherwise, use the PyTorch default ignore_index (-100), which assumes no padding ID needs ignoring.\n",
    "    ignore_index = pad_token_id if pad_token_id >= 0 else -100\n",
    "\n",
    "    # Get the actual vocabulary size from the trained tokenizer (might differ slightly from target).\n",
    "    effective_vocab_size = sp.vocab_size()\n",
    "    print(f\"Effective vocabulary size: {effective_vocab_size}\")\n",
    "\n",
    "    # ------------------ Load and Tokenize Datasets ------------------\n",
    "    print(\"\\nLoading and tokenizing datasets...\")\n",
    "    # Tokenize the training and validation/test files into flat lists of token IDs.\n",
    "    train_tokens = load_and_tokenize(train_file, sp)\n",
    "    val_tokens = load_and_tokenize(test_file, sp)\n",
    "\n",
    "    # ------------------ Build Fixed-Length Token Sequences ------------------\n",
    "    print(\"\\nBuilding sequences...\")\n",
    "    # Create overlapping sequences of length `max_seq_length + 1`.\n",
    "    train_seqs = build_sequences(train_tokens, max_seq_length)\n",
    "    val_seqs = build_sequences(val_tokens, max_seq_length)\n",
    "    print(f\"Number of training sequences created: {len(train_seqs)}\")\n",
    "    print(f\"Number of validation sequences created: {len(val_seqs)}\")\n",
    "\n",
    "    # Check if sequence creation was successful.\n",
    "    if not train_seqs or not val_seqs:\n",
    "        raise ValueError(\"Failed to create training or validation sequences. \"\n",
    "                         \"Check data loading, tokenization, and sequence length.\")\n",
    "\n",
    "    # ------------------ Create Dataset Objects and DataLoaders ------------------\n",
    "    print(\"\\nCreating Dataset objects and DataLoaders...\")\n",
    "    # Instantiate custom Dataset objects.\n",
    "    train_dataset = LanguageModelDataset(train_seqs, max_seq_length)\n",
    "    val_dataset = LanguageModelDataset(val_seqs, max_seq_length)\n",
    "    print(f\"Train dataset size: {len(train_dataset)} samples\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)} samples\")\n",
    "\n",
    "    # --- DataLoader Configuration ---\n",
    "    # `num_workers`: Number of subprocesses for data loading. `0` means data is loaded in the main process.\n",
    "    # Using `0` is often safer on Windows and macOS and simplifies debugging.\n",
    "    # Higher values can speed up loading on Linux systems with multiple cores, especially if preprocessing is heavy.\n",
    "    num_workers = 0\n",
    "    # `pin_memory`: If True and using GPU, copies tensors into pinned memory before returning them.\n",
    "    # Can speed up data transfer from CPU to GPU.\n",
    "    pin_memory = True if device.type == \"cuda\" else False\n",
    "\n",
    "    # Create DataLoader for training data.\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True, # Shuffle training data each epoch.\n",
    "                              num_workers=num_workers,\n",
    "                              pin_memory=pin_memory)\n",
    "    # Create DataLoader for validation data.\n",
    "    val_loader = DataLoader(val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False, # No need to shuffle validation data.\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=pin_memory)\n",
    "\n",
    "    # ------------------ Initialize Models ------------------\n",
    "    print(\"\\nInitializing models...\")\n",
    "    # Create instances of the three models. Use the effective vocabulary size.\n",
    "    models = {\n",
    "        \"RNN\": RNNLanguageModel(effective_vocab_size, embed_dim, hidden_dim, num_layers, dropout=dropout_rate),\n",
    "        \"LSTM\": LSTMLanguageModel(effective_vocab_size, embed_dim, hidden_dim, num_layers, dropout=dropout_rate),\n",
    "        \"Transformer\": TransformerLanguageModel(effective_vocab_size, embed_dim, num_heads, hidden_dim,\n",
    "                                               num_layers, max_seq_length, dropout=dropout_rate)\n",
    "    }\n",
    "\n",
    "    # Dictionary to store evaluation results for each model.\n",
    "    model_results = {}\n",
    "\n",
    "    # ------------------ Train, Evaluate and Compare Models ------------------\n",
    "    # Loop through each model type (RNN, LSTM, Transformer).\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n===== Processing Model: {name} =====\")\n",
    "        # Move model to the target device.\n",
    "        model.to(device)\n",
    "\n",
    "        # Count and print the number of trainable parameters.\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Model: {name}, Trainable Parameters: {num_params:,}\")\n",
    "\n",
    "        # --- Setup for Training ---\n",
    "        # Loss Function: CrossEntropyLoss is standard for multi-class classification (predicting the next token).\n",
    "        # `ignore_index` ensures that padding tokens (if used) do not contribute to the loss.\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "\n",
    "        # Optimizer: AdamW is Adam with decoupled weight decay, often preferred for Transformers.\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # Learning Rate Scheduler: Reduces LR when validation loss stops improving.\n",
    "        # `mode='min'`: Reduce LR when the monitored quantity (val loss) stops decreasing.\n",
    "        # `factor`: Multiplicative factor for LR reduction.\n",
    "        # `patience`: Number of epochs to wait before reducing LR.\n",
    "        # `verbose=False`: Removed deprecated argument. Progress is logged manually.\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=lr_factor, patience=lr_patience)\n",
    "\n",
    "        # --- Train the Model ---\n",
    "        # Calls the training function, which handles the epoch loop, early stopping, etc.\n",
    "        train_losses, val_losses, training_time = train_model(\n",
    "            model, train_loader, val_loader, num_epochs,\n",
    "            criterion, optimizer, scheduler, device,\n",
    "            patience=early_stopping_patience, model_name=name\n",
    "        )\n",
    "\n",
    "        # --- Plot Loss Curves ---\n",
    "        # Visualize the training and validation loss progression.\n",
    "        plot_loss_curve(train_losses, val_losses, name)\n",
    "\n",
    "        # --- Final Evaluation ---\n",
    "        # Evaluate the final model (best one loaded by `train_model` if early stopping occurred) on the validation set.\n",
    "        print(f\"\\n--- Evaluating Final {name} Model on Validation Set ---\")\n",
    "        final_val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "        # Calculate final perplexity and token accuracy based on the final validation loss.\n",
    "        perplexity = compute_perplexity(final_val_loss)\n",
    "        token_accuracy = compute_token_accuracy(model, val_loader, device) * 100 # Convert to percentage.\n",
    "\n",
    "        # --- BLEU Score Calculation ---\n",
    "        # Calculate BLEU score on a sample of the validation set for an indicative measure.\n",
    "        num_bleu_samples = 50 # Number of samples to use for BLEU calculation.\n",
    "        total_bleu = 0.0\n",
    "        bleu_samples_evaluated = 0\n",
    "        if len(val_dataset) > 0:\n",
    "            print(f\"Calculating BLEU score on {min(num_bleu_samples, len(val_dataset))} validation samples...\")\n",
    "            # Ensure we don't try to sample more items than available in the validation set.\n",
    "            indices_to_sample = random.sample(range(len(val_dataset)), k=min(num_bleu_samples, len(val_dataset)))\n",
    "\n",
    "            for idx in indices_to_sample:\n",
    "                # Get input and target tensors for the sample.\n",
    "                sample_input_tokens, sample_target_tokens = val_dataset[idx]\n",
    "                # Decode tokens back to text for prompt and reference.\n",
    "                # Convert tensors to lists before decoding.\n",
    "                prompt_text = sp.decode(sample_input_tokens.tolist())\n",
    "                reference_text = sp.decode(sample_target_tokens.tolist()) # The ground truth completion/next sequence.\n",
    "\n",
    "                # Generate text using the trained model.\n",
    "                # Use a moderate temperature (e.g., 0.7) for generation, as greedy decoding (temp~0)\n",
    "                # might produce repetitive text, potentially lowering BLEU.\n",
    "                generated_text = model.prompt(sp, prompt_text, max_length=max_seq_length, temperature=0.7)\n",
    "\n",
    "                # Note: The generated text includes the prompt. The reference_text corresponds to the target sequence (shifted input).\n",
    "                # For a fairer comparison, one might extract only the generated part after the prompt,\n",
    "                # but this simple approach compares the full generated sequence against the target sequence.\n",
    "                bleu_score_sample = compute_bleu(reference_text, generated_text)\n",
    "                total_bleu += bleu_score_sample\n",
    "                bleu_samples_evaluated += 1\n",
    "\n",
    "            # Calculate the average BLEU score over the samples.\n",
    "            avg_bleu = total_bleu / bleu_samples_evaluated if bleu_samples_evaluated > 0 else 0.0\n",
    "            print(f\"Average BLEU-{num_bleu_samples} score: {avg_bleu:.4f}\")\n",
    "        else:\n",
    "            # Handle case where validation dataset is empty.\n",
    "            avg_bleu = 0.0\n",
    "            print(\"Validation dataset is empty, cannot compute BLEU score.\")\n",
    "        # --- End BLEU Score Calculation ---\n",
    "\n",
    "        # --- Print Evaluation Summary ---\n",
    "        print(f\"\\n--- {name} Final Evaluation Summary ---\")\n",
    "        print(f\"  Validation Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"  Perplexity:        {perplexity:.2f}\")\n",
    "        print(f\"  Token Accuracy:    {token_accuracy:.2f}%\")\n",
    "        print(f\"  Avg BLEU Score:    {avg_bleu:.4f} (Sampled, lower temp generation)\")\n",
    "        print(\"  (Note: Perplexity/Accuracy reflect next-token prediction; BLEU reflects n-gram overlap in generated samples)\")\n",
    "\n",
    "        # --- Generate Sample Outputs ---\n",
    "        # Generate text from a fixed prompt with different temperatures to observe behavior.\n",
    "        fixed_prompt = \"The best way to learn programming is\"\n",
    "        print(\"\\n--- Sample Generations ---\")\n",
    "        print(f\"Prompt: '{fixed_prompt}'\")\n",
    "        for temp in [0.5, 1.0, 1.5]: # Low, standard, high temperatures\n",
    "            sample_output = model.prompt(sp, fixed_prompt, max_length=60, temperature=temp) # Generate shorter samples\n",
    "            print(f\"  Temp={temp:.1f}: {sample_output}\")\n",
    "\n",
    "        # --- Save the Trained Model ---\n",
    "        # Save the state dictionary of the final (best) model.\n",
    "        model_save_path = f\"{name}_model_final.pt\"\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"\\n{name} model state dictionary saved to {model_save_path}\")\n",
    "\n",
    "        # --- Store Results ---\n",
    "        # Store the key metrics for this model in the results dictionary.\n",
    "        model_results[name] = {\n",
    "            \"ValLoss\": final_val_loss,\n",
    "            \"Perplexity\": perplexity,\n",
    "            \"Token Accuracy (%)\": token_accuracy,\n",
    "            \"BLEU Score\": avg_bleu,\n",
    "            \"Training Time (s)\": training_time,\n",
    "            \"Parameters\": num_params\n",
    "        }\n",
    "\n",
    "    # ------------------ Compare Model Performance ------------------\n",
    "    print(\"\\n===== Model Performance Comparison =====\")\n",
    "    # Define table header.\n",
    "    header = f\"{'Model':<15} {'Parameters':<15} {'Perplexity':<12} {'Token Acc (%)':<15} {'BLEU Score':<12} {'Train Time (s)':<15}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header)) # Print separator line.\n",
    "    # Print metrics for each model.\n",
    "    for model_name, metrics in model_results.items():\n",
    "        print(f\"{model_name:<15} {metrics['Parameters']:, <14} {metrics['Perplexity']:<12.2f} \"\n",
    "              f\"{metrics['Token Accuracy (%)']:<15.2f} {metrics['BLEU Score']:<12.4f} {metrics['Training Time (s)']:<15.2f}\")\n",
    "\n",
    "    # --- Create Comparative Bar Plots ---\n",
    "    # Extract metrics for plotting.\n",
    "    model_names = list(model_results.keys())\n",
    "    perplexities = [model_results[m][\"Perplexity\"] for m in model_names]\n",
    "    accuracies = [model_results[m][\"Token Accuracy (%)\"] for m in model_names]\n",
    "    bleu_scores = [model_results[m][\"BLEU Score\"] for m in model_names]\n",
    "    train_times = [model_results[m][\"Training Time (s)\"] for m in model_names]\n",
    "    # params = [model_results[m][\"Parameters\"] for m in model_names] # Could also plot parameters\n",
    "\n",
    "    # Plotting setup.\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10)) # Create a 2x2 grid of subplots.\n",
    "    x = np.arange(len(model_names)) # X-axis positions for bars.\n",
    "    width = 0.3 # Width of the bars.\n",
    "\n",
    "    # Plot 1: Perplexity (Lower is better)\n",
    "    bars1 = axs[0, 0].bar(x, perplexities, width, color=\"skyblue\", label='Perplexity')\n",
    "    axs[0, 0].set_ylabel('Perplexity')\n",
    "    axs[0, 0].set_title('Validation Perplexity (Lower is Better)')\n",
    "    axs[0, 0].set_xticks(x)\n",
    "    axs[0, 0].set_xticklabels(model_names)\n",
    "    axs[0, 0].bar_label(bars1, fmt='%.2f') # Add labels on top of bars.\n",
    "\n",
    "    # Plot 2: Token Accuracy (Higher is better)\n",
    "    bars2 = axs[0, 1].bar(x, accuracies, width, color=\"lightgreen\", label='Accuracy')\n",
    "    axs[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axs[0, 1].set_title('Validation Token Accuracy (Higher is Better)')\n",
    "    axs[0, 1].set_xticks(x)\n",
    "    axs[0, 1].set_xticklabels(model_names)\n",
    "    # Adjust y-axis limits for better visualization if values are close.\n",
    "    if accuracies:\n",
    "        axs[0, 1].set_ylim(bottom=max(0, min(accuracies) - 5), top=min(100, max(accuracies) + 5))\n",
    "    axs[0, 1].bar_label(bars2, fmt='%.2f')\n",
    "\n",
    "    # Plot 3: BLEU Score (Higher is better, context dependent)\n",
    "    bars3 = axs[1, 0].bar(x, bleu_scores, width, color=\"salmon\", label='BLEU')\n",
    "    axs[1, 0].set_ylabel('BLEU Score')\n",
    "    axs[1, 0].set_title(f'Average BLEU-{num_bleu_samples} Score (Higher is Better)')\n",
    "    axs[1, 0].set_xticks(x)\n",
    "    axs[1, 0].set_xticklabels(model_names)\n",
    "    axs[1, 0].bar_label(bars3, fmt='%.4f')\n",
    "\n",
    "    # Plot 4: Training Time\n",
    "    bars4 = axs[1, 1].bar(x, train_times, width, color=\"plum\", label='Time')\n",
    "    axs[1, 1].set_ylabel('Time (s)')\n",
    "    axs[1, 1].set_title('Total Training Time')\n",
    "    axs[1, 1].set_xticks(x)\n",
    "    axs[1, 1].set_xticklabels(model_names)\n",
    "    axs[1, 1].bar_label(bars4, fmt='%.1f')\n",
    "\n",
    "    # Add overall title and adjust layout.\n",
    "    plt.suptitle(\"Model Performance Comparison\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make space for suptitle.\n",
    "\n",
    "    # Save the comparison plot.\n",
    "    comparison_plot_path = \"model_performance_comparison.png\"\n",
    "    plt.savefig(comparison_plot_path, dpi=300)\n",
    "    print(f\"\\nComparison plot saved as {comparison_plot_path}\")\n",
    "    plt.show() # Display the plot.\n",
    "\n",
    "    # --- Final Timing ---\n",
    "    total_time = time.time() - global_start\n",
    "    print(f\"\\nScript finished. Total elapsed time: {total_time / 60:.2f} minutes\")\n",
    "\n",
    "# --- Script Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the main function when the script is run directly.\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'RNNLanguageModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencePieceProcessor\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mRNNLanguageModel\u001b[39;00m  \u001b[38;5;66;03m# replace with the name of the module/cell where your class lives\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# --- Device Selection (reuse the same logic you had) ---\u001b[39;00m\n\u001b[1;32m     12\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'RNNLanguageModel'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Generate RNN sample generations for the prompt â€œWhich do you prefer? Dogs or cats?â€\n",
    "Using your trained RNNLanguageModel and BPE tokenizer.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "# --- Device Selection (reuse your logic) ---\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# --- Hyperparameters (must match your training) ---\n",
    "vocab_size   = effective_vocab_size  # e.g. 10000\n",
    "embed_dim    = embed_dim            # e.g. 256\n",
    "hidden_dim   = hidden_dim           # e.g. 128\n",
    "num_layers   = num_layers           # e.g. 2\n",
    "dropout_rate = dropout_rate         # e.g. 0.2\n",
    "\n",
    "# --- 1) Re-create the RNN model and load its weights from the uploaded file ---\n",
    "model = RNNLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout_rate\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load(\"RNN_model_final.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "# --- 2) Load your BPE tokenizer ---\n",
    "sp = SentencePieceProcessor(model_file=\"bpe_tokenizer_v1.model\")\n",
    "\n",
    "# --- 3) Generate and print samples for the new prompt ---\n",
    "new_prompt = \"Which do you prefer? Dogs or cats?\"\n",
    "print(f\"\\n--- RNN generations for: '{new_prompt}' ---\")\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    out = model.prompt(sp, new_prompt, max_length=60, temperature=temp)\n",
    "    print(f\"  Temp={temp:.1f}: {out}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
