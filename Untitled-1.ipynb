{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mubaraqolojo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data.txt\n",
      "  input_format: \n",
      "  model_prefix: tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training SentencePiece tokenizer on data.txt...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Not found: \"data.txt\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 440\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample BLEU score (RNN model): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 440\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[7], line 369\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining SentencePiece tokenizer on data.txt...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# Ensure that the file 'data.txt' exists and contains your dataset text.\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     spm\u001b[38;5;241m.\u001b[39mSentencePieceTrainer\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    370\u001b[0m                                    model_prefix\u001b[38;5;241m=\u001b[39mtokenizer_model_prefix,\n\u001b[1;32m    371\u001b[0m                                    vocab_size\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[1;32m    372\u001b[0m                                    model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbpe\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    373\u001b[0m                                    character_coverage\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# Load the trained tokenizer\u001b[39;00m\n\u001b[1;32m    375\u001b[0m sp \u001b[38;5;241m=\u001b[39m spm\u001b[38;5;241m.\u001b[39mSentencePieceProcessor(model_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_model_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentencepiece/__init__.py:1047\u001b[0m, in \u001b[0;36mSentencePieceTrainer.Train\u001b[0;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTrain\u001b[39m(arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logstream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1046\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m _LogStream(ostream\u001b[38;5;241m=\u001b[39mlogstream):\n\u001b[0;32m-> 1047\u001b[0m     SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_Train(arg\u001b[38;5;241m=\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentencepiece/__init__.py:1040\u001b[0m, in \u001b[0;36mSentencePieceTrainer._Train\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_TrainFromMap2(new_kwargs, sentence_iterator)\n\u001b[1;32m   1039\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_TrainFromMap(new_kwargs)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentencepiece/__init__.py:985\u001b[0m, in \u001b[0;36mSentencePieceTrainer._TrainFromMap\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_TrainFromMap\u001b[39m(args):\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sentencepiece\u001b[38;5;241m.\u001b[39mSentencePieceTrainer__TrainFromMap(args)\n",
      "\u001b[0;31mOSError\u001b[0m: Not found: \"data.txt\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Foundational AI Project 2\n",
    "CSC 7700/4700: Implement language models for text generation using PyTorch.\n",
    "This script implements three models:\n",
    "  - A vanilla RNN-based language model\n",
    "  - An LSTM-based language model\n",
    "  - A Transformer-based language model\n",
    "\n",
    "The project workflow is as follows:\n",
    "1. Train a SentencePiece BPE tokenizer (vocab size 10,000) on a text dataset (\"data.txt\").\n",
    "2. Convert the text into token IDs and prepare a dataset using sliding windows.\n",
    "3. Define the three model architectures, each with:\n",
    "    • an embedding layer,\n",
    "    • hidden layers (RNN, LSTM, or transformer encoders),\n",
    "    • and a fully connected output layer.\n",
    "4. Each model implements:\n",
    "    • forward: returns vocabulary token probabilities for each time step,\n",
    "    • prompt: autoregressively generates text given an input prompt.\n",
    "5. Train each model using CrossEntropyLoss and AdamW with a learning rate scheduler.\n",
    "6. Plot training/validation loss curves, compute perplexity, and demonstrate text generation.\n",
    "7. (For BLEU score evaluation, a sample computation is provided.)\n",
    "\n",
    "Author: [Mubaraq Olojo]\n",
    "Date: [April 2024]\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Make sure NLTK packages are downloaded (especially for BLEU)\n",
    "nltk.download('punkt')\n",
    "\n",
    "########################################################################\n",
    "        # Utility: Positional Encoding for Transformer\n",
    "########################################################################\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the positional encoding as described in \"Attention is All You Need.\"\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create constant 'pe' matrix with values dependent on position and dimension.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        # Compute the positional encodings once in log space.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_length, d_model)\n",
    "        Returns:\n",
    "            Tensor with positional encodings added.\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "########################################################################\n",
    "        # Dataset for Language Modeling\n",
    "########################################################################\n",
    "class LanguageModelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Prepares sequences for language modeling.\n",
    "    Each sample is a sequence of length `seq_length` (input) and the next token as target.\n",
    "    We use a sliding window approach.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, seq_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: List of lists containing token IDs.\n",
    "            seq_length: The length of input sequence (target is the next token).\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length\n",
    "        self.samples = []\n",
    "        # Each sequence in 'sequences' is assumed to be of length seq_length+1 (input + target)\n",
    "        for seq in sequences:\n",
    "            if len(seq) == seq_length + 1:\n",
    "                self.samples.append((seq[:-1], seq[1:]))  # input sequence and target sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp, target = self.samples[idx]\n",
    "        return torch.tensor(inp, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "################################################################\n",
    "# Model Definitions\n",
    "################################################################\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla RNN-based language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Use batch_first=True so that input shape is (batch, seq_length, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_length)\n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size, seq_length, vocab_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # (batch, seq_length, embed_dim)\n",
    "        output, _ = self.rnn(embedded)  # (batch, seq_length, hidden_dim)\n",
    "        logits = self.fc(output)  # (batch, seq_length, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def prompt(self, tokenizer, prompt_text, max_length=50, temperature=0.0):\n",
    "        \"\"\"\n",
    "        Generates text given a prompt.\n",
    "        For undergraduates, temperature=0 uses greedy (argmax) sampling.\n",
    "        For graduate students, a nonzero temperature can be used for stochastic sampling.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        # Tokenize input prompt\n",
    "        token_ids = tokenizer.encode(prompt_text, out_type=int)\n",
    "        input_ids = torch.tensor([token_ids], dtype=torch.long).to(next(self.parameters()).device)\n",
    "        generated = token_ids.copy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                logits = self.forward(input_ids)  # (1, seq_length, vocab_size)\n",
    "                next_token_logits = logits[0, -1, :]  # logits for the last time step\n",
    "\n",
    "                if temperature == 0:\n",
    "                    # Greedy sampling: choose the token with highest probability\n",
    "                    next_token = torch.argmax(next_token_logits).item()\n",
    "                else:\n",
    "                    # Temperature-based sampling (for grad students)\n",
    "                    next_token_logits = next_token_logits / temperature\n",
    "                    probs = torch.softmax(next_token_logits, dim=0)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "                generated.append(next_token)\n",
    "                # Stop if end-of-sequence token is generated\n",
    "                if next_token == tokenizer.eos_id():\n",
    "                    break\n",
    "                # Append the new token and continue generation\n",
    "                input_ids = torch.tensor([generated], dtype=torch.long).to(next(self.parameters()).device)\n",
    "\n",
    "        # Decode the generated tokens back to text\n",
    "        return tokenizer.decode(generated)\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # (batch, seq_length, embed_dim)\n",
    "        output, _ = self.lstm(embedded)  # (batch, seq_length, hidden_dim)\n",
    "        logits = self.fc(output)  # (batch, seq_length, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def prompt(self, tokenizer, prompt_text, max_length=50, temperature=0.0):\n",
    "        self.eval()\n",
    "        token_ids = tokenizer.encode(prompt_text, out_type=int)\n",
    "        input_ids = torch.tensor([token_ids], dtype=torch.long).to(next(self.parameters()).device)\n",
    "        generated = token_ids.copy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                logits = self.forward(input_ids)\n",
    "                next_token_logits = logits[0, -1, :]\n",
    "\n",
    "                if temperature == 0:\n",
    "                    next_token = torch.argmax(next_token_logits).item()\n",
    "                else:\n",
    "                    next_token_logits = next_token_logits / temperature\n",
    "                    probs = torch.softmax(next_token_logits, dim=0)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "                generated.append(next_token)\n",
    "                if next_token == tokenizer.eos_id():\n",
    "                    break\n",
    "                input_ids = torch.tensor([generated], dtype=torch.long).to(next(self.parameters()).device)\n",
    "        return tokenizer.decode(generated)\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_seq_length, dropout=0.1):\n",
    "        super(TransformerLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout, max_seq_length)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
    "                                                   dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_length)\n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size, seq_length, vocab_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # (batch, seq_length, embed_dim)\n",
    "        encoded = self.pos_encoder(embedded)  # add positional encoding\n",
    "        # Transformer expects input shape: (seq_length, batch_size, embed_dim)\n",
    "        encoded = encoded.transpose(0, 1)\n",
    "        transformer_output = self.transformer_encoder(encoded)\n",
    "        transformer_output = transformer_output.transpose(0, 1)  # back to (batch, seq_length, embed_dim)\n",
    "        logits = self.fc(transformer_output)\n",
    "        return logits\n",
    "\n",
    "    def prompt(self, tokenizer, prompt_text, max_length=50, temperature=0.0):\n",
    "        self.eval()\n",
    "        token_ids = tokenizer.encode(prompt_text, out_type=int)\n",
    "        input_ids = torch.tensor([token_ids], dtype=torch.long).to(next(self.parameters()).device)\n",
    "        generated = token_ids.copy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                logits = self.forward(input_ids)\n",
    "                next_token_logits = logits[0, -1, :]\n",
    "\n",
    "                if temperature == 0:\n",
    "                    next_token = torch.argmax(next_token_logits).item()\n",
    "                else:\n",
    "                    next_token_logits = next_token_logits / temperature\n",
    "                    probs = torch.softmax(next_token_logits, dim=0)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "                generated.append(next_token)\n",
    "                if next_token == tokenizer.eos_id():\n",
    "                    break\n",
    "                input_ids = torch.tensor([generated], dtype=torch.long).to(next(self.parameters()).device)\n",
    "        return tokenizer.decode(generated)\n",
    "\n",
    "########################################################################\n",
    "# Training, Evaluation and Utility Functions\n",
    "########################################################################\n",
    "def train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Trains the model and returns the training and validation loss history.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # inputs: (B, seq_length)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # outputs: (B, seq_length, vocab_size)\n",
    "            # Reshape outputs and targets to compute loss for all tokens at once.\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                running_val_loss += loss.item()\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs} - Training Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the provided data loader and returns the average loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    \"\"\"\n",
    "    Computes perplexity from the cross-entropy loss.\n",
    "    \"\"\"\n",
    "    return np.exp(loss)\n",
    "\n",
    "def compute_bleu(reference, candidate):\n",
    "    \"\"\"\n",
    "    Computes BLEU score using nltk.\n",
    "    \"\"\"\n",
    "    reference_tokens = nltk.word_tokenize(reference)\n",
    "    candidate_tokens = nltk.word_tokenize(candidate)\n",
    "    return sentence_bleu([reference_tokens], candidate_tokens)\n",
    "\n",
    "def plot_loss_curve(train_losses, val_losses, model_name):\n",
    "    \"\"\"\n",
    "    Plots training and validation loss curves and saves the plot.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(f'{model_name} Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{model_name}_loss_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "########################################################################\n",
    "# Main Execution\n",
    "########################################################################\n",
    "def main():\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    vocab_size = 10000\n",
    "    embed_dim = 256\n",
    "    hidden_dim = 512\n",
    "    num_layers = 2\n",
    "    num_heads = 8           # For Transformer\n",
    "    max_seq_length = 50     # Input sequence length\n",
    "    batch_size = 128\n",
    "    num_epochs = 10         # Adjust to 30 with early stopping for full training\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # Tokenizer training/loading\n",
    "    tokenizer_model_prefix = \"tokenizer\"\n",
    "    if not os.path.exists(f\"{tokenizer_model_prefix}.model\"):\n",
    "        print(\"Training SentencePiece tokenizer on data.txt...\")\n",
    "        # Ensure that the file 'data.txt' exists and contains your dataset text.\n",
    "        spm.SentencePieceTrainer.train(input='data.txt',\n",
    "                                       model_prefix=tokenizer_model_prefix,\n",
    "                                       vocab_size=vocab_size,\n",
    "                                       model_type='bpe',\n",
    "                                       character_coverage=1.0)\n",
    "    # Load the trained tokenizer\n",
    "    sp = spm.SentencePieceProcessor(model_file=f\"{tokenizer_model_prefix}.model\")\n",
    "\n",
    "    # Load and preprocess dataset\n",
    "    print(\"Loading dataset from data.txt...\")\n",
    "    with open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Tokenize entire text into a list of token IDs\n",
    "    token_ids = sp.encode(text, out_type=int)\n",
    "    print(f\"Total tokens in dataset: {len(token_ids)}\")\n",
    "\n",
    "    # Create sequences using a sliding window approach.\n",
    "    # Each sequence will be of length (max_seq_length + 1) so that input and target can be formed.\n",
    "    sequences = []\n",
    "    for i in range(0, len(token_ids) - max_seq_length):\n",
    "        seq = token_ids[i:i + max_seq_length + 1]\n",
    "        sequences.append(seq)\n",
    "    print(f\"Total sequences generated: {len(sequences)}\")\n",
    "\n",
    "    # Split data into training and validation sets (e.g., 90% training, 10% validation)\n",
    "    split_idx = int(0.9 * len(sequences))\n",
    "    train_sequences = sequences[:split_idx]\n",
    "    val_sequences = sequences[split_idx:]\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = LanguageModelDataset(train_sequences, max_seq_length)\n",
    "    val_dataset = LanguageModelDataset(val_sequences, max_seq_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Instantiate models\n",
    "    models = {\n",
    "        \"RNN\": RNNLanguageModel(vocab_size, embed_dim, hidden_dim, num_layers).to(device),\n",
    "        \"LSTM\": LSTMLanguageModel(vocab_size, embed_dim, hidden_dim, num_layers).to(device),\n",
    "        \"Transformer\": TransformerLanguageModel(vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_seq_length).to(device)\n",
    "    }\n",
    "\n",
    "    # Train, evaluate, and demonstrate each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining {model_name} model...\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        train_losses, val_losses = train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, device)\n",
    "        plot_loss_curve(train_losses, val_losses, model_name)\n",
    "\n",
    "        # Evaluate perplexity on the validation set\n",
    "        val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "        ppl = compute_perplexity(val_loss)\n",
    "        print(f\"{model_name} - Validation Loss: {val_loss:.4f}, Perplexity: {ppl:.2f}\")\n",
    "\n",
    "        # Generate sample text using the prompt \"Which do you prefer? Dogs or cats?\"\n",
    "        sample_prompt = \"Which do you prefer? Dogs or cats?\"\n",
    "        generated_text = model.prompt(sp, sample_prompt, max_length=50, temperature=0.0)  # Greedy decoding\n",
    "        print(f\"\\nSample output from {model_name} model for prompt:\\n\\\"{sample_prompt}\\\"\\n{generated_text}\\n\")\n",
    "\n",
    "    # Example BLEU score evaluation (for demonstration purposes)\n",
    "    # You would normally compare generated text to a ground truth reference.\n",
    "    reference = \"Dogs are great companions.\"\n",
    "    candidate = models[\"RNN\"].prompt(sp, \"Dogs are\", max_length=10, temperature=0.0)\n",
    "    bleu_score = compute_bleu(reference, candidate)\n",
    "    print(f\"Sample BLEU score (RNN model): {bleu_score:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
