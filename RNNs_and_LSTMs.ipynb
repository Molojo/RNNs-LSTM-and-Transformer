{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add RNNs and LSTMs notebook, remove unused Untitled-1.ipynb, and update .DS_Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/mubaraqolojo/Downloads/RNN and LSTMs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 JSON Sample Entry:\n",
      "{\n",
      "  \"prompt\": \"are occasions on which the governors and the governed meet together,at festivals, on a journey, voyaging or fighting. the sturdy pauper finds that in the hour of danger he is not despised; he sees the rich man puffing and panting, and\",\n",
      "  \"completion\": \"draws\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "train_file = \"/Users/mubaraqolojo/Downloads/RNN and LSTMs/train.jsonl\"\n",
    "\n",
    "with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            obj = json.loads(line)\n",
    "            print(\"🔍 JSON Sample Entry:\")\n",
    "            print(json.dumps(obj, indent=2))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Using device: mps\n",
      "Found 39557 valid text entries in /Users/mubaraqolojo/Downloads/RNN and LSTMs/train.jsonl\n",
      "Combined training text length: 14673121 characters\n",
      "Found 9890 valid text entries in /Users/mubaraqolojo/Downloads/RNN and LSTMs/test.jsonl\n",
      "Combined training text length: 3684340 characters\n",
      "Train tokens length: 3402656\n",
      "Validation tokens length: 855555\n",
      "Generated train sequences: 3402606\n",
      "Generated validation sequences: 855505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training RNN model ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 415\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal training process time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_end\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mglobal_start\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 415\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[4], line 388\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    385\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m    386\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 388\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m    389\u001b[0m     model, train_loader, val_loader,\n\u001b[1;32m    390\u001b[0m     num_epochs, criterion, optimizer, scheduler, device, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m    391\u001b[0m )\n\u001b[1;32m    392\u001b[0m model_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    393\u001b[0m total_model_time \u001b[38;5;241m=\u001b[39m model_end_time \u001b[38;5;241m-\u001b[39m model_start_time\n",
      "Cell \u001b[0;32mIn[4], line 189\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, criterion, optimizer, scheduler, device, patience)\u001b[0m\n\u001b[1;32m    187\u001b[0m output \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m    188\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 189\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    190\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    191\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Foundational AI Project 2 – Language Modeling with RNNs, LSTMs, and Transformer (Graduate Version)\n",
    "\n",
    "This script trains three different language models (RNN, LSTM, Transformer) for text generation.\n",
    "It tokenizes the input data using a BPE tokenizer (SentencePiece), builds fixed-length token\n",
    "sequences, and then trains the models with early stopping and learning rate scheduling.\n",
    "Evaluation metrics such as perplexity and token accuracy are computed, loss curves are plotted,\n",
    "and sample text is generated using a prompt. After training, each model is saved to file.\n",
    "Timing measurements are used to display the duration of each epoch and overall training per model.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import time  # For measuring time durations\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# File paths (update these as needed)\n",
    "train_file = \"/Users/mubaraqolojo/Downloads/RNN and LSTMs/train.jsonl\"\n",
    "test_file = \"/Users/mubaraqolojo/Downloads/RNN and LSTMs/test.jsonl\"\n",
    "\n",
    "# Check that dataset files exist\n",
    "if not os.path.exists(train_file) or not os.path.exists(test_file):\n",
    "    raise FileNotFoundError(\"train.jsonl and/or test.jsonl not found.\")\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, prompt_text, max_length=50, temperature=0.0):\n",
    "    \"\"\"\n",
    "    Generate text from a trained model given a prompt.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained PyTorch model (RNN/LSTM/Transformer).\n",
    "        tokenizer (SentencePieceProcessor): Tokenizer for encoding/decoding text.\n",
    "        prompt_text (str): Initial input prompt.\n",
    "        max_length (int): Maximum number of tokens to generate.\n",
    "        temperature (float): Sampling temperature. 0.0 selects greedy decoding.\n",
    "        \n",
    "    Returns:\n",
    "        str: Decoded text generated by the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    token_ids = tokenizer.encode(prompt_text, out_type=int)\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "    generated = token_ids.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            if temperature == 0.0:\n",
    "                next_token = torch.argmax(next_token_logits).item()\n",
    "            else:\n",
    "                scaled_logits = next_token_logits / temperature\n",
    "                probs = torch.softmax(scaled_logits, dim=0)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated.append(next_token)\n",
    "            if next_token == tokenizer.eos_id():\n",
    "                break\n",
    "            input_ids = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements sinusoidal positional encoding as described in 'Attention is All You Need'.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class LanguageModelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for language modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, seq_length):\n",
    "        self.samples = [(seq[:-1], seq[1:]) for seq in sequences if len(seq) == seq_length + 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp, target = self.samples[idx]\n",
    "        return torch.tensor(inp, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        return self.fc(output)\n",
    "\n",
    "    def prompt(self, tokenizer, prompt_text, max_length=50, temperature=0.0):\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        return self.fc(output)\n",
    "\n",
    "    def prompt(self, tokenizer, prompt_text, max_length=50, temperature=0.0):\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_seq_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout, max_seq_length)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
    "                                                   dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        encoded = self.pos_encoder(embedded)\n",
    "        encoded = encoded.transpose(0, 1)\n",
    "        out = self.transformer_encoder(encoded)\n",
    "        out = out.transpose(0, 1)\n",
    "        return self.fc(out)\n",
    "\n",
    "    def prompt(self, tokenizer, prompt_text, max_length=50, temperature=0.0):\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, scheduler, device, patience=5):\n",
    "    model.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        epoch_duration = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Duration: {epoch_duration:.2f}s\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss += criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1)).item()\n",
    "    return loss / len(data_loader)\n",
    "\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    return np.exp(loss)\n",
    "\n",
    "\n",
    "def compute_token_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.numel()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def compute_bleu(reference, candidate):\n",
    "    return sentence_bleu([nltk.word_tokenize(reference)], nltk.word_tokenize(candidate))\n",
    "\n",
    "\n",
    "def plot_loss_curve(train_losses, val_losses, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker='o', linestyle='-', linewidth=2)\n",
    "    plt.plot(val_losses, label=\"Validation Loss\", marker='s', linestyle='-', linewidth=2)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Loss\", fontsize=12)\n",
    "    plt.title(f\"{model_name} Loss Curve\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name}_loss.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_and_tokenize(file, sp):\n",
    "    texts = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            prompt = obj.get(\"prompt\", \"\")\n",
    "            completion = obj.get(\"completion\", \"\")\n",
    "            text = (prompt.strip() + \" \" + completion.strip()).strip()\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "    print(f\"Found {len(texts)} valid text entries in {file}\")\n",
    "    # Use newline as the separator so that each sentence is on its own line.\n",
    "    combined = \"\\n\".join(texts)\n",
    "    print(f\"Combined training text length: {len(combined)} characters\")\n",
    "    return sp.encode(combined, out_type=int)\n",
    "\n",
    "\n",
    "def build_sequences(token_ids, max_len):\n",
    "    return [token_ids[i:i + max_len + 1] for i in range(len(token_ids) - max_len)]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global_start = time.time()\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"[✓] Using device: {device}\")\n",
    "\n",
    "    vocab_size = 10000\n",
    "    embed_dim = 256\n",
    "    hidden_dim = 512\n",
    "    num_layers = 2\n",
    "    num_heads = 8\n",
    "    max_seq_length = 50\n",
    "    batch_size = 128\n",
    "    num_epochs = 30\n",
    "    learning_rate = 1e-3\n",
    "    pad_token_id = 3\n",
    "\n",
    "    tokenizer_model_prefix = \"tokenizer\"\n",
    "    if not os.path.exists(f\"{tokenizer_model_prefix}.model\"):\n",
    "        print(\"Training tokenizer...\")\n",
    "        texts = []\n",
    "        with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                prompt = obj.get(\"prompt\", \"\")\n",
    "                completion = obj.get(\"completion\", \"\")\n",
    "                text = (prompt.strip() + \" \" + completion.strip()).strip()\n",
    "                if text:\n",
    "                    texts.append(text)\n",
    "        if not texts:\n",
    "            raise ValueError(\"No valid text found in the training file. Cannot train tokenizer.\")\n",
    "        combined = \"\\n\".join(texts)\n",
    "        if not combined.strip():\n",
    "            raise ValueError(\"Combined text is empty. Check your training data.\")\n",
    "        print(f\"Combined training text length: {len(combined)} characters\")\n",
    "        with open(\"temp.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(combined)\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=\"temp.txt\",\n",
    "            model_prefix=tokenizer_model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "            model_type=\"bpe\",\n",
    "            character_coverage=1.0\n",
    "        )\n",
    "        os.remove(\"temp.txt\")\n",
    "\n",
    "    sp = spm.SentencePieceProcessor(model_file=f\"{tokenizer_model_prefix}.model\")\n",
    "\n",
    "    train_tokens = load_and_tokenize(train_file, sp)\n",
    "    val_tokens = load_and_tokenize(test_file, sp)\n",
    "\n",
    "    train_seqs = build_sequences(train_tokens, max_seq_length)\n",
    "    val_seqs = build_sequences(val_tokens, max_seq_length)\n",
    "\n",
    "    print(f\"Train tokens length: {len(train_tokens)}\")\n",
    "    print(f\"Validation tokens length: {len(val_tokens)}\")\n",
    "    print(f\"Generated train sequences: {len(train_seqs)}\")\n",
    "    print(f\"Generated validation sequences: {len(val_seqs)}\")\n",
    "\n",
    "    num_workers = 0  \n",
    "    pin_memory = True if device.type == \"cuda\" else False\n",
    "\n",
    "    train_dataset = LanguageModelDataset(train_seqs, max_seq_length)\n",
    "    val_dataset = LanguageModelDataset(val_seqs, max_seq_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    models = {\n",
    "        \"RNN\": RNNLanguageModel(vocab_size, embed_dim, hidden_dim, num_layers),\n",
    "        \"LSTM\": LSTMLanguageModel(vocab_size, embed_dim, hidden_dim, num_layers),\n",
    "        \"Transformer\": TransformerLanguageModel(vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_seq_length)\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} model ---\")\n",
    "        model_start_time = time.time()\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
    "        \n",
    "        train_losses, val_losses = train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs, criterion, optimizer, scheduler, device, patience=5\n",
    "        )\n",
    "        model_end_time = time.time()\n",
    "        total_model_time = model_end_time - model_start_time\n",
    "        print(f\"Total training time for {name} model: {total_model_time:.2f} seconds\")\n",
    "        \n",
    "        plot_loss_curve(train_losses, val_losses, name)\n",
    "        \n",
    "        val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "        perplexity = compute_perplexity(val_loss)\n",
    "        accuracy = compute_token_accuracy(model, val_loader, device) * 100\n",
    "        \n",
    "        print(f\"{name} | Perplexity: {perplexity:.2f} | Token Accuracy: {accuracy:.2f}%\")\n",
    "        sample_output = model.prompt(sp, \"Which do you prefer? Dogs or cats?\", max_length=50)\n",
    "        print(\"Sample output:\", sample_output)\n",
    "        \n",
    "        model_save_path = f\"{name}_model.pt\"\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"{name} model saved to {model_save_path}\")\n",
    "\n",
    "    global_end = time.time()\n",
    "    print(f\"\\nTotal training process time: {global_end - global_start:.2f} seconds\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
