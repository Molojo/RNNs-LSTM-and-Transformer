{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add RNNs and LSTMs notebook, remove unused Untitled-1.ipynb, and update .DS_Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/mubaraqolojo/Downloads/RNN and LSTMs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç JSON Sample Entry:\n",
      "{\n",
      "  \"prompt\": \"are occasions on which the governors and the governed meet together,at festivals, on a journey, voyaging or fighting. the sturdy pauper finds that in the hour of danger he is not despised; he sees the rich man puffing and panting, and\",\n",
      "  \"completion\": \"draws\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "train_file = \"/Users/mubaraqolojo/Downloads/RNN and LSTMs/train.jsonl\"\n",
    "\n",
    "with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            obj = json.loads(line)\n",
    "            print(\"üîç JSON Sample Entry:\")\n",
    "            print(json.dumps(obj, indent=2))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Foundational AI Project 2 ‚Äì Language Modeling with RNNs, LSTMs, and Transformer (Graduate Version)\n",
    "\n",
    "This script trains three different language models (RNN, LSTM, Transformer) for text generation.\n",
    "It tokenizes the input data using a BPE tokenizer (SentencePiece), builds fixed-length token\n",
    "sequences, and then trains the models with early stopping and learning rate scheduling.\n",
    "Evaluation metrics such as perplexity and token accuracy are computed, loss curves are plotted,\n",
    "and sample text is generated using a prompt. After training, each model is saved to file.\n",
    "Timing measurements are used to display the duration of each epoch and overall training per model.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import time  # For measuring time durations\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# File paths (update these as needed)\n",
    "train_file = \"/Users/mubaraqolojo/Downloads/RNN and LSTMs/train.jsonl\"\n",
    "test_file = \"/Users/mubaraqolojo/Downloads/RNN and LSTMs/test.jsonl\"\n",
    "\n",
    "# Check that dataset files exist\n",
    "if not os.path.exists(train_file) or not os.path.exists(test_file):\n",
    "    raise FileNotFoundError(\"train.jsonl and/or test.jsonl not found.\")\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, prompt_text, max_length=50, temperature=0.0):\n",
    "    \"\"\"\n",
    "    Generate text from a trained model given a prompt.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained PyTorch model (RNN/LSTM/Transformer).\n",
    "        tokenizer (SentencePieceProcessor): Tokenizer for encoding/decoding text.\n",
    "        prompt_text (str): Initial input prompt.\n",
    "        max_length (int): Maximum number of tokens to generate.\n",
    "        temperature (float): Sampling temperature. 0.0 selects greedy decoding.\n",
    "        \n",
    "    Returns:\n",
    "        str: Decoded text generated by the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    token_ids = tokenizer.encode(prompt_text, out_type=int)\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "    generated = token_ids.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            if temperature == 0.0:\n",
    "                next_token = torch.argmax(next_token_logits).item()\n",
    "            else:\n",
    "                scaled_logits = next_token_logits / temperature\n",
    "                probs = torch.softmax(scaled_logits, dim=0)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated.append(next_token)\n",
    "            if next_token == tokenizer.eos_id():\n",
    "                break\n",
    "            input_ids = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements sinusoidal positional encoding as described in 'Attention is All You Need'.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class LanguageModelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for language modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, seq_length):\n",
    "        self.samples = [(seq[:-1], seq[1:]) for seq in sequences if len(seq) == seq_length + 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp, target = self.samples[idx]\n",
    "        return torch.tensor(inp, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        return self.fc(output)\n",
    "\n",
    "    def prompt(self, tokenizer, prompt_text, max_length=50, temperature=0.0):\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        return self.fc(output)\n",
    "\n",
    "    def prompt(self, tokenizer, prompt_text, max_length=50, temperature=0.0):\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_seq_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout, max_seq_length)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
    "                                                   dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        encoded = self.pos_encoder(embedded)\n",
    "        encoded = encoded.transpose(0, 1)\n",
    "        out = self.transformer_encoder(encoded)\n",
    "        out = out.transpose(0, 1)\n",
    "        return self.fc(out)\n",
    "\n",
    "    def prompt(self, tokenizer, prompt_text, max_length=50, temperature=0.0):\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, scheduler, device, patience=5):\n",
    "    model.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        epoch_duration = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Duration: {epoch_duration:.2f}s\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss += criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1)).item()\n",
    "    return loss / len(data_loader)\n",
    "\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    return np.exp(loss)\n",
    "\n",
    "\n",
    "def compute_token_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.numel()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def compute_bleu(reference, candidate):\n",
    "    return sentence_bleu([nltk.word_tokenize(reference)], nltk.word_tokenize(candidate))\n",
    "\n",
    "\n",
    "def plot_loss_curve(train_losses, val_losses, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker='o', linestyle='-', linewidth=2)\n",
    "    plt.plot(val_losses, label=\"Validation Loss\", marker='s', linestyle='-', linewidth=2)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Loss\", fontsize=12)\n",
    "    plt.title(f\"{model_name} Loss Curve\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name}_loss.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_and_tokenize(file, sp):\n",
    "    texts = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            prompt = obj.get(\"prompt\", \"\")\n",
    "            completion = obj.get(\"completion\", \"\")\n",
    "            text = (prompt.strip() + \" \" + completion.strip()).strip()\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "    print(f\"Found {len(texts)} valid text entries in {file}\")\n",
    "    # Use newline as the separator so that each sentence is on its own line.\n",
    "    combined = \"\\n\".join(texts)\n",
    "    print(f\"Combined training text length: {len(combined)} characters\")\n",
    "    return sp.encode(combined, out_type=int)\n",
    "\n",
    "\n",
    "def build_sequences(token_ids, max_len):\n",
    "    return [token_ids[i:i + max_len + 1] for i in range(len(token_ids) - max_len)]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global_start = time.time()\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"[‚úì] Using device: {device}\")\n",
    "\n",
    "    vocab_size = 10000\n",
    "    embed_dim = 256\n",
    "    hidden_dim = 512\n",
    "    num_layers = 2\n",
    "    num_heads = 8\n",
    "    max_seq_length = 50\n",
    "    batch_size = 128\n",
    "    num_epochs = 30\n",
    "    learning_rate = 1e-3\n",
    "    pad_token_id = 3\n",
    "\n",
    "    tokenizer_model_prefix = \"tokenizer\"\n",
    "    if not os.path.exists(f\"{tokenizer_model_prefix}.model\"):\n",
    "        print(\"Training tokenizer...\")\n",
    "        texts = []\n",
    "        with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                prompt = obj.get(\"prompt\", \"\")\n",
    "                completion = obj.get(\"completion\", \"\")\n",
    "                text = (prompt.strip() + \" \" + completion.strip()).strip()\n",
    "                if text:\n",
    "                    texts.append(text)\n",
    "        if not texts:\n",
    "            raise ValueError(\"No valid text found in the training file. Cannot train tokenizer.\")\n",
    "        combined = \"\\n\".join(texts)\n",
    "        if not combined.strip():\n",
    "            raise ValueError(\"Combined text is empty. Check your training data.\")\n",
    "        print(f\"Combined training text length: {len(combined)} characters\")\n",
    "        with open(\"temp.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(combined)\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=\"temp.txt\",\n",
    "            model_prefix=tokenizer_model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "            model_type=\"bpe\",\n",
    "            character_coverage=1.0\n",
    "        )\n",
    "        os.remove(\"temp.txt\")\n",
    "\n",
    "    sp = spm.SentencePieceProcessor(model_file=f\"{tokenizer_model_prefix}.model\")\n",
    "\n",
    "    train_tokens = load_and_tokenize(train_file, sp)\n",
    "    val_tokens = load_and_tokenize(test_file, sp)\n",
    "\n",
    "    train_seqs = build_sequences(train_tokens, max_seq_length)\n",
    "    val_seqs = build_sequences(val_tokens, max_seq_length)\n",
    "\n",
    "    print(f\"Train tokens length: {len(train_tokens)}\")\n",
    "    print(f\"Validation tokens length: {len(val_tokens)}\")\n",
    "    print(f\"Generated train sequences: {len(train_seqs)}\")\n",
    "    print(f\"Generated validation sequences: {len(val_seqs)}\")\n",
    "\n",
    "    num_workers = 0  \n",
    "    pin_memory = True if device.type == \"cuda\" else False\n",
    "\n",
    "    train_dataset = LanguageModelDataset(train_seqs, max_seq_length)\n",
    "    val_dataset = LanguageModelDataset(val_seqs, max_seq_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    models = {\n",
    "        \"RNN\": RNNLanguageModel(vocab_size, embed_dim, hidden_dim, num_layers),\n",
    "        \"LSTM\": LSTMLanguageModel(vocab_size, embed_dim, hidden_dim, num_layers),\n",
    "        \"Transformer\": TransformerLanguageModel(vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_seq_length)\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} model ---\")\n",
    "        model_start_time = time.time()\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
    "        \n",
    "        train_losses, val_losses = train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs, criterion, optimizer, scheduler, device, patience=5\n",
    "        )\n",
    "        model_end_time = time.time()\n",
    "        total_model_time = model_end_time - model_start_time\n",
    "        print(f\"Total training time for {name} model: {total_model_time:.2f} seconds\")\n",
    "        \n",
    "        plot_loss_curve(train_losses, val_losses, name)\n",
    "        \n",
    "        val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "        perplexity = compute_perplexity(val_loss)\n",
    "        accuracy = compute_token_accuracy(model, val_loader, device) * 100\n",
    "        \n",
    "        print(f\"{name} | Perplexity: {perplexity:.2f} | Token Accuracy: {accuracy:.2f}%\")\n",
    "        sample_output = model.prompt(sp, \"Which do you prefer? Dogs or cats?\", max_length=50)\n",
    "        print(\"Sample output:\", sample_output)\n",
    "        \n",
    "        model_save_path = f\"{name}_model.pt\"\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"{name} model saved to {model_save_path}\")\n",
    "\n",
    "    global_end = time.time()\n",
    "    print(f\"\\nTotal training process time: {global_end - global_start:.2f} seconds\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
