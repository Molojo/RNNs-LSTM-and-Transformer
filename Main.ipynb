{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Using device: mps\n",
      "Found existing tokenizer model: bpe_tokenizer_v1.model\n",
      "Tokenizer loaded. Vocab size: 10000. Special IDs: UNK=0, BOS=1, EOS=2, PAD=3\n",
      "Using Padding Token ID: 3\n",
      "Effective vocabulary size: 10000\n",
      "\n",
      "Loading and tokenizing datasets...\n",
      "Loaded 39557 text entries from train.jsonl (out of 39557 lines). Total combined length: 14870901 characters\n",
      "Tokenized into 3757733 tokens.\n",
      "Loaded 9890 text entries from test.jsonl (out of 9890 lines). Total combined length: 3733785 characters\n",
      "Tokenized into 943202 tokens.\n",
      "\n",
      "Building sequences...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1425\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# --- Script Entry Point ---\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;66;03m# Execute the main function when the script is run directly.\u001b[39;00m\n\u001b[0;32m-> 1425\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 1167\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBuilding sequences...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;66;03m# Create overlapping sequences of length `max_seq_length + 1`.\u001b[39;00m\n\u001b[0;32m-> 1167\u001b[0m train_seqs \u001b[38;5;241m=\u001b[39m build_sequences(train_tokens, max_seq_length)\n\u001b[1;32m   1168\u001b[0m val_seqs \u001b[38;5;241m=\u001b[39m build_sequences(val_tokens, max_seq_length)\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of training sequences created: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_seqs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 313\u001b[0m, in \u001b[0;36mbuild_sequences\u001b[0;34m(token_ids, seq_length)\u001b[0m\n\u001b[1;32m    308\u001b[0m      \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Use a list comprehension for efficient sliding window creation.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# For each starting index `i`, take a slice of length `seq_length + 1`.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Stop when the slice would go beyond the end of the `token_ids` list.\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m sequences \u001b[38;5;241m=\u001b[39m [token_ids[i : i \u001b[38;5;241m+\u001b[39m seq_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(token_ids) \u001b[38;5;241m-\u001b[39m seq_length)]\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequences\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Foundational AI Project 2 – Language Modeling with RNNs, LSTMs, and Transformer (Graduate Version)\n",
    "\n",
    "This script trains three language models (RNN, LSTM, Transformer) for text generation.\n",
    "It uses a SentencePiece BPE tokenizer (vocab size=10000) to tokenize text from JSONL files,\n",
    "builds fixed-length sequences via a sliding window approach, and trains the models\n",
    "using early stopping with a ReduceLROnPlateau learning rate scheduler.\n",
    "Evaluation metrics include perplexity (exp(cross-entropy loss)), token accuracy,\n",
    "and BLEU score (computed with nltk on a sample). Sample outputs and loss curves with detailed plots\n",
    "are generated, and model performance is compared.\n",
    "Graduate-level requirements such as temperature-based decoding are supported in the prompt methods.\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # Learning rate scheduler that reduces LR when a metric plateaus\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sentencepiece as spm  # For BPE tokenization\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction # BLEU score computation with smoothing\n",
    "\n",
    "# --- NLTK Data Check ---\n",
    "# Ensure the 'punkt' tokenizer data (used by nltk.word_tokenize for BLEU) is available.\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading NLTK 'punkt' tokenizer data...\")\n",
    "    nltk.download('punkt', quiet=True) # Download quietly\n",
    "\n",
    "# --- Reproducibility ---\n",
    "# Set random seeds for Python, NumPy, and PyTorch to ensure reproducible results.\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# Ensure reproducibility on CUDA if available (can slightly slow down computation)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Device Selection ---\n",
    "# Select the appropriate computation device (GPU > MPS > CPU).\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(seed) # Set seed for all GPUs\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    # Check if MPS (Apple Silicon GPU) is available and functional.\n",
    "    try:\n",
    "        # Perform a simple tensor operation on MPS to verify usability.\n",
    "        torch.ones(1, device=\"mps\")\n",
    "        device = torch.device(\"mps\")\n",
    "    except Exception:\n",
    "        # Fallback to CPU if MPS check fails.\n",
    "        print(\"MPS device found but may not be usable. Falling back to CPU.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"[✓] Using device: {device}\")\n",
    "\n",
    "###############################################################################\n",
    "# Positional Encoding Module (for Transformer)\n",
    "###############################################################################\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects positional information into token embeddings using sinusoidal functions.\n",
    "    This allows the Transformer model, which lacks inherent sequence order awareness,\n",
    "    to utilize token position information. Based on 'Attention is All You Need'.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): The embedding dimension (d_model).\n",
    "        max_len (int): The maximum sequence length for which to precompute encodings.\n",
    "                       Should be at least as large as the longest sequence length.\n",
    "        dropout (float): Dropout rate applied after adding positional encodings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, max_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a matrix for positional encodings: [max_len, embed_dim]\n",
    "        pos_enc = torch.zeros(max_len, embed_dim)\n",
    "\n",
    "        # Create position indices: [max_len, 1] (tensor([[0.], [1.], ..., [max_len-1]]))\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Calculate the division term for the frequencies.\n",
    "        # Formula: 1 / (10000^(2i / embed_dim))\n",
    "        # Use log space for numerical stability: exp(-log(10000) * (2i / embed_dim))\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float) * -(math.log(10000.0) / embed_dim))\n",
    "\n",
    "        # Calculate sinusoidal encodings:\n",
    "        # Even indices (0, 2, 4, ...): PE(pos, 2i) = sin(pos / (10000^(2i / embed_dim)))\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Odd indices (1, 3, 5, ...): PE(pos, 2i+1) = cos(pos / (10000^(2i / embed_dim)))\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension: [1, max_len, embed_dim]\n",
    "        # This allows easy broadcasting when adding to batch embeddings.\n",
    "        pos_enc = pos_enc.unsqueeze(0)\n",
    "\n",
    "        # Register 'pos_enc' as a buffer. Buffers are part of the model's state_dict\n",
    "        # but are not considered model parameters (not updated by optimizer).\n",
    "        self.register_buffer(\"pos_enc\", pos_enc)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Adds positional encoding to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of token embeddings.\n",
    "                        Shape: [batch_size, seq_length, embed_dim].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor with positional information added.\n",
    "                    Shape: [batch_size, seq_length, embed_dim].\n",
    "        \"\"\"\n",
    "        # Add positional encoding to the input embeddings.\n",
    "        # Slice precomputed encodings 'pos_enc' to match the input sequence length (x.size(1)).\n",
    "        # Shape: [batch_size, seq_length, embed_dim] + [1, seq_length, embed_dim] -> [batch_size, seq_length, embed_dim]\n",
    "        x = x + self.pos_enc[:, :x.size(1)]\n",
    "\n",
    "        # Apply dropout for regularization.\n",
    "        return self.dropout(x)\n",
    "\n",
    "###############################################################################\n",
    "# Data Preparation Functions\n",
    "###############################################################################\n",
    "def train_tokenizer_if_needed(tokenizer_model_prefix: str = \"tokenizer\", vocab_size: int = 10000, training_text_file: str = \"merged_corpus.txt\") -> spm.SentencePieceProcessor:\n",
    "    \"\"\"\n",
    "    Trains a SentencePiece BPE (Byte-Pair Encoding) tokenizer on the provided text file\n",
    "    if the tokenizer model files (.model, .vocab) do not already exist.\n",
    "\n",
    "    Args:\n",
    "        tokenizer_model_prefix (str): The prefix for the output model and vocabulary files.\n",
    "        vocab_size (int): The target vocabulary size for the tokenizer.\n",
    "        training_text_file (str): The path to the plain text file used for training the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        spm.SentencePieceProcessor: An instance of the loaded SentencePiece processor.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the training text file does not exist and model files are missing.\n",
    "        Exception: If tokenizer training fails.\n",
    "    \"\"\"\n",
    "    model_path = f\"{tokenizer_model_prefix}.model\"\n",
    "    vocab_path = f\"{tokenizer_model_prefix}.vocab\"\n",
    "\n",
    "    # Check if both model and vocab files exist.\n",
    "    if not os.path.exists(model_path) or not os.path.exists(vocab_path):\n",
    "        print(f\"Tokenizer model ('{model_path}' or '{vocab_path}') not found. Training...\")\n",
    "        # Check if the training data file exists.\n",
    "        if not os.path.exists(training_text_file):\n",
    "             raise FileNotFoundError(f\"Tokenizer training file '{training_text_file}' not found. Cannot train tokenizer.\")\n",
    "\n",
    "        try:\n",
    "            # Train the SentencePiece model using the specified parameters.\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                input=training_text_file,          # Path to the training text file.\n",
    "                model_prefix=tokenizer_model_prefix, # Prefix for output files (.model, .vocab).\n",
    "                vocab_size=vocab_size,             # Target size of the vocabulary.\n",
    "                model_type=\"bpe\",                  # Use Byte-Pair Encoding algorithm.\n",
    "                character_coverage=1.0,            # Try to cover all characters in the training data.\n",
    "                # Define IDs for standard special tokens. SentencePiece uses these defaults if not specified.\n",
    "                # UNK (Unknown): Represents out-of-vocabulary words.\n",
    "                # BOS (Beginning-of-Sequence): Optional start token.\n",
    "                # EOS (End-of-Sequence): Marks the end of a sentence or sequence.\n",
    "                # PAD (Padding): Used to make sequences in a batch the same length.\n",
    "                unk_id=0,       # Typically ID 0 for <unk>\n",
    "                bos_id=1,       # Typically ID 1 for <s>\n",
    "                eos_id=2,       # Typically ID 2 for </s>\n",
    "                pad_id=3        # Explicitly set PAD ID to 3. If set to -1, PAD is disabled.\n",
    "            )\n",
    "            print(\"Tokenizer training complete.\")\n",
    "        except Exception as e:\n",
    "            # Handle potential errors during training.\n",
    "            print(f\"Error training tokenizer: {e}\")\n",
    "            # Clean up potentially incomplete model/vocab files if training failed.\n",
    "            if os.path.exists(model_path): os.remove(model_path)\n",
    "            if os.path.exists(vocab_path): os.remove(vocab_path)\n",
    "            raise # Re-raise the exception after cleanup.\n",
    "    else:\n",
    "        # If model files exist, skip training.\n",
    "        print(f\"Found existing tokenizer model: {model_path}\")\n",
    "\n",
    "    # Load the trained tokenizer model from the file.\n",
    "    sp = spm.SentencePieceProcessor(model_file=model_path)\n",
    "    # Print tokenizer details for verification.\n",
    "    print(f\"Tokenizer loaded. Vocab size: {sp.vocab_size()}. Special IDs: \"\n",
    "          f\"UNK={sp.unk_id()}, BOS={sp.bos_id()}, EOS={sp.eos_id()}, PAD={sp.pad_id()}\")\n",
    "    return sp\n",
    "\n",
    "def load_and_tokenize(file_path: str, sp: spm.SentencePieceProcessor) -> list[int]:\n",
    "    \"\"\"\n",
    "    Loads text data from a JSONL file (each line is a JSON object), extracts \"prompt\"\n",
    "    and \"completion\" fields, combines them, concatenates all entries using the\n",
    "    tokenizer's EOS token as a separator, and tokenizes the entire text into a single\n",
    "    list of token IDs.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file.\n",
    "        sp (SentencePieceProcessor): The initialized SentencePiece tokenizer instance.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A flat list of token IDs representing the tokenized content of the file.\n",
    "    \"\"\"\n",
    "    texts = [] # List to hold individual text entries (prompt + completion)\n",
    "    count = 0  # Counter for total lines processed\n",
    "\n",
    "    # Read the JSONL file line by line.\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            try:\n",
    "                # Attempt to parse the line as a JSON object.\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip lines that are not valid JSON.\n",
    "                # print(f\"Warning: Skipping invalid JSON line in {file_path} (line {count})\")\n",
    "                continue\n",
    "            # Extract \"prompt\" and \"completion\" fields, defaulting to empty strings if missing.\n",
    "            prompt = obj.get(\"prompt\", \"\")\n",
    "            completion = obj.get(\"completion\", \"\")\n",
    "            # Combine prompt and completion, stripping leading/trailing whitespace.\n",
    "            text = (prompt + \" \" + completion).strip()\n",
    "            # Add the combined text to the list if it's not empty.\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "\n",
    "    # --- Determine the EOS token string for separation ---\n",
    "    eos_id = sp.eos_id() # Get the integer ID for the EOS token.\n",
    "    eos_string = \"\"      # Initialize EOS string representation.\n",
    "\n",
    "    if eos_id is not None and eos_id >= 0: # Check if a valid EOS ID exists.\n",
    "        try:\n",
    "            # Decode the EOS ID back to its string representation (e.g., \"</s>\").\n",
    "            # Need to pass it as a list to decode.\n",
    "            eos_string = sp.decode([eos_id])\n",
    "            # Handle cases where the decoded special token might be empty or unexpected.\n",
    "            # Use common default '</s>' if ID is 2 and decode is empty.\n",
    "            if not eos_string and eos_id == 2:\n",
    "                eos_string = \"</s>\"\n",
    "            # If still empty after checks, issue a warning and fallback to newline.\n",
    "            if not eos_string:\n",
    "                 print(f\"Warning: Decoded EOS token ID {eos_id} resulted in an empty string. Using newline fallback.\")\n",
    "                 eos_string = \"\\n\"\n",
    "        except Exception as e:\n",
    "             # Handle errors during decoding and fallback to newline.\n",
    "             print(f\"Warning: Could not decode EOS token ID {eos_id}. Error: {e}. Using newline fallback.\")\n",
    "             eos_string = \"\\n\"\n",
    "    else:\n",
    "        # If no EOS token ID is defined in the tokenizer, use newline as separator.\n",
    "        print(\"Warning: EOS token ID not found in tokenizer. Using newline as separator.\")\n",
    "        eos_string = \"\\n\"\n",
    "\n",
    "    # Define the separator string, adding newlines around the EOS token for clarity.\n",
    "    # Strip potential whitespace from the decoded token itself.\n",
    "    separator = f\"\\n{eos_string.strip()}\\n\"\n",
    "\n",
    "    # Join all extracted text entries into a single large string using the separator.\n",
    "    # This creates one continuous corpus for sequence building.\n",
    "    combined = separator.join(texts)\n",
    "    # --- End of EOS handling and text combination ---\n",
    "\n",
    "    print(f\"Loaded {len(texts)} text entries from {file_path} (out of {count} lines). \"\n",
    "          f\"Total combined length: {len(combined)} characters\")\n",
    "\n",
    "    # Tokenize the entire combined text into a list of integer IDs.\n",
    "    token_ids = sp.encode(combined, out_type=int)\n",
    "    print(f\"Tokenized into {len(token_ids)} tokens.\")\n",
    "    return token_ids\n",
    "\n",
    "def build_sequences(token_ids: list[int], seq_length: int) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Creates overlapping sequences of a fixed length from a flat list of token IDs\n",
    "    using a sliding window approach. Each sequence has length `seq_length + 1`\n",
    "    to facilitate creating input/target pairs (input = seq[:-1], target = seq[1:]).\n",
    "\n",
    "    Args:\n",
    "        token_ids (list[int]): The flat list of token IDs from the tokenized corpus.\n",
    "        seq_length (int): The desired length of the *input* sequences. The generated\n",
    "                          sequences will have length `seq_length + 1`.\n",
    "\n",
    "    Returns:\n",
    "        list[list[int]]: A list of sequences, where each sequence is a list of token IDs\n",
    "                         of length `seq_length + 1`. Returns an empty list if no\n",
    "                         valid sequences can be created.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `seq_length` is not positive.\n",
    "    \"\"\"\n",
    "    # Validate seq_length.\n",
    "    if seq_length <= 0:\n",
    "        raise ValueError(\"seq_length must be positive.\")\n",
    "    # Check if there are enough tokens to form even one sequence.\n",
    "    if len(token_ids) <= seq_length:\n",
    "         print(f\"Warning: Token list length ({len(token_ids)}) is not greater than seq_length ({seq_length}). \"\n",
    "               \"Cannot generate sequences.\")\n",
    "         return []\n",
    "\n",
    "    # Use a list comprehension for efficient sliding window creation.\n",
    "    # For each starting index `i`, take a slice of length `seq_length + 1`.\n",
    "    # Stop when the slice would go beyond the end of the `token_ids` list.\n",
    "    sequences = [token_ids[i : i + seq_length + 1] for i in range(len(token_ids) - seq_length)]\n",
    "    return sequences\n",
    "\n",
    "###############################################################################\n",
    "# Custom Dataset Class for Language Modeling\n",
    "###############################################################################\n",
    "class LanguageModelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for language modeling. It takes a list of token sequences\n",
    "    (each of length `seq_length + 1`) and provides input/target pairs for training.\n",
    "    Each sample retrieved by `__getitem__` is a tuple (input_tokens, target_tokens),\n",
    "    where `target_tokens` are the `input_tokens` shifted one position to the right.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences: list[list[int]], seq_length: int):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            sequences (list[list[int]]): A list of token sequences, where each sequence\n",
    "                                         is expected to have length `seq_length + 1`.\n",
    "            seq_length (int): The length of the input sequences (targets will also have this length).\n",
    "        \"\"\"\n",
    "        # Create input/target pairs: input = sequence[:-1], target = sequence[1:]\n",
    "        # Include a check to filter out any sequences that might not have the exact required length.\n",
    "        # This should ideally not happen if `build_sequences` is used correctly, but acts as a safeguard.\n",
    "        self.samples = [(seq[:-1], seq[1:]) for seq in sequences if len(seq) == seq_length + 1]\n",
    "\n",
    "        # Report if any sequences were filtered out due to incorrect length.\n",
    "        if len(self.samples) < len(sequences):\n",
    "             print(f\"Warning: Filtered out {len(sequences) - len(self.samples)} sequences due to \"\n",
    "                   f\"incorrect length (expected {seq_length + 1}).\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the total number of samples (input/target pairs) in the dataset.\"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieves the sample (input/target pair) at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "                - input_tokens (torch.Tensor): Tensor of input token IDs (dtype=torch.long).\n",
    "                - target_tokens (torch.Tensor): Tensor of target token IDs (dtype=torch.long).\n",
    "        \"\"\"\n",
    "        inp_ids, target_ids = self.samples[idx]\n",
    "        # Convert the lists of token IDs to PyTorch tensors of type long.\n",
    "        return torch.tensor(inp_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "###############################################################################\n",
    "# Text Generation Function and Model Definitions\n",
    "###############################################################################\n",
    "def generate_text(model: nn.Module,\n",
    "                  tokenizer: spm.SentencePieceProcessor,\n",
    "                  prompt_text: str,\n",
    "                  max_length: int = 128,\n",
    "                  temperature: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Generates text autoregressively starting from a given prompt using the provided model.\n",
    "\n",
    "    Supports temperature-based sampling for controlling randomness.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained language model (RNN, LSTM, or Transformer).\n",
    "        tokenizer (SentencePieceProcessor): The tokenizer used for encoding/decoding.\n",
    "        prompt_text (str): The initial text to seed the generation.\n",
    "        max_length (int): The maximum number of *new* tokens to generate after the prompt.\n",
    "        temperature (float): Controls the randomness of sampling.\n",
    "                             - temperature=0 (or close to 0): Greedy decoding (always pick the most likely token).\n",
    "                             - temperature=1.0: Standard sampling from the model's predicted probabilities.\n",
    "                             - temperature > 1.0: Increases randomness, makes less likely tokens more probable.\n",
    "                             - 0 < temperature < 1.0: Decreases randomness, favors more likely tokens.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text, including the original prompt.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model_device = next(model.parameters()).device\n",
    "\n",
    "    # initial tokens\n",
    "    generated = tokenizer.encode(prompt_text,\n",
    "                                 out_type=int,\n",
    "                                 add_bos=False,\n",
    "                                 add_eos=False).copy()\n",
    "\n",
    "    eos_token_id = tokenizer.eos_id()\n",
    "    if eos_token_id is None or eos_token_id < 0:\n",
    "        eos_token_id = -1\n",
    "\n",
    "    # get the model's max_seq_length if it exists\n",
    "    max_allowed = getattr(model, \"max_seq_length\", None)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # TRUNCATE here:\n",
    "            seq = generated\n",
    "            if max_allowed and len(seq) > max_allowed:\n",
    "                seq = seq[-max_allowed:]\n",
    "\n",
    "            input_ids = torch.tensor([seq], dtype=torch.long, device=model_device)\n",
    "            logits = model(input_ids)\n",
    "            next_logits = logits[0, -1, :]\n",
    "\n",
    "            if temperature < 1e-5:\n",
    "                next_token = torch.argmax(next_logits).item()\n",
    "            else:\n",
    "                probs = torch.softmax(next_logits / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if next_token == eos_token_id:\n",
    "                break\n",
    "            generated.append(next_token)\n",
    "\n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple language model based on vanilla Recurrent Neural Networks (RNNs).\n",
    "\n",
    "    Architecture:\n",
    "        1. Embedding Layer: Maps input token IDs to dense vectors.\n",
    "        2. Dropout Layer: Applied after embedding.\n",
    "        3. RNN Layer(s): Processes the sequence of embeddings.\n",
    "        4. Dropout Layer: Applied after RNN output.\n",
    "        5. Linear Layer (FC): Maps RNN hidden states to vocabulary logits.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embed_dim (int): Dimension of token embeddings.\n",
    "        hidden_dim (int): Dimension of RNN hidden states.\n",
    "        num_layers (int): Number of stacked RNN layers.\n",
    "        dropout (float): Dropout probability for dropout layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_layers: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # `batch_first=True`: Input/output tensors have shape [batch, seq, feature].\n",
    "        # Dropout is applied between RNN layers if num_layers > 1.\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers,\n",
    "                          batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        # Fully connected layer to project RNN output to vocabulary size.\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        # Separate dropout layer applied after embedding and RNN output.\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the RNN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of token IDs. Shape: [batch_size, seq_length].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of logits. Shape: [batch_size, seq_length, vocab_size].\n",
    "        \"\"\"\n",
    "        # 1. Embeddings\n",
    "        # Shape: [batch_size, seq_length] -> [batch_size, seq_length, embed_dim]\n",
    "        embeds = self.dropout_layer(self.embedding(x))\n",
    "\n",
    "        # 2. RNN\n",
    "        # `output` contains hidden states for all time steps. Shape: [batch, seq_length, hidden_dim]\n",
    "        # `_` holds the final hidden state (h_n). We don't need it directly for logits.\n",
    "        output, _ = self.rnn(embeds)\n",
    "        # Apply dropout to the RNN output sequence.\n",
    "        output = self.dropout_layer(output)\n",
    "\n",
    "        # 3. Fully Connected Layer\n",
    "        # Project hidden states to vocabulary logits.\n",
    "        # Shape: [batch, seq_length, hidden_dim] -> [batch, seq_length, vocab_size]\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "    def prompt(self, tokenizer: spm.SentencePieceProcessor, prompt_text: str, max_length: int = 128, temperature: float = 1.0) -> str:\n",
    "        \"\"\" Convenience method to generate text using this model. \"\"\"\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A language model based on Long Short-Term Memory (LSTM) networks.\n",
    "\n",
    "    Architecture is similar to RNNLanguageModel, but uses LSTM layers which are\n",
    "    generally better at capturing long-range dependencies.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embed_dim (int): Dimension of token embeddings.\n",
    "        hidden_dim (int): Dimension of LSTM hidden and cell states.\n",
    "        num_layers (int): Number of stacked LSTM layers.\n",
    "        dropout (float): Dropout probability for dropout layers and between LSTM layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_layers: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # `batch_first=True`: Input/output tensors have shape [batch, seq, feature].\n",
    "        # Dropout is applied between LSTM layers if num_layers > 1.\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        # Fully connected layer to project LSTM output to vocabulary size.\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        # Separate dropout layer applied after embedding and LSTM output.\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the LSTM model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of token IDs. Shape: [batch_size, seq_length].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of logits. Shape: [batch_size, seq_length, vocab_size].\n",
    "        \"\"\"\n",
    "        # 1. Embeddings\n",
    "        # Shape: [batch_size, seq_length] -> [batch_size, seq_length, embed_dim]\n",
    "        embeds = self.dropout_layer(self.embedding(x))\n",
    "\n",
    "        # 2. LSTM\n",
    "        # `output` contains hidden states for all time steps. Shape: [batch, seq_length, hidden_dim]\n",
    "        # `_` holds the final hidden state (h_n) and cell state (c_n). We don't need them directly for logits.\n",
    "        output, _ = self.lstm(embeds)\n",
    "        # Apply dropout to the LSTM output sequence.\n",
    "        output = self.dropout_layer(output)\n",
    "\n",
    "        # 3. Fully Connected Layer\n",
    "        # Project hidden states to vocabulary logits.\n",
    "        # Shape: [batch, seq_length, hidden_dim] -> [batch, seq_length, vocab_size]\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "    def prompt(self, tokenizer: spm.SentencePieceProcessor, prompt_text: str, max_length: int = 128, temperature: float = 1.0) -> str:\n",
    "        \"\"\" Convenience method to generate text using this model. \"\"\"\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A language model based on the Transformer architecture (specifically, the Encoder part).\n",
    "    Uses self-attention mechanism to capture dependencies between tokens, potentially\n",
    "    handling long-range dependencies better than RNNs/LSTMs. Includes positional encoding\n",
    "    and a causal mask for autoregressive generation.\n",
    "\n",
    "    Architecture:\n",
    "        1. Embedding Layer: Maps token IDs to vectors. Embeddings are scaled.\n",
    "        2. Positional Encoding: Adds positional information to embeddings.\n",
    "        3. Transformer Encoder: Consists of multiple Transformer Encoder Layers.\n",
    "           - Each layer has Multi-Head Self-Attention and a Feed-Forward Network.\n",
    "           - A causal mask is applied to ensure autoregressive property (attend only to past tokens).\n",
    "        4. Linear Layer (FC): Maps Transformer output to vocabulary logits.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embed_dim (int): Dimension of token embeddings (d_model).\n",
    "        num_heads (int): Number of attention heads in Multi-Head Self-Attention. Must divide embed_dim.\n",
    "        hidden_dim (int): Dimension of the feed-forward network inside Transformer layers.\n",
    "        num_layers (int): Number of stacked Transformer Encoder layers.\n",
    "        max_seq_length (int): Maximum sequence length the model can handle (used for causal mask).\n",
    "        dropout (float): Dropout probability used in positional encoding and Transformer layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, num_heads: int, hidden_dim: int, num_layers: int, max_seq_length: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        # Ensure embedding dimension is divisible by the number of heads.\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=max_seq_length, dropout=dropout)\n",
    "\n",
    "        # Define a single Transformer Encoder layer.\n",
    "        # `batch_first=True` ensures input/output shapes are [batch, seq, feature].\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
    "                                                   dim_feedforward=hidden_dim, dropout=dropout,\n",
    "                                                   batch_first=True) # Crucial for shape consistency\n",
    "        # Stack multiple encoder layers.\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Fully connected layer to project Transformer output to vocabulary size.\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        # Store embedding dimension and max sequence length for later use.\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # --- Causal Mask ---\n",
    "        # Generate and register a causal (subsequent) mask. This prevents attention\n",
    "        # to future positions, which is essential for autoregressive language modeling.\n",
    "        # The mask shape will be [max_seq_length, max_seq_length].\n",
    "        self.register_buffer('causal_mask', self._generate_square_subsequent_mask(max_seq_length))\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates a square causal mask of size [sz, sz].\n",
    "        Masked positions (future tokens) are filled with float('-inf'),\n",
    "        unmasked positions (current and past tokens) are filled with float(0.0).\n",
    "        The Transformer layer adds this mask to the attention scores before softmax.\n",
    "        \"\"\"\n",
    "        # Create an upper triangle matrix of 1s.\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        # Fill with -inf where mask is 0 (future positions), and 0.0 where mask is 1 (current/past).\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of token IDs. Shape: [batch_size, seq_length].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of logits. Shape: [batch_size, seq_length, vocab_size].\n",
    "        \"\"\"\n",
    "        # Get the actual sequence length from the input batch.\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # --- Prepare Mask ---\n",
    "        # Select the appropriate part of the precomputed causal mask for the current sequence length.\n",
    "        # The mask needs to be on the same device as the input tensor `x`.\n",
    "        # Shape required by TransformerEncoder: [seq_len, seq_len]\n",
    "        current_mask = self.causal_mask[:seq_len, :seq_len].to(x.device)\n",
    "\n",
    "        # --- Embeddings and Positional Encoding ---\n",
    "        # Convert token IDs to embeddings. Shape: [batch, seq_len] -> [batch, seq_len, embed_dim]\n",
    "        # Scale embeddings by sqrt(embed_dim) as suggested in the 'Attention is All You Need' paper.\n",
    "        embeds = self.embedding(x) * math.sqrt(self.embed_dim)\n",
    "        # Add positional encodings. Shape remains [batch, seq_len, embed_dim].\n",
    "        encoded = self.pos_encoder(embeds)\n",
    "\n",
    "        # --- Transformer Encoder ---\n",
    "        # Pass the encoded sequence through the Transformer encoder layers.\n",
    "        # The `mask` argument ensures causal attention.\n",
    "        # Input shape (due to batch_first=True): [batch, seq_len, embed_dim]\n",
    "        # Output shape: [batch, seq_len, embed_dim]\n",
    "        transformer_out = self.transformer_encoder(encoded, mask=current_mask)\n",
    "\n",
    "        # --- Fully Connected Layer ---\n",
    "        # Project Transformer output to vocabulary logits.\n",
    "        # Shape: [batch, seq_len, embed_dim] -> [batch, seq_len, vocab_size]\n",
    "        logits = self.fc(transformer_out)\n",
    "        return logits\n",
    "\n",
    "    def prompt(self, tokenizer: spm.SentencePieceProcessor, prompt_text: str, max_length: int = 128, temperature: float = 1.0) -> str:\n",
    "        \"\"\" Convenience method to generate text using this model. \"\"\"\n",
    "        # The forward pass already incorporates the causal mask, so generate_text works correctly.\n",
    "        return generate_text(self, tokenizer, prompt_text, max_length, temperature)\n",
    "\n",
    "###############################################################################\n",
    "# Training, Evaluation, and Plotting Functions\n",
    "###############################################################################\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, num_epochs: int,\n",
    "                criterion: nn.Module, optimizer: optim.Optimizer, scheduler: ReduceLROnPlateau,\n",
    "                device: torch.device, patience: int = 5, model_name: str = \"Model\") -> tuple[list, list, float]:\n",
    "    \"\"\"\n",
    "    Trains the language model using the provided data loaders and hyperparameters.\n",
    "    Implements mini-batch gradient descent, gradient clipping, ReduceLROnPlateau\n",
    "    learning rate scheduling, and early stopping based on validation loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The language model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        num_epochs (int): The maximum number of epochs to train for.\n",
    "        criterion (nn.Module): The loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer (optim.Optimizer): The optimizer (e.g., AdamW).\n",
    "        scheduler (ReduceLROnPlateau): The learning rate scheduler.\n",
    "        device (torch.device): The device (CPU/GPU) to train on.\n",
    "        patience (int): Number of epochs to wait for validation loss improvement before stopping early.\n",
    "        model_name (str): Name of the model (used for logging).\n",
    "\n",
    "    Returns:\n",
    "        tuple[list, list, float]: A tuple containing:\n",
    "            - train_losses (list): List of average training losses per epoch.\n",
    "            - val_losses (list): List of average validation losses per epoch.\n",
    "            - total_training_time (float): Total time taken for training in seconds.\n",
    "    \"\"\"\n",
    "    # Move the model to the specified device.\n",
    "    model.to(device)\n",
    "\n",
    "    # Lists to store loss values for plotting.\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    # Variables for early stopping.\n",
    "    best_val_loss = float('inf') # Initialize with infinity; lower is better.\n",
    "    best_model_state = None      # To store the state_dict of the best model.\n",
    "    epochs_no_improve = 0        # Counter for epochs without validation loss improvement.\n",
    "\n",
    "    print(f\"--- Starting Training for {model_name} ---\")\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # --- Training Phase ---\n",
    "        model.train() # Set the model to training mode (enables dropout, etc.).\n",
    "        total_train_loss = 0.0\n",
    "        num_batches = 0\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            # Move batch data to the target device.\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Zero the gradients accumulated from the previous batch.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: Get model predictions (logits).\n",
    "            # Shape: [batch, seq_length, vocab_size]\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            # Reshape outputs and targets for CrossEntropyLoss:\n",
    "            # Outputs: [batch * seq_length, vocab_size]\n",
    "            # Targets: [batch * seq_length]\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "            # Check for NaN loss, which can indicate instability.\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"Warning: NaN loss detected at epoch {epoch}, batch {batch_idx}. Skipping batch update.\")\n",
    "                # Consider logging more details or stopping if NaNs persist.\n",
    "                continue # Skip backward pass and optimizer step for this batch.\n",
    "\n",
    "            # Backward pass: Compute gradients of the loss w.r.t. model parameters.\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Clipping: Prevent exploding gradients by clipping the norm of gradients.\n",
    "            # Helps stabilize training, especially with RNNs/LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update model parameters using the computed gradients.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss for averaging later.\n",
    "            total_train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Optional: Log progress within an epoch.\n",
    "            # if (batch_idx + 1) % 100 == 0:\n",
    "            #     print(f\"  Epoch {epoch}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Calculate average training loss for the epoch.\n",
    "        if num_batches == 0:\n",
    "             print(\"Warning: No batches processed in training epoch {epoch}. Check data loader.\")\n",
    "             avg_train_loss = 0.0\n",
    "        else:\n",
    "            avg_train_loss = total_train_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval() # Set the model to evaluation mode (disables dropout, etc.).\n",
    "        total_val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "        with torch.no_grad(): # Disable gradient calculations during validation.\n",
    "            for inputs, targets in val_loader:\n",
    "                # Move batch data to the target device.\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                # Forward pass.\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate loss.\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "                # Check for NaN loss during validation.\n",
    "                if torch.isnan(loss):\n",
    "                     print(f\"Warning: NaN loss detected during validation epoch {epoch}. Skipping batch contribution.\")\n",
    "                     continue\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                num_val_batches += 1\n",
    "\n",
    "        # Calculate average validation loss for the epoch.\n",
    "        if num_val_batches == 0:\n",
    "            print(f\"Warning: No batches processed in validation epoch {epoch}. Check validation loader.\")\n",
    "            avg_val_loss = float('inf') # Assign infinity if no validation batches.\n",
    "        else:\n",
    "            avg_val_loss = total_val_loss / num_val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        epoch_duration = time.time() - epoch_start\n",
    "        # Get the current learning rate from the optimizer.\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Print epoch summary.\n",
    "        print(f\"Epoch {epoch:02d}/{num_epochs} | LR: {current_lr:.6f} | Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | Duration: {epoch_duration:.2f}s\")\n",
    "\n",
    "        # --- Learning Rate Scheduling ---\n",
    "        # Step the scheduler based on the average validation loss.\n",
    "        # ReduceLROnPlateau will decrease LR if `avg_val_loss` hasn't improved for `scheduler.patience` epochs.\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # --- Early Stopping Logic ---\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            # Validation loss improved.\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0 # Reset the counter.\n",
    "            # Save the model state dictionary of the best performing model so far.\n",
    "            # Clone tensors to CPU to avoid GPU memory issues when loading later.\n",
    "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            print(f\"  New best validation loss: {best_val_loss:.4f}. Saving model state.\")\n",
    "            # Optional: Save the best model checkpoint to disk immediately.\n",
    "            # torch.save(best_model_state, f\"{model_name}_best_checkpoint.pt\")\n",
    "        else:\n",
    "            # Validation loss did not improve.\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n",
    "            # Check if patience limit is reached.\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch} epochs due to no improvement for {patience} epochs.\")\n",
    "                # Load the best model state if it was saved.\n",
    "                if best_model_state:\n",
    "                    print(\"Restoring best model state...\")\n",
    "                    # Ensure the model is on the correct device before loading state dict keys might need mapping if device changed\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                else:\n",
    "                    # This case should ideally not happen if training ran for at least one epoch where loss improved.\n",
    "                    print(\"Warning: Early stopping triggered, but no best model state was saved (potential issue).\")\n",
    "                break # Exit the training loop.\n",
    "\n",
    "    # --- End of Training Loop ---\n",
    "    total_training_time = time.time() - total_start_time\n",
    "    print(f\"--- Finished Training for {model_name} ---\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
    "\n",
    "    # If early stopping was triggered, ensure the best model state is loaded before returning.\n",
    "    # This check handles the case where training finishes exactly on the patience limit.\n",
    "    if epochs_no_improve >= patience and best_model_state:\n",
    "        # Check if the current model state is already the best one (it should be if restored in the loop)\n",
    "        # This load is potentially redundant but safe.\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Ensured best model state is loaded after early stopping.\")\n",
    "\n",
    "    return train_losses, val_losses, total_training_time\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, data_loader: DataLoader, criterion: nn.Module, device: torch.device) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on a given dataset (e.g., validation or test set).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained language model.\n",
    "        data_loader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        criterion (nn.Module): The loss function used during training.\n",
    "        device (torch.device): The device (CPU/GPU) to run evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss over the entire dataset. Returns float('inf') if evaluation fails.\n",
    "    \"\"\"\n",
    "    model.eval() # Set the model to evaluation mode.\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad(): # Disable gradient calculations.\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Reshape for loss calculation.\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            # Accumulate loss if it's valid.\n",
    "            if not torch.isnan(loss):\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            else:\n",
    "                 print(\"Warning: NaN loss encountered during final evaluation.\")\n",
    "\n",
    "    # Calculate average loss.\n",
    "    if num_batches == 0:\n",
    "        print(\"Error: No batches processed during evaluation. Check data loader.\")\n",
    "        return float('inf') # Indicate error or empty dataset.\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def compute_perplexity(loss: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculates perplexity from the average cross-entropy loss.\n",
    "    Perplexity = exp(average_loss). Lower perplexity indicates a better model fit.\n",
    "\n",
    "    Args:\n",
    "        loss (float): The average cross-entropy loss.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated perplexity. Returns float('inf') if loss is invalid or too large.\n",
    "    \"\"\"\n",
    "    # Handle invalid loss values.\n",
    "    if loss is None or loss == float('inf') or loss < 0:\n",
    "        return float('inf')\n",
    "    try:\n",
    "        # Calculate perplexity using the exponential function.\n",
    "        perplexity = math.exp(loss)\n",
    "        return perplexity\n",
    "    except OverflowError:\n",
    "        # Handle cases where the loss is extremely large, causing exp() to overflow.\n",
    "        print(f\"Warning: Loss value {loss} too large, resulting in perplexity overflow.\")\n",
    "        return float('inf')\n",
    "\n",
    "def compute_token_accuracy(model: nn.Module, data_loader: DataLoader, device: torch.device) -> float:\n",
    "    \"\"\"\n",
    "    Computes the token-level accuracy of the model on a given dataset.\n",
    "    Accuracy = (Number of correctly predicted tokens) / (Total number of tokens).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained language model.\n",
    "        data_loader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): The device (CPU/GPU) to run evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        float: The token accuracy (between 0.0 and 1.0).\n",
    "    \"\"\"\n",
    "    model.eval() # Set the model to evaluation mode.\n",
    "    correct = 0 # Counter for correctly predicted tokens.\n",
    "    total = 0   # Counter for total tokens evaluated.\n",
    "    with torch.no_grad(): # Disable gradient calculations.\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Get model logits. Shape: [batch, seq_len, vocab_size]\n",
    "            outputs = model(inputs)\n",
    "            # Get the index of the highest logit for each position (predicted token ID).\n",
    "            # Shape: [batch, seq_len]\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            # Compare predictions with the actual target tokens element-wise.\n",
    "            # Sum up the number of correct predictions in the batch.\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            # Add the total number of tokens in the batch targets to the total count.\n",
    "            total += targets.numel()\n",
    "\n",
    "    # Calculate accuracy, handling the case of zero total tokens.\n",
    "    if total == 0:\n",
    "        print(\"Warning: Zero tokens found in accuracy calculation. Check data loader.\")\n",
    "        return 0.0\n",
    "    return correct / total\n",
    "\n",
    "def compute_bleu(reference: str, candidate: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes the BLEU (Bilingual Evaluation Understudy) score between a candidate\n",
    "    (generated) sentence and a reference sentence using NLTK.\n",
    "\n",
    "    Note: BLEU is primarily designed for machine translation and measures n-gram precision.\n",
    "    Its applicability to open-ended text generation can be limited, but it provides\n",
    "    a rough measure of lexical overlap.\n",
    "\n",
    "    Args:\n",
    "        reference (str): The ground truth sentence (target).\n",
    "        candidate (str): The generated sentence (prediction).\n",
    "\n",
    "    Returns:\n",
    "        float: The BLEU score (typically between 0 and 1). Returns 0.0 if candidate\n",
    "               or reference is empty or if an error occurs.\n",
    "    \"\"\"\n",
    "    # Return 0 if either string is empty, as BLEU requires content.\n",
    "    if not candidate or not reference:\n",
    "        return 0.0\n",
    "\n",
    "    # Use a smoothing function (Chen & Cherry method 1) to handle cases where\n",
    "    # higher-order n-grams (e.g., 4-grams) might not appear in the candidate,\n",
    "    # preventing a score of 0 just because of short sentences or lack of overlap.\n",
    "    chencherry = SmoothingFunction()\n",
    "\n",
    "    try:\n",
    "        # Tokenize the reference and candidate sentences using NLTK's word tokenizer.\n",
    "        # `sentence_bleu` expects references as a list of lists of tokens.\n",
    "        ref_tokens = [nltk.word_tokenize(reference.lower())] # Use lowercase for case-insensitive comparison\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "\n",
    "        # Calculate BLEU score (defaults to BLEU-4, considering 1 to 4-grams).\n",
    "        bleu = sentence_bleu(ref_tokens, cand_tokens, smoothing_function=chencherry.method1)\n",
    "    except Exception as e:\n",
    "        # Catch potential errors during tokenization or BLEU calculation.\n",
    "        print(f\"Warning: Could not compute BLEU score. Error: {e}\")\n",
    "        # Print truncated versions for debugging.\n",
    "        print(f\"  Reference (truncated): '{reference[:100]}...'\")\n",
    "        print(f\"  Candidate (truncated): '{candidate[:100]}...'\")\n",
    "        bleu = 0.0 # Return 0.0 on error.\n",
    "    return bleu\n",
    "\n",
    "def plot_loss_curve(train_losses: list[float], val_losses: list[float], model_name: str):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss curves over epochs and saves the plot to a file.\n",
    "    Includes annotations for minimum validation loss and final loss values.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list[float]): List of average training losses per epoch.\n",
    "        val_losses (list[float]): List of average validation losses per epoch.\n",
    "        model_name (str): Name of the model (used for title and filename).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6)) # Set figure size.\n",
    "    epochs = range(1, len(train_losses) + 1) # X-axis values (epochs).\n",
    "\n",
    "    # Plot training and validation losses.\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o', linestyle='-', linewidth=2)\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\", marker='s', linestyle='-', linewidth=2)\n",
    "\n",
    "    # Add labels and title.\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Loss\", fontsize=12)\n",
    "    plt.title(f\"{model_name} Training & Validation Loss Curve\", fontsize=14)\n",
    "\n",
    "    # Add legend.\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    # Add grid for better readability.\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # --- Annotations ---\n",
    "    # Find minimum validation loss and the epoch it occurred at.\n",
    "    if val_losses: # Ensure val_losses is not empty.\n",
    "        min_val_loss = min(val_losses)\n",
    "        min_val_epoch = val_losses.index(min_val_loss) + 1 # Add 1 for 1-based epoch indexing.\n",
    "        # Add an annotation arrow pointing to the minimum validation loss point.\n",
    "        plt.annotate(f\"Min Val Loss: {min_val_loss:.4f} at Epoch {min_val_epoch}\",\n",
    "                     xy=(min_val_epoch, min_val_loss), # Point coordinates\n",
    "                     xytext=(min_val_epoch + 0.5, min_val_loss + 0.1 * min_val_loss), # Text position (offset)\n",
    "                     arrowprops=dict(facecolor='red', shrink=0.05, width=1, headwidth=8), # Arrow style\n",
    "                     fontsize=10, color='red')\n",
    "\n",
    "    # Annotate final loss values at the last epoch.\n",
    "    if len(epochs) > 0:\n",
    "        final_epoch = epochs[-1]\n",
    "        final_train_loss = train_losses[-1]\n",
    "        final_val_loss = val_losses[-1]\n",
    "        # Use scatter points to highlight the final values.\n",
    "        # `zorder=5` ensures points are plotted on top of lines.\n",
    "        plt.scatter(final_epoch, final_train_loss, color='blue', s=50, zorder=5,\n",
    "                    label=f\"Final Train: {final_train_loss:.4f}\")\n",
    "        plt.scatter(final_epoch, final_val_loss, color='orange', s=50, zorder=5,\n",
    "                    label=f\"Final Val: {final_val_loss:.4f}\")\n",
    "        plt.legend() # Update legend to include scatter point labels.\n",
    "\n",
    "    # Adjust layout to prevent labels from overlapping.\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to a file.\n",
    "    filename = f\"{model_name}_loss_curve.png\"\n",
    "    plt.savefig(filename, dpi=300) # Save with high resolution.\n",
    "    print(f\"Loss curve saved as {filename}\")\n",
    "\n",
    "    # Display the plot.\n",
    "    plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# Main Training and Evaluation Pipeline\n",
    "###############################################################################\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the entire process:\n",
    "    1. Sets up hyperparameters and file paths.\n",
    "    2. Prepares the tokenizer (trains if necessary).\n",
    "    3. Loads and preprocesses data (tokenization, sequence building).\n",
    "    4. Creates DataLoaders.\n",
    "    5. Initializes RNN, LSTM, and Transformer models.\n",
    "    6. Trains each model with evaluation, LR scheduling, and early stopping.\n",
    "    7. Calculates final metrics (perplexity, accuracy, BLEU).\n",
    "    8. Generates sample text from each model.\n",
    "    9. Saves trained models.\n",
    "    10. Compares model performance using tables and plots.\n",
    "    \"\"\"\n",
    "    global_start = time.time() # Record start time for total duration.\n",
    "\n",
    "        # ------------------ Hyperparameters ------------------\n",
    "    # These values can be tuned based on the specific dataset, task, and available hardware.\n",
    "    vocab_size = 10000         # Target vocabulary size for the BPE tokenizer.\n",
    "    embed_dim = 256            # Dimensionality of token embeddings.\n",
    "    hidden_dim = 256           # Hidden dimension for RNN/LSTM and feed-forward layer in Transformer.\n",
    "                               # Increased from original script for potentially more capacity.\n",
    "    num_layers = 2             # Number of layers in RNN/LSTM/Transformer encoder stacks.\n",
    "    num_heads = 8              # Number of attention heads in Transformer (must divide embed_dim).\n",
    "    max_seq_length = 512       # Maximum length of input sequences fed to the models.\n",
    "    batch_size = 128            # Number of sequences per training batch. Smaller batches can sometimes help generalization\n",
    "                               # and reduce memory usage, but may slow down training.\n",
    "    num_epochs = 30            # Maximum number of training epochs. Early stopping might finish sooner.\n",
    "    learning_rate = 1e-3      # Initial learning rate for the AdamW optimizer.\n",
    "    dropout_rate = 0.1         # Dropout rate for regularization in models.\n",
    "    weight_decay = 0.01        # Weight decay (L2 regularization) for the AdamW optimizer. Helps prevent overfitting.\n",
    "    # Note: pad_token_id is determined dynamically from the tokenizer later.\n",
    "\n",
    "    # ReduceLROnPlateau Scheduler Parameters\n",
    "    lr_patience = 2            # Number of epochs with no improvement in validation loss before reducing LR.\n",
    "    lr_factor = 0.5            # Factor by which the learning rate will be reduced (new_lr = lr * factor).\n",
    "\n",
    "    # Early Stopping Parameters\n",
    "    early_stopping_patience = 5 # Number of epochs with no improvement in validation loss before stopping training.\n",
    "\n",
    "    # ------------------ File Paths ------------------\n",
    "    train_file = \"train.jsonl\"                  # Path to the training data JSONL file.\n",
    "    test_file = \"test.jsonl\"                    # Path to the validation/test data JSONL file.\n",
    "    tokenizer_model_prefix = \"bpe_tokenizer_v1\" # Prefix for saving/loading the tokenizer model.\n",
    "    tokenizer_training_file = \"merged_corpus.txt\" # Path to the text file used FOR training the tokenizer.\n",
    "                                                  # This file will be created if it doesn't exist.\n",
    "\n",
    "    # --- File Existence Checks ---\n",
    "    if not os.path.exists(train_file):\n",
    "        raise FileNotFoundError(f\"Training data file not found: {train_file}\")\n",
    "    if not os.path.exists(test_file):\n",
    "        raise FileNotFoundError(f\"Validation/Test data file not found: {test_file}\")\n",
    "\n",
    "    # ------------------ Prepare Training Text for Tokenizer (if needed) ------------------\n",
    "    # Check if the tokenizer model already exists. If not, prepare the training text file.\n",
    "    if not os.path.exists(f\"{tokenizer_model_prefix}.model\"):\n",
    "        # If the consolidated training text file doesn't exist, create it from the train_file.\n",
    "        if not os.path.exists(tokenizer_training_file):\n",
    "            print(f\"Creating combined text file for tokenizer training: {tokenizer_training_file}\")\n",
    "            texts = []\n",
    "            # Read the training JSONL file.\n",
    "            with open(train_file, \"r\", encoding=\"utf-8\") as f_in:\n",
    "                for line in f_in:\n",
    "                    try:\n",
    "                        # Parse each line as JSON.\n",
    "                        obj = json.loads(line)\n",
    "                        # Extract prompt and completion.\n",
    "                        prompt = obj.get(\"prompt\", \"\")\n",
    "                        completion = obj.get(\"completion\", \"\")\n",
    "                        # Combine and strip whitespace.\n",
    "                        text = (prompt + \" \" + completion).strip()\n",
    "                        # Add non-empty text to the list.\n",
    "                        if text:\n",
    "                            texts.append(text)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Skip invalid JSON lines.\n",
    "                        continue\n",
    "            # Join all texts with newlines and write to the tokenizer training file.\n",
    "            combined_text = \"\\n\".join(texts)\n",
    "            with open(tokenizer_training_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "                f_out.write(combined_text)\n",
    "            print(f\"Saved combined text ({len(texts)} entries) to {tokenizer_training_file}\")\n",
    "        else:\n",
    "            # Use the existing file if found.\n",
    "            print(f\"Using existing tokenizer training file: {tokenizer_training_file}\")\n",
    "\n",
    "    # ------------------ Tokenizer Preparation ------------------\n",
    "    # Train the tokenizer if needed, or load the existing one.\n",
    "    sp = train_tokenizer_if_needed(tokenizer_model_prefix=tokenizer_model_prefix,\n",
    "                                    vocab_size=vocab_size,\n",
    "                                    training_text_file=tokenizer_training_file)\n",
    "\n",
    "    # Determine the padding token ID from the loaded tokenizer.\n",
    "    pad_token_id = sp.pad_id()\n",
    "    print(f\"Using Padding Token ID: {pad_token_id}\")\n",
    "\n",
    "    # Set the `ignore_index` for the CrossEntropyLoss function.\n",
    "    # If the tokenizer defined a PAD token (pad_id >= 0), ignore it during loss calculation.\n",
    "    # Otherwise, use the PyTorch default ignore_index (-100), which assumes no padding ID needs ignoring.\n",
    "    ignore_index = pad_token_id if pad_token_id >= 0 else -100\n",
    "\n",
    "    # Get the actual vocabulary size from the trained tokenizer (might differ slightly from target).\n",
    "    effective_vocab_size = sp.vocab_size()\n",
    "    print(f\"Effective vocabulary size: {effective_vocab_size}\")\n",
    "\n",
    "    # ------------------ Load and Tokenize Datasets ------------------\n",
    "    print(\"\\nLoading and tokenizing datasets...\")\n",
    "    # Tokenize the training and validation/test files into flat lists of token IDs.\n",
    "    train_tokens = load_and_tokenize(train_file, sp)\n",
    "    val_tokens = load_and_tokenize(test_file, sp)\n",
    "\n",
    "    # ------------------ Build Fixed-Length Token Sequences ------------------\n",
    "    print(\"\\nBuilding sequences...\")\n",
    "    # Create overlapping sequences of length `max_seq_length + 1`.\n",
    "    train_seqs = build_sequences(train_tokens, max_seq_length)\n",
    "    val_seqs = build_sequences(val_tokens, max_seq_length)\n",
    "    print(f\"Number of training sequences created: {len(train_seqs)}\")\n",
    "    print(f\"Number of validation sequences created: {len(val_seqs)}\")\n",
    "\n",
    "    # Check if sequence creation was successful.\n",
    "    if not train_seqs or not val_seqs:\n",
    "        raise ValueError(\"Failed to create training or validation sequences. \"\n",
    "                         \"Check data loading, tokenization, and sequence length.\")\n",
    "\n",
    "    # ------------------ Create Dataset Objects and DataLoaders ------------------\n",
    "    print(\"\\nCreating Dataset objects and DataLoaders...\")\n",
    "    # Instantiate custom Dataset objects.\n",
    "    train_dataset = LanguageModelDataset(train_seqs, max_seq_length)\n",
    "    val_dataset = LanguageModelDataset(val_seqs, max_seq_length)\n",
    "    print(f\"Train dataset size: {len(train_dataset)} samples\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)} samples\")\n",
    "\n",
    "    # --- DataLoader Configuration ---\n",
    "    # `num_workers`: Number of subprocesses for data loading. `0` means data is loaded in the main process.\n",
    "    # Using `0` is often safer on Windows and macOS and simplifies debugging.\n",
    "    # Higher values can speed up loading on Linux systems with multiple cores, especially if preprocessing is heavy.\n",
    "    num_workers = 0\n",
    "    # `pin_memory`: If True and using GPU, copies tensors into pinned memory before returning them.\n",
    "    # Can speed up data transfer from CPU to GPU.\n",
    "    pin_memory = True if device.type == \"cuda\" else False\n",
    "\n",
    "    # Create DataLoader for training data.\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True, # Shuffle training data each epoch.\n",
    "                              num_workers=num_workers,\n",
    "                              pin_memory=pin_memory)\n",
    "    # Create DataLoader for validation data.\n",
    "    val_loader = DataLoader(val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False, # No need to shuffle validation data.\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=pin_memory)\n",
    "\n",
    "    # ------------------ Initialize Models ------------------\n",
    "    print(\"\\nInitializing models...\")\n",
    "    # Create instances of the three models. Use the effective vocabulary size.\n",
    "    models = {\n",
    "        \"RNN\": RNNLanguageModel(effective_vocab_size, embed_dim, hidden_dim, num_layers, dropout=dropout_rate),\n",
    "        \"LSTM\": LSTMLanguageModel(effective_vocab_size, embed_dim, hidden_dim, num_layers, dropout=dropout_rate),\n",
    "        \"Transformer\": TransformerLanguageModel(effective_vocab_size, embed_dim, num_heads, hidden_dim,\n",
    "                                               num_layers, max_seq_length, dropout=dropout_rate)\n",
    "    }\n",
    "\n",
    "    # Dictionary to store evaluation results for each model.\n",
    "    model_results = {}\n",
    "\n",
    "    # ------------------ Train, Evaluate and Compare Models ------------------\n",
    "    # Loop through each model type (RNN, LSTM, Transformer).\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n===== Processing Model: {name} =====\")\n",
    "        # Move model to the target device.\n",
    "        model.to(device)\n",
    "\n",
    "        # Count and print the number of trainable parameters.\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Model: {name}, Trainable Parameters: {num_params:,}\")\n",
    "\n",
    "        # --- Setup for Training ---\n",
    "        # Loss Function: CrossEntropyLoss is standard for multi-class classification (predicting the next token).\n",
    "        # `ignore_index` ensures that padding tokens (if used) do not contribute to the loss.\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "\n",
    "        # Optimizer: AdamW is Adam with decoupled weight decay, often preferred for Transformers.\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # Learning Rate Scheduler: Reduces LR when validation loss stops improving.\n",
    "        # `mode='min'`: Reduce LR when the monitored quantity (val loss) stops decreasing.\n",
    "        # `factor`: Multiplicative factor for LR reduction.\n",
    "        # `patience`: Number of epochs to wait before reducing LR.\n",
    "        # `verbose=False`: Removed deprecated argument. Progress is logged manually.\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=lr_factor, patience=lr_patience)\n",
    "\n",
    "        # --- Train the Model ---\n",
    "        # Calls the training function, which handles the epoch loop, early stopping, etc.\n",
    "        train_losses, val_losses, training_time = train_model(\n",
    "            model, train_loader, val_loader, num_epochs,\n",
    "            criterion, optimizer, scheduler, device,\n",
    "            patience=early_stopping_patience, model_name=name\n",
    "        )\n",
    "\n",
    "        # --- Plot Loss Curves ---\n",
    "        # Visualize the training and validation loss progression.\n",
    "        plot_loss_curve(train_losses, val_losses, name)\n",
    "\n",
    "        # --- Final Evaluation ---\n",
    "        # Evaluate the final model (best one loaded by `train_model` if early stopping occurred) on the validation set.\n",
    "        print(f\"\\n--- Evaluating Final {name} Model on Validation Set ---\")\n",
    "        final_val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "        # Calculate final perplexity and token accuracy based on the final validation loss.\n",
    "        perplexity = compute_perplexity(final_val_loss)\n",
    "        token_accuracy = compute_token_accuracy(model, val_loader, device) * 100 # Convert to percentage.\n",
    "\n",
    "        # --- BLEU Score Calculation ---\n",
    "        # Calculate BLEU score on a sample of the validation set for an indicative measure.\n",
    "        num_bleu_samples = 50 # Number of samples to use for BLEU calculation.\n",
    "        total_bleu = 0.0\n",
    "        bleu_samples_evaluated = 0\n",
    "        if len(val_dataset) > 0:\n",
    "            print(f\"Calculating BLEU score on {min(num_bleu_samples, len(val_dataset))} validation samples...\")\n",
    "            # Ensure we don't try to sample more items than available in the validation set.\n",
    "            indices_to_sample = random.sample(range(len(val_dataset)), k=min(num_bleu_samples, len(val_dataset)))\n",
    "\n",
    "            for idx in indices_to_sample:\n",
    "                # Get input and target tensors for the sample.\n",
    "                sample_input_tokens, sample_target_tokens = val_dataset[idx]\n",
    "                # Decode tokens back to text for prompt and reference.\n",
    "                # Convert tensors to lists before decoding.\n",
    "                prompt_text = sp.decode(sample_input_tokens.tolist())\n",
    "                reference_text = sp.decode(sample_target_tokens.tolist()) # The ground truth completion/next sequence.\n",
    "\n",
    "                # Generate text using the trained model.\n",
    "                # Use a moderate temperature (e.g., 0.7) for generation, as greedy decoding (temp~0)\n",
    "                # might produce repetitive text, potentially lowering BLEU.\n",
    "                generated_text = model.prompt(sp, prompt_text, max_length=max_seq_length, temperature=0.7)\n",
    "\n",
    "                # Note: The generated text includes the prompt. The reference_text corresponds to the target sequence (shifted input).\n",
    "                # For a fairer comparison, one might extract only the generated part after the prompt,\n",
    "                # but this simple approach compares the full generated sequence against the target sequence.\n",
    "                bleu_score_sample = compute_bleu(reference_text, generated_text)\n",
    "                total_bleu += bleu_score_sample\n",
    "                bleu_samples_evaluated += 1\n",
    "\n",
    "            # Calculate the average BLEU score over the samples.\n",
    "            avg_bleu = total_bleu / bleu_samples_evaluated if bleu_samples_evaluated > 0 else 0.0\n",
    "            print(f\"Average BLEU-{num_bleu_samples} score: {avg_bleu:.4f}\")\n",
    "        else:\n",
    "            # Handle case where validation dataset is empty.\n",
    "            avg_bleu = 0.0\n",
    "            print(\"Validation dataset is empty, cannot compute BLEU score.\")\n",
    "        # --- End BLEU Score Calculation ---\n",
    "\n",
    "        # --- Print Evaluation Summary ---\n",
    "        print(f\"\\n--- {name} Final Evaluation Summary ---\")\n",
    "        print(f\"  Validation Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"  Perplexity:        {perplexity:.2f}\")\n",
    "        print(f\"  Token Accuracy:    {token_accuracy:.2f}%\")\n",
    "        print(f\"  Avg BLEU Score:    {avg_bleu:.4f} (Sampled, lower temp generation)\")\n",
    "        print(\"  (Note: Perplexity/Accuracy reflect next-token prediction; BLEU reflects n-gram overlap in generated samples)\")\n",
    "\n",
    "                # --- Generate Sample Outputs ---\n",
    "        # 1) “Best way to learn programming” prompt\n",
    "        fixed_prompt = \"The best way to learn programming is\"\n",
    "        print(\"\\n--- Sample Generations (Prompt 1) ---\")\n",
    "        print(f\"Prompt: '{fixed_prompt}'\")\n",
    "        for temp in [0.5, 1.0, 1.5]:\n",
    "            sample_output = model.prompt(sp, fixed_prompt, max_length=60, temperature=temp)\n",
    "            print(f\"  Temp={temp:.1f}: {sample_output}\")\n",
    "\n",
    "        # 2) “Which do you prefer? Cats or Dogs?” prompt\n",
    "        second_prompt = \"Which do you prefer? Cats or Dogs?\"\n",
    "        print(\"\\n--- Sample Generations (Prompt 2) ---\")\n",
    "        print(f\"Prompt: '{second_prompt}'\")\n",
    "        for temp in [0.5, 1.0, 1.5]:\n",
    "            sample_output = model.prompt(sp, second_prompt, max_length=30, temperature=temp)\n",
    "            print(f\"  Temp={temp:.1f}: {sample_output}\")\n",
    "\n",
    "\n",
    "        # --- Save the Trained Model ---\n",
    "        # Save the state dictionary of the final (best) model.\n",
    "        model_save_path = f\"{name}_model_final.pt\"\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"\\n{name} model state dictionary saved to {model_save_path}\")\n",
    "\n",
    "        # --- Store Results ---\n",
    "        # Store the key metrics for this model in the results dictionary.\n",
    "        model_results[name] = {\n",
    "            \"ValLoss\": final_val_loss,\n",
    "            \"Perplexity\": perplexity,\n",
    "            \"Token Accuracy (%)\": token_accuracy,\n",
    "            \"BLEU Score\": avg_bleu,\n",
    "            \"Training Time (s)\": training_time,\n",
    "            \"Parameters\": num_params\n",
    "        }\n",
    "\n",
    "    # ------------------ Compare Model Performance ------------------\n",
    "    print(\"\\n===== Model Performance Comparison =====\")\n",
    "    # Define table header.\n",
    "    header = f\"{'Model':<15} {'Parameters':<15} {'Perplexity':<12} {'Token Acc (%)':<15} {'BLEU Score':<12} {'Train Time (s)':<15}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header)) # Print separator line.\n",
    "    # Print metrics for each model.\n",
    "    for model_name, metrics in model_results.items():\n",
    "        print(f\"{model_name:<15} {metrics['Parameters']:, <14} {metrics['Perplexity']:<12.2f} \"\n",
    "              f\"{metrics['Token Accuracy (%)']:<15.2f} {metrics['BLEU Score']:<12.4f} {metrics['Training Time (s)']:<15.2f}\")\n",
    "\n",
    "    # --- Create Comparative Bar Plots ---\n",
    "    # Extract metrics for plotting.\n",
    "    model_names = list(model_results.keys())\n",
    "    perplexities = [model_results[m][\"Perplexity\"] for m in model_names]\n",
    "    accuracies = [model_results[m][\"Token Accuracy (%)\"] for m in model_names]\n",
    "    bleu_scores = [model_results[m][\"BLEU Score\"] for m in model_names]\n",
    "    train_times = [model_results[m][\"Training Time (s)\"] for m in model_names]\n",
    "    # params = [model_results[m][\"Parameters\"] for m in model_names] # Could also plot parameters\n",
    "\n",
    "    # Plotting setup.\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10)) # Create a 2x2 grid of subplots.\n",
    "    x = np.arange(len(model_names)) # X-axis positions for bars.\n",
    "    width = 0.3 # Width of the bars.\n",
    "\n",
    "    # Plot 1: Perplexity (Lower is better)\n",
    "    bars1 = axs[0, 0].bar(x, perplexities, width, color=\"skyblue\", label='Perplexity')\n",
    "    axs[0, 0].set_ylabel('Perplexity')\n",
    "    axs[0, 0].set_title('Validation Perplexity (Lower is Better)')\n",
    "    axs[0, 0].set_xticks(x)\n",
    "    axs[0, 0].set_xticklabels(model_names)\n",
    "    axs[0, 0].bar_label(bars1, fmt='%.2f') # Add labels on top of bars.\n",
    "\n",
    "    # Plot 2: Token Accuracy (Higher is better)\n",
    "    bars2 = axs[0, 1].bar(x, accuracies, width, color=\"lightgreen\", label='Accuracy')\n",
    "    axs[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axs[0, 1].set_title('Validation Token Accuracy (Higher is Better)')\n",
    "    axs[0, 1].set_xticks(x)\n",
    "    axs[0, 1].set_xticklabels(model_names)\n",
    "    # Adjust y-axis limits for better visualization if values are close.\n",
    "    if accuracies:\n",
    "        axs[0, 1].set_ylim(bottom=max(0, min(accuracies) - 5), top=min(100, max(accuracies) + 5))\n",
    "    axs[0, 1].bar_label(bars2, fmt='%.2f')\n",
    "\n",
    "    # Plot 3: BLEU Score (Higher is better, context dependent)\n",
    "    bars3 = axs[1, 0].bar(x, bleu_scores, width, color=\"salmon\", label='BLEU')\n",
    "    axs[1, 0].set_ylabel('BLEU Score')\n",
    "    axs[1, 0].set_title(f'Average BLEU-{num_bleu_samples} Score (Higher is Better)')\n",
    "    axs[1, 0].set_xticks(x)\n",
    "    axs[1, 0].set_xticklabels(model_names)\n",
    "    axs[1, 0].bar_label(bars3, fmt='%.4f')\n",
    "\n",
    "    # Plot 4: Training Time\n",
    "    bars4 = axs[1, 1].bar(x, train_times, width, color=\"plum\", label='Time')\n",
    "    axs[1, 1].set_ylabel('Time (s)')\n",
    "    axs[1, 1].set_title('Total Training Time')\n",
    "    axs[1, 1].set_xticks(x)\n",
    "    axs[1, 1].set_xticklabels(model_names)\n",
    "    axs[1, 1].bar_label(bars4, fmt='%.1f')\n",
    "\n",
    "    # Add overall title and adjust layout.\n",
    "    plt.suptitle(\"Model Performance Comparison\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make space for suptitle.\n",
    "\n",
    "    # Save the comparison plot.\n",
    "    comparison_plot_path = \"model_performance_comparison.png\"\n",
    "    plt.savefig(comparison_plot_path, dpi=300)\n",
    "    print(f\"\\nComparison plot saved as {comparison_plot_path}\")\n",
    "    plt.show() # Display the plot.\n",
    "\n",
    "    # --- Final Timing ---\n",
    "    total_time = time.time() - global_start\n",
    "    print(f\"\\nScript finished. Total elapsed time: {total_time / 60:.2f} minutes\")\n",
    "\n",
    "# --- Script Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the main function when the script is run directly.\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
